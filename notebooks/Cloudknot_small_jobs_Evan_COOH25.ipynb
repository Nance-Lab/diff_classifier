{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use Cloudknot to parallelize a tracking method.  Example Cloudknot functions are provided in the knotlet module, but the user must build his/her own functions for this step to work properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import cloudknot as ck\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np\n",
    "\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "\n",
    "import diff_classifier.utils as ut\n",
    "import diff_classifier.msd as msd\n",
    "import diff_classifier.features as ft\n",
    "import diff_classifier.heatmaps as hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, run \"sudo docker system prune -a\" in order to clear docker images.\n",
    "Also change variable start_knot to a unique number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define the nomenclature I use to name my files, as well as specify exceptions (files that weren't generated or are missing and will be skipped in the analysis).  In this case, I was analyzing data collected in tissue slices.  Videos are named according to the pup number, the slice number, the hemisphere, and the video number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = []\n",
    "result_futures = {}\n",
    "start_knot = 2 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "#slices = [\"1\", \"2\", \"3\", \"4\", \"5\"] #Number of slices per pup\n",
    "folder = '08_03_18_varying_PEG_excess' #Folder in AWS S3 containing files to be analyzed\n",
    "bucket = 'evanepst.data'\n",
    "vids = 20\n",
    "#xs = ['pt25', 'pt50', 'pt75', '1', '2', '3', '4']\n",
    "xs = ['PSCOOH', 'pt25xs']\n",
    "for x in xs:\n",
    "    for num in range(1, vids+1):\n",
    "        to_track.append('100_{}_XY{}'.format(x, '%02d' % num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100_PSCOOH_XY01',\n",
       " '100_PSCOOH_XY02',\n",
       " '100_PSCOOH_XY03',\n",
       " '100_PSCOOH_XY04',\n",
       " '100_PSCOOH_XY05',\n",
       " '100_PSCOOH_XY06',\n",
       " '100_PSCOOH_XY07',\n",
       " '100_PSCOOH_XY08',\n",
       " '100_PSCOOH_XY09',\n",
       " '100_PSCOOH_XY10',\n",
       " '100_PSCOOH_XY11',\n",
       " '100_PSCOOH_XY12',\n",
       " '100_PSCOOH_XY13',\n",
       " '100_PSCOOH_XY14',\n",
       " '100_PSCOOH_XY15',\n",
       " '100_PSCOOH_XY16',\n",
       " '100_PSCOOH_XY17',\n",
       " '100_PSCOOH_XY18',\n",
       " '100_PSCOOH_XY19',\n",
       " '100_PSCOOH_XY20',\n",
       " '100_pt25xs_XY01',\n",
       " '100_pt25xs_XY02',\n",
       " '100_pt25xs_XY03',\n",
       " '100_pt25xs_XY04',\n",
       " '100_pt25xs_XY05',\n",
       " '100_pt25xs_XY06',\n",
       " '100_pt25xs_XY07',\n",
       " '100_pt25xs_XY08',\n",
       " '100_pt25xs_XY09',\n",
       " '100_pt25xs_XY10',\n",
       " '100_pt25xs_XY11',\n",
       " '100_pt25xs_XY12',\n",
       " '100_pt25xs_XY13',\n",
       " '100_pt25xs_XY14',\n",
       " '100_pt25xs_XY15',\n",
       " '100_pt25xs_XY16',\n",
       " '100_pt25xs_XY17',\n",
       " '100_pt25xs_XY18',\n",
       " '100_pt25xs_XY19',\n",
       " '100_pt25xs_XY20']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run this in a Python 3 notebook, and switch back to Python 2 when submitting final job to Cloudknot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "remote_folder = folder\n",
    "tnum=30 #number of training datasets\n",
    "\n",
    "pref = []\n",
    "for num in to_track:                    \n",
    "    for row in range(0, 4):\n",
    "        for col in range(0, 4):\n",
    "            pref.append(\"{}_{}_{}\".format(num, row, col))\n",
    "\n",
    "y = np.array([38.3, 44.6, 39.2, 28.9, 27.8, 46, 51.4, 34.94, 50, 49.56, 45, 46.26, 36, 46, 45, 48, 45.6, \n",
    "              41.5, 48.3, 50, 39, 57.5, 36, 48, 57, 47, 42, 44, 39.5, 40.8])\n",
    "\n",
    "# Creates regression object based of training dataset composed of input images and manually\n",
    "# calculated quality cutoffs from tracking with GUI interface.\n",
    "regress = ij.regress_sys(folder, pref, y, tnum, have_output=True, bucket_name = bucket)\n",
    "#Read up on how regress_sys works before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle object\n",
    "filename = 'regress4.obj'\n",
    "#filehandler = open(filename, 'w')\n",
    "#pickle.dump(regress, filehandler)\n",
    "\n",
    "with open(filename,'wb') as fp:\n",
    "    joblib.dump(regress,fp)\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "aws.upload_s3(filename, folder+'/'+filename, bucket_name = bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Cloudknot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below is sent to each individual machine the user calls upon.  A single video is sent to each machine for analysis, and the resulting outputs are uploaded to S3.  This case uses files that are only temporarily stored in a private bucket.  \n",
    "\n",
    "The following function is broken down into four separate sections performing different tasks of the analysis:\n",
    "\n",
    "* **parameter prediction**: A regression tool is used to predict the quality tracking parameter used by Trackmate based off a training dataset of images whose qualities were assessed manually beforehand.  If analyzing a large number of samples, the user should build a similar training dataset.\n",
    "\n",
    "* **splitting section**: Splits videos to be analyzed into smaller chunks to make analysis feasible.\n",
    "\n",
    "* **tracking section**: Tracks the videos using a Trackmate script.\n",
    "\n",
    "* **MSDs and features calculations**: Calculates MSDs and relevant features and outputs associated files and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updated to make:**\n",
    "\n",
    "Update the bucket name in all functions.\n",
    "Update row and column locations in string in assemble_msds\n",
    "Update the remote folder in relevant sections.\n",
    "Update tracking parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(prefix, folder):\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    bucket='evanepst.data'\n",
    "    remote_folder = folder\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "#     try:\n",
    "#         for name in names:\n",
    "#             aws.download_s3(remote_folder+'/'+name, name, bucket_name=bucket)\n",
    "#     except:\n",
    "    aws.download_s3(remote_name, local_name, bucket_name=bucket)\n",
    "    names = ij.partition_im(local_name)\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    for name in names:\n",
    "        aws.upload_s3(name, remote_folder+'/'+name, bucket_name=bucket)\n",
    "        os.remove(name)\n",
    "        print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_msds(prefix, folder):\n",
    "    \n",
    "    bucket = 'evanepst.data'\n",
    "    remote_folder = folder\n",
    "    #local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "    #MSD and features section\n",
    "    #################################################################################################\n",
    "    files_to_big = False\n",
    "    size_limit = 10\n",
    "\n",
    "    counter = 0\n",
    "    for name in names:\n",
    "        row = int(name.split('.')[0].split('_')[3])\n",
    "        col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "        filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "        aws.download_s3(remote_folder+'/'+filename, filename, bucket_name=bucket)\n",
    "        local_name = filename\n",
    "\n",
    "        if counter == 0:\n",
    "            to_add = ut.csv_to_pd(local_name)\n",
    "            to_add['X'] = to_add['X'] + ires*col\n",
    "            to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "            merged = msd.all_msds2(to_add, frames=frames)\n",
    "        else:\n",
    "\n",
    "            if merged.shape[0] > 0:\n",
    "                to_add = ut.csv_to_pd(local_name)\n",
    "                to_add['X'] = to_add['X'] + ires*col\n",
    "                to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "            else:\n",
    "                to_add = ut.csv_to_pd(local_name)\n",
    "                to_add['X'] = to_add['X'] + ires*col\n",
    "                to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "            merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "            print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "    for name in names:\n",
    "        outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "        os.remove(outfile)\n",
    "\n",
    "    merged.to_csv(msd_file)\n",
    "    aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name=bucket)\n",
    "    merged_ft = ft.calculate_features(merged)\n",
    "    merged_ft.to_csv(ft_file)\n",
    "\n",
    "    aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name=bucket)\n",
    "\n",
    "    os.remove(ft_file)\n",
    "    os.remove(msd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking(subprefix):\n",
    "    \n",
    "    folder = '08_03_18_varying_PEG_excess'\n",
    "    bucket = 'evanepst.data'\n",
    "    \n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    \n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    remote_folder = folder\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(subprefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "    \n",
    "    filename = 'regress4.obj'\n",
    "    aws.download_s3(remote_folder+'/'+filename, filename, bucket_name=bucket)\n",
    "    with open(filename, 'rb') as fp:\n",
    "        regress = joblib.load(fp)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    #Tracking section\n",
    "    ################################################################################################\n",
    "\n",
    "    outfile = 'Traj_' + subprefix + '.csv'\n",
    "    local_im = op.join(local_folder, '{}.tif'.format(subprefix))\n",
    "\n",
    "    row = int(subprefix.split('_')[3])\n",
    "    col = int(subprefix.split('_')[4])\n",
    "\n",
    "    try:\n",
    "        aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name=bucket)\n",
    "    except:\n",
    "        aws.download_s3('{}/{}'.format(remote_folder, '{}.tif'.format(subprefix)), local_im, bucket_name=bucket)        \n",
    "        quality = ij.regress_tracking_params(regress, subprefix, regmethod='PassiveAggressiveRegressor')\n",
    "\n",
    "        if row==3:\n",
    "            y = 485\n",
    "        else:\n",
    "            y = 511\n",
    "\n",
    "        ij.track(local_im, outfile, template=None, fiji_bin=None, radius=3.0, threshold=0.0,\n",
    "                 do_median_filtering=False, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                 linking_max_distance=6.0, gap_closing_max_distance=10.0, max_frame_gap=3,\n",
    "                 track_displacement=20.0)\n",
    "\n",
    "        aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name=bucket)\n",
    "    print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track[19:]:\n",
    "    split(prefix, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split('100nm_10k_PEG_XY20',folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track[19:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloudknot requires a Docker image to load on each machine that is used.  This image has all the required dependencies for the code to run.  The Docker image created is available as 'arokem/python3-fiji:0.3'.  It essentially just includes a Fiji install in the correct location, and points to the correct Github installs.\n",
    "\n",
    "Note: Use \"sudo docker system prune -a\" to clear existing Dockers before creating a new Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=tracking, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-4]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting analysis with Cloudknot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual location where the commands are sent to AWS to start machines and begin the analysis.  The meat of is in the function \"Knot.\"  The user specifies a few essentials:\n",
    "\n",
    "* **name**: The user-defined name of the knot of machines to be started. Used to identify jobs in AWS.\n",
    "* **docker_image**: The Docker image used to initialize each machine.\n",
    "* **memory**: desired memory of each machine to be used.\n",
    "* **resource_type**: in order to get the cheapest machines, I set this to SPOT so we can bid on machines.\n",
    "* **bid_percentage**: in order to ensure I get a machine in each case, I set to 100%.  You can lower this.\n",
    "* **image_id**:\n",
    "* **pars_policies**: I give each machine access to the required S3 bucket here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the image_id has enough space for the job. Currently, this notebook is optimized to require no extra memory,\n",
    "so the default should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for prefix in to_track:    \n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}'.format(prefix, i, j))\n",
    "\n",
    "knot = ck.Knot(name='download_and_track_{}_e{}'.format('Evan', start_knot),\n",
    "               docker_image = my_image,\n",
    "               memory = 16000,\n",
    "               resource_type = \"SPOT\",\n",
    "               bid_percentage = 100,\n",
    "               image_id = 'ami-07afca4c05ab97109', #May need to change this line\n",
    "               pars_policies=('AmazonS3FullAccess',))\n",
    "result_futures = knot.map(names)\n",
    "#print('Successfully started knot for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely shut down all resources started after the analysis, it is good practice to clobber them using the clobber function.  The user can do this manually in the AWS Batch interface as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating MSDs for row 0 and col 1\n",
      "Done calculating MSDs for row 0 and col 2\n",
      "Done calculating MSDs for row 0 and col 3\n",
      "Done calculating MSDs for row 1 and col 0\n",
      "Done calculating MSDs for row 1 and col 1\n",
      "Done calculating MSDs for row 1 and col 2\n",
      "Done calculating MSDs for row 1 and col 3\n",
      "Done calculating MSDs for row 2 and col 0\n",
      "Done calculating MSDs for row 2 and col 1\n",
      "Done calculating MSDs for row 2 and col 2\n",
      "Done calculating MSDs for row 2 and col 3\n",
      "Done calculating MSDs for row 3 and col 0\n",
      "Done calculating MSDs for row 3 and col 1\n",
      "Done calculating MSDs for row 3 and col 2\n",
      "Done calculating MSDs for row 3 and col 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/source/diff-classifier/diff_classifier/features.py:478: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ar = width/height\n",
      "/home/ubuntu/source/diff-classifier/diff_classifier/features.py:671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ratio = (df['MSDs'][n1]/df['MSDs'][n2]) - (df['Frame'][n1]/df['Frame'][n2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/optimize/minpack.py:785: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n"
     ]
    }
   ],
   "source": [
    "for prefix in to_track:\n",
    "    assemble_msds(prefix, folder)\n",
    "    print('Successfully output msds for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '100nm_5k_PEG_XY01'\n",
    "msd = 'msd_{}.csv'.format(prefix)\n",
    "aws.download_s3('{}/{}'.format(folder, msd), msd, bucket_name=bucket)\n",
    "geomean, geoSEM = hm.plot_individual_msds(prefix, x_range=10, y_range=10, remote_folder=folder, bucket=bucket)\n",
    "os.remove(msd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(geomean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix = '100nm_5k_PEG_XY01'\n",
    "for prefix in to_track:\n",
    "    msd = 'msd_{}.csv'.format(prefix)\n",
    "    aws.download_s3('{}/{}'.format(folder, msd), msd, bucket_name=bucket)\n",
    "    geomean, geoSEM = hm.plot_individual_msds(prefix, x_range=10, y_range=10, remote_folder=folder, bucket=bucket)\n",
    "    os.remove(msd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
