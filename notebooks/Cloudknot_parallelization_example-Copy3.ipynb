{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use Cloudknot to parallelize a tracking method.  Example Cloudknot functions are provided in the knotlet module, but the user must build his/her own functions for this step to work properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import cloudknot as ck\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define the nomenclature I use to name my files, as well as specify exceptions (files that weren't generated or are missing and will be skipped in the analysis).  In this case, I was analyzing data collected in tissue slices.  Videos are named according to the pup number, the slice number, the hemisphere, and the video number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = []\n",
    "result_futures = {}\n",
    "start_knot = 10 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "slices = [\"1\", \"2\", \"3\", \"4\", \"5\"] #Number of slices per pup\n",
    "folder = '06_15_18_gel_validation' #Folder in AWS S3 containing files to be analyzed\n",
    "\n",
    "for num in slices:\n",
    "    to_track.append('COOH_t1_XY{}'.format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Cloudknot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below is sent to each individual machine the user calls upon.  A single video is sent to each machine for analysis, and the resulting outputs are uploaded to S3.  This case uses files that are only temporarily stored in a private bucket.  \n",
    "\n",
    "The following function is broken down into four separate sections performing different tasks of the analysis:\n",
    "\n",
    "* **parameter prediction**: A regression tool is used to predict the quality tracking parameter used by Trackmate based off a training dataset of images whose qualities were assessed manually beforehand.  If analyzing a large number of samples, the user should build a similar training dataset.\n",
    "\n",
    "* **splitting section**: Splits videos to be analyzed into smaller chunks to make analysis feasible.\n",
    "\n",
    "* **tracking section**: Tracks the videos using a Trackmate script.\n",
    "\n",
    "* **MSDs and features calculations**: Calculates MSDs and relevant features and outputs associated files and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split_track_msds(prefix):\n",
    "    \"\"\"\n",
    "    1. Checks to see if features file exists.\n",
    "    2. If not, checks to see if image partitioning has occured.\n",
    "    3. If yes, checks to see if tracking has occured.\n",
    "    4. Regardless, tracks, calculates MSDs and features.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    \n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    \n",
    "    folder = '06_15_18_gel_validation'\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = '06_15_18_gel_validation'\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        obj = s3.head_object(Bucket='hpontes.data', Key=remote_folder+'/'+ft_file)\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            for name in names:\n",
    "                aws.download_s3(remote_folder+'/'+name, name, bucket_name='hpontes.data')\n",
    "        except:\n",
    "            aws.download_s3(remote_name, local_name, bucket_name='hpontes.data')\n",
    "            names = ij.partition_im(local_name)\n",
    "            \n",
    "            names = []\n",
    "            for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "            for name in names:\n",
    "                aws.upload_s3(name, remote_folder+'/'+name, bucket_name='hpontes.data')\n",
    "                print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "            os.remove(filename)\n",
    "        #Tracking section\n",
    "        ################################################################################################\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = op.join(local_folder, name)\n",
    "\n",
    "            row = int(name.split('.')[0].split('_')[3])\n",
    "            col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "            try:\n",
    "                aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name='hpontes.data')\n",
    "            except:\n",
    "                test_intensity = ij.mean_intensity(local_im)\n",
    "                if test_intensity > 500:\n",
    "                    quality = 450\n",
    "                else:\n",
    "                    quality = 200\n",
    "                \n",
    "                if row==3:\n",
    "                    y = 485\n",
    "                else:\n",
    "                    y = 511\n",
    "\n",
    "                ij.track(local_im, outfile, template=None, fiji_bin=None, radius=3.5, threshold=0.5,\n",
    "                         do_median_filtering=False, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=4.0, gap_closing_max_distance=7.0, max_frame_gap=2,\n",
    "                         track_displacement=20.0)\n",
    "\n",
    "                aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name='hpontes.data')\n",
    "            print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))\n",
    "\n",
    "\n",
    "        #MSD and features section\n",
    "        #################################################################################################\n",
    "        files_to_big = False\n",
    "        size_limit = 10\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = name\n",
    "            file_size_MB = op.getsize(local_im)/1000000\n",
    "            if file_size_MB > size_limit:\n",
    "                file_to_big = True\n",
    "\n",
    "        if files_to_big:\n",
    "            print('One or more of the {} trajectory files exceeds {}MB in size.  Will not continue with MSD calculations.'.format(\n",
    "                  prefix, size_limit))\n",
    "        else:\n",
    "            counter = 0\n",
    "            for name in names:\n",
    "                row = int(name.split('.')[0].split('_')[3])\n",
    "                col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "                filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "                local_name = local_folder+'/'+filename\n",
    "\n",
    "                if counter == 0:\n",
    "                    to_add = ut.csv_to_pd(local_name)\n",
    "                    to_add['X'] = to_add['X'] + ires*col\n",
    "                    to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                    merged = msd.all_msds2(to_add, frames=frames)\n",
    "                else:\n",
    "\n",
    "                    if merged.shape[0] > 0:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "                    else:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "                    merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "                    print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "                counter = counter + 1\n",
    "\n",
    "            merged.to_csv(msd_file)\n",
    "            aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name='hpontes.data')\n",
    "            merged_ft = ft.calculate_features(merged)\n",
    "            merged_ft.to_csv(ft_file)\n",
    "\n",
    "            aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name='hpontes.data')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpgc53zum9.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_0_0.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpn2e5fugl.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_0_1.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpvjat630m.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_0_2.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpeh_zb9jh.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_0_3.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmp73_yp4m8.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_1_0.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmphnaem_1z.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_1_1.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpne9jcpip.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_1_2.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpl70e07yl.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_1_3.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpf948u3bc.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_2_0.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpy0t79_8x.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_2_1.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpt76tlm6g.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_2_2.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmp6mtq4560.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_2_3.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmp3xxjzpr1.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_3_0.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpaxxfu0sa.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_3_1.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpyhrufcnk.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_3_2.csv\n",
      "/home/ubuntu/Fiji.app/ImageJ-linux64 --ij2 --headless --run /tmp/tmpp0acojpn.py\n",
      "Done with tracking.  Should output file of name 06_15_18_gel_validation/Traj_COOH_t1_XY2_3_3.csv\n",
      "Done calculating MSDs for row 0 and col 1\n",
      "Done calculating MSDs for row 0 and col 2\n",
      "Done calculating MSDs for row 0 and col 3\n",
      "Done calculating MSDs for row 1 and col 0\n",
      "No data in csv file.\n",
      "Done calculating MSDs for row 1 and col 1\n",
      "Done calculating MSDs for row 1 and col 2\n",
      "Done calculating MSDs for row 1 and col 3\n",
      "Done calculating MSDs for row 2 and col 0\n",
      "No data in csv file.\n",
      "Done calculating MSDs for row 2 and col 1\n",
      "Done calculating MSDs for row 2 and col 2\n",
      "Done calculating MSDs for row 2 and col 3\n",
      "Done calculating MSDs for row 3 and col 0\n",
      "Done calculating MSDs for row 3 and col 1\n",
      "Done calculating MSDs for row 3 and col 2\n",
      "Done calculating MSDs for row 3 and col 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/source/diff-classifier/diff_classifier/features.py:671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ratio = (df['MSDs'][n1]/df['MSDs'][n2]) - (df['Frame'][n1]/df['Frame'][n2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/source/diff-classifier/diff_classifier/features.py:478: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  ar = width/height\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/optimize/minpack.py:779: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n",
      "Optimal parameters not found. Print NaN instead.\n"
     ]
    }
   ],
   "source": [
    "download_split_track_msds(to_track[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloudknot requires a Docker image to load on each machine that is used.  This image has all the required dependencies for the code to run.  The Docker image created is available as 'arokem/python3-fiji:0.3'.  It essentially just includes a Fiji install in the correct location, and points to the correct Github installs.\n",
    "\n",
    "Note: Use \"sudo docker system prune -a\" to clear existing Dockers before creating a new Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=download_split_track_msds, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-4]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Docker Image\n",
    "my_image.build(\"0.1\", image_name=\"test_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting analysis with Cloudknot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual location where the commands are sent to AWS to start machines and begin the analysis.  The meat of is in the function \"Knot.\"  The user specifies a few essentials:\n",
    "\n",
    "* **name**: The user-defined name of the knot of machines to be started. Used to identify jobs in AWS.\n",
    "* **docker_image**: The Docker image used to initialize each machine.\n",
    "* **memory**: desired memory of each machine to be used.\n",
    "* **resource_type**: in order to get the cheapest machines, I set this to SPOT so we can bid on machines.\n",
    "* **bid_percentage**: in order to ensure I get a machine in each case, I set to 100%.  You can lower this.\n",
    "* **image_id**:\n",
    "* **pars_policies**: I give each machine access to the required S3 bucket here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot = ck.Knot(name='download_and_track_b{}'.format(start_knot),\n",
    "       docker_image = my_image,\n",
    "       memory = 16000,\n",
    "       resource_type = \"SPOT\",\n",
    "       bid_percentage = 100,\n",
    "       image_id = 'ami-0652d3be6d5566500',\n",
    "       pars_policies=('AmazonS3FullAccess',))\n",
    "result_futures = knot.map(to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely shut down all resources started after the analysis, it is good practice to clobber them using the clobber function.  The user can do this manually in the AWS Batch interface as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_split_track_msds(to_track[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot2 = ck.Knot(name='download_and_track_b2{}'.format(start_knot),\n",
    "       docker_image = my_image,\n",
    "       memory = 16000,\n",
    "       #resource_type = \"SPOT\",\n",
    "       #bid_percentage = 100,\n",
    "       image_id = 'ami-0652d3be6d5566500',\n",
    "       pars_policies=('AmazonS3FullAccess',))\n",
    "result_futures = knot.map(to_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot2.clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
