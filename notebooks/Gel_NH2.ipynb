{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use Cloudknot to parallelize a tracking method.  Example Cloudknot functions are provided in the knotlet module, but the user must build his/her own functions for this step to work properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import cloudknot as ck\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np\n",
    "\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "\n",
    "import diff_classifier.utils as ut\n",
    "import diff_classifier.msd as msd\n",
    "import diff_classifier.features as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define the nomenclature I use to name my files, as well as specify exceptions (files that weren't generated or are missing and will be skipped in the analysis).  In this case, I was analyzing data collected in tissue slices.  Videos are named according to the pup number, the slice number, the hemisphere, and the video number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = []\n",
    "result_futures = {}\n",
    "start_knot = 33 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "slices = [\"1\", \"2\", \"3\", \"4\", \"5\"]#Number of slices per pup\n",
    "well = ['1', '2', '3', '4']\n",
    "folder = '06_15_18_gel_validation' #Folder in AWS S3 containing files to be analyzed\n",
    "\n",
    "for i in slices:\n",
    "    for j in well: \n",
    "        to_track.append('NH2_t{}_XY{}'.format(j, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named _pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c3d8cf2ecbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiff_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknotlets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named _pickle"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "\n",
    "missing = []\n",
    "\n",
    "slices = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "folder = '06_15_18_gel_validation'\n",
    "\n",
    "vids = 5\n",
    "tnum=10 #number of training datasets\n",
    "\n",
    "to_track = []\n",
    "slices = [\"1\", \"2\", \"3\", \"4\", \"5\"]#Number of slices per pup\n",
    "well = ['1', '2', '3', '4']\n",
    "folder = '06_15_18_gel_validation' #Folder in AWS S3 containing files to be analyzed\n",
    "\n",
    "for i in slices:\n",
    "    for j in well: \n",
    "        pref = 'NH2_t{}_XY{}'.format(j, i)\n",
    "        for row in range(0, 4):\n",
    "            for col in range(0, 4):\n",
    "                to_track.append(\"{}_{}_{}\".format(pref, row, col))\n",
    "\n",
    "y = np.array([250, 375, 315, 300, 300, 300, 300, 250, 300, 300]) #5,6,7,9,10\n",
    "\n",
    "# Creates regression object based of training dataset composed of input images and manually\n",
    "# calculated quality cutoffs from tracking with GUI interface.\n",
    "regress = ij.regress_sys(folder, to_track, y, tnum, have_output=True) #Read up on how regress_sys works before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle object\n",
    "filename = 'regress.obj'\n",
    "#filehandler = open(filename, 'w')\n",
    "#pickle.dump(regress, filehandler)\n",
    "\n",
    "with open(filename,'wb') as fp:\n",
    "    pickle.dump(regress,fp)\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "aws.upload_s3(filename, folder+'/'+filename, bucket_name='hpontes.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Cloudknot Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below is sent to each individual machine the user calls upon.  A single video is sent to each machine for analysis, and the resulting outputs are uploaded to S3.  This case uses files that are only temporarily stored in a private bucket.  \n",
    "\n",
    "The following function is broken down into four separate sections performing different tasks of the analysis:\n",
    "\n",
    "* **parameter prediction**: A regression tool is used to predict the quality tracking parameter used by Trackmate based off a training dataset of images whose qualities were assessed manually beforehand.  If analyzing a large number of samples, the user should build a similar training dataset.\n",
    "\n",
    "* **splitting section**: Splits videos to be analyzed into smaller chunks to make analysis feasible.\n",
    "\n",
    "* **tracking section**: Tracks the videos using a Trackmate script.\n",
    "\n",
    "* **MSDs and features calculations**: Calculates MSDs and relevant features and outputs associated files and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(prefix, folder):\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = folder\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        for name in names:\n",
    "            aws.download_s3(remote_folder+'/'+name, name, bucket_name='hpontes.data')\n",
    "    except:\n",
    "        aws.download_s3(remote_name, local_name, bucket_name='hpontes.data')\n",
    "        names = ij.partition_im(local_name)\n",
    "\n",
    "        names = []\n",
    "        for i in range(0, 4):\n",
    "            for j in range(0, 4):\n",
    "                names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "        os.remove(filename)\n",
    "        \n",
    "    for name in names:\n",
    "        aws.upload_s3(name, remote_folder+'/'+name, bucket_name='hpontes.data')\n",
    "        os.remove(name)\n",
    "        print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "    \n",
    "    !rm *.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_msds(prefix, folder):\n",
    "    \n",
    "    remote_folder = folder\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "    #MSD and features section\n",
    "    #################################################################################################\n",
    "\n",
    "    counter = 0\n",
    "    for name in names:\n",
    "        row = int(name.split('.')[0].split('_')[3])\n",
    "        col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "        filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "        aws.download_s3('{}/{}'.format(remote_folder, filename), filename, bucket_name='hpontes.data')\n",
    "        local_name = local_folder+'/'+filename\n",
    "\n",
    "        if counter == 0:\n",
    "            to_add = ut.csv_to_pd(local_name)\n",
    "            to_add['X'] = to_add['X'] + ires*col\n",
    "            to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "            merged = msd.all_msds2(to_add, frames=frames)\n",
    "        else:\n",
    "\n",
    "            if merged.shape[0] > 0:\n",
    "                to_add = ut.csv_to_pd(local_name)\n",
    "                to_add['X'] = to_add['X'] + ires*col\n",
    "                to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "            else:\n",
    "                to_add = ut.csv_to_pd(local_name)\n",
    "                to_add['X'] = to_add['X'] + ires*col\n",
    "                to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "            merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "            print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "        counter = counter + 1\n",
    "\n",
    "\n",
    "    for name in names:\n",
    "        outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "        os.remove(outfile)\n",
    "\n",
    "    merged.to_csv(msd_file)\n",
    "    aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name='hpontes.data')\n",
    "    merged_ft = ft.calculate_features(merged)\n",
    "    merged_ft.to_csv(ft_file)\n",
    "\n",
    "    aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name='hpontes.data')\n",
    "\n",
    "    os.remove(ft_file)\n",
    "    os.remove(msd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking(subprefix):\n",
    "    \n",
    "    folder = '06_15_18_gel_validation'\n",
    "    \n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    \n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    \n",
    "    remote_folder = folder\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(subprefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    #Tracking section\n",
    "    ################################################################################################\n",
    "\n",
    "    outfile = 'Traj_' + subprefix + '.csv'\n",
    "    local_im = op.join(local_folder, '{}.tif'.format(subprefix))\n",
    "\n",
    "    row = int(subprefix.split('_')[3])\n",
    "    col = int(subprefix.split('_')[4])\n",
    "\n",
    "    try:\n",
    "        aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name='hpontes.data')\n",
    "    except:\n",
    "        aws.download_s3('{}/{}'.format(remote_folder, '{}.tif'.format(subprefix)), local_im, bucket_name='hpontes.data')\n",
    "        test_intensity = ij.mean_intensity(local_im)\n",
    "        \n",
    "        filename = 'regress.obj'\n",
    "        aws.download_s3(remote_folder+'/'+filename, filename, bucket_name = 'hpontes.data')\n",
    "        with open(filename, 'rb') as fp:\n",
    "            regress = pickle.load(fp)\n",
    "        \n",
    "        quality = ij.regress_tracking_params(regress, subprefix, regmethod='PassiveAggressiveRegressor')\n",
    "        \n",
    "        if row==3:\n",
    "            y = 485\n",
    "        else:\n",
    "            y = 511\n",
    "\n",
    "        ij.track(local_im, outfile, template=None, fiji_bin=None, radius=3.5, threshold=0.5,\n",
    "                 do_median_filtering=False, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                 linking_max_distance=4.0, gap_closing_max_distance=7.0, max_frame_gap=2,\n",
    "                 track_displacement=20.0)\n",
    "\n",
    "        aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name='hpontes.data')\n",
    "    print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split_track_msds(prefix):\n",
    "    \"\"\"\n",
    "    1. Checks to see if features file exists.\n",
    "    2. If not, checks to see if image partitioning has occured.\n",
    "    3. If yes, checks to see if tracking has occured.\n",
    "    4. Regardless, tracks, calculates MSDs and features.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    \n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    \n",
    "    folder = '06_15_18_gel_validation'\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = '06_15_18_gel_validation'\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        obj = s3.head_object(Bucket='hpontes.data', Key=remote_folder+'/'+ft_file)\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            for name in names:\n",
    "                aws.download_s3(remote_folder+'/'+name, name, bucket_name='hpontes.data')\n",
    "        except:\n",
    "            aws.download_s3(remote_name, local_name, bucket_name='hpontes.data')\n",
    "            names = ij.partition_im(local_name)\n",
    "            \n",
    "            names = []\n",
    "            for i in range(0, 4):\n",
    "                for j in range(0, 4):\n",
    "                    names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "            for name in names:\n",
    "                aws.upload_s3(name, remote_folder+'/'+name, bucket_name='hpontes.data')\n",
    "                print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "            os.remove(filename)\n",
    "        #Tracking section\n",
    "        ################################################################################################\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = op.join(local_folder, name)\n",
    "\n",
    "            row = int(name.split('.')[0].split('_')[3])\n",
    "            col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "            try:\n",
    "                aws.download_s3(remote_folder+'/'+outfile, outfile, bucket_name='hpontes.data')\n",
    "            except:\n",
    "                test_intensity = ij.mean_intensity(local_im)\n",
    "                if test_intensity > 500:\n",
    "                    quality = 450\n",
    "                else:\n",
    "                    quality = 200\n",
    "                \n",
    "                if row==3:\n",
    "                    y = 485\n",
    "                else:\n",
    "                    y = 511\n",
    "\n",
    "                ij.track(local_im, outfile, template=None, fiji_bin=None, radius=3.5, threshold=0.5,\n",
    "                         do_median_filtering=False, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=4.0, gap_closing_max_distance=7.0, max_frame_gap=2,\n",
    "                         track_displacement=20.0)\n",
    "\n",
    "                aws.upload_s3(outfile, remote_folder+'/'+outfile, bucket_name='hpontes.data')\n",
    "            print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))\n",
    "\n",
    "\n",
    "        #MSD and features section\n",
    "        #################################################################################################\n",
    "        files_to_big = False\n",
    "        size_limit = 10\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = name\n",
    "            file_size_MB = op.getsize(local_im)/1000000\n",
    "            if file_size_MB > size_limit:\n",
    "                file_to_big = True\n",
    "\n",
    "        if files_to_big:\n",
    "            print('One or more of the {} trajectory files exceeds {}MB in size.  Will not continue with MSD calculations.'.format(\n",
    "                  prefix, size_limit))\n",
    "        else:\n",
    "            counter = 0\n",
    "            for name in names:\n",
    "                row = int(name.split('.')[0].split('_')[3])\n",
    "                col = int(name.split('.')[0].split('_')[4])\n",
    "\n",
    "                filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "                local_name = local_folder+'/'+filename\n",
    "\n",
    "                if counter == 0:\n",
    "                    to_add = ut.csv_to_pd(local_name)\n",
    "                    to_add['X'] = to_add['X'] + ires*col\n",
    "                    to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                    merged = msd.all_msds2(to_add, frames=frames)\n",
    "                else:\n",
    "\n",
    "                    if merged.shape[0] > 0:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "                    else:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "                    merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "                    print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "                counter = counter + 1\n",
    "\n",
    "            merged.to_csv(msd_file)\n",
    "            aws.upload_s3(msd_file, remote_folder+'/'+msd_file, bucket_name='hpontes.data')\n",
    "            merged_ft = ft.calculate_features(merged)\n",
    "            merged_ft.to_csv(ft_file)\n",
    "\n",
    "            aws.upload_s3(ft_file, remote_folder+'/'+ft_file, bucket_name='hpontes.data')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track:\n",
    "    split(prefix, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Docker Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloudknot requires a Docker image to load on each machine that is used.  This image has all the required dependencies for the code to run.  The Docker image created is available as 'arokem/python3-fiji:0.3'.  It essentially just includes a Fiji install in the correct location, and points to the correct Github installs.\n",
    "\n",
    "Note: Use \"sudo docker system prune -a\" to clear existing Dockers before creating a new Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=tracking, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-4]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting analysis with Cloudknot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual location where the commands are sent to AWS to start machines and begin the analysis.  The meat of is in the function \"Knot.\"  The user specifies a few essentials:\n",
    "\n",
    "* **name**: The user-defined name of the knot of machines to be started. Used to identify jobs in AWS.\n",
    "* **docker_image**: The Docker image used to initialize each machine.\n",
    "* **memory**: desired memory of each machine to be used.\n",
    "* **resource_type**: in order to get the cheapest machines, I set this to SPOT so we can bid on machines.\n",
    "* **bid_percentage**: in order to ensure I get a machine in each case, I set to 100%.  You can lower this.\n",
    "* **image_id**:\n",
    "* **pars_policies**: I give each machine access to the required S3 bucket here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for prefix in to_track:\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}'.format(prefix, i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot = ck.Knot(name='download_and_track_b{}'.format(start_knot),\n",
    "               docker_image = my_image,\n",
    "               memory = 16000,\n",
    "               resource_type = \"SPOT\",\n",
    "               bid_percentage = 100,\n",
    "               image_id = 'ami-0d78cdbdc56922921',\n",
    "               pars_policies=('AmazonS3FullAccess',))\n",
    "result_futures = knot.map(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely shut down all resources started after the analysis, it is good practice to clobber them using the clobber function.  The user can do this manually in the AWS Batch interface as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track:\n",
    "    knot[prefix].clobber()\n",
    "    print('Successfully clobbered resources for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for prefix in to_track:\n",
    "    assemble_msds(prefix, folder)\n",
    "    print('Successfully output msds for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws. down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
