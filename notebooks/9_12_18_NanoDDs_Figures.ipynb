{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the example docs for diff_classifier.  This example will walk you through using diff_classifier to track particle trajectories from a video collected via fluorescence microscopy, or some other imaging technique.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and checking video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly track the video, the user must enter the following relevant information regarding the video to be tracked:\n",
    "* **prefix**: the name of the file to be tracked, minus the file extension.  Typically tif files are the best to track.\n",
    "* **local_folder**: the location of the video to be tracked.\n",
    "* **ires**: the desired resolution of sub-videos to be tracked.  Since the images to be tracked may be too large to analyze all at once, the user may need to crop images to perform correctly.  512 x 512 pixels is the standard size.\n",
    "* **frames**: number of frames contained in the video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from diff_classifier import aws as aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'P1_s2_XY2_v'\n",
    "local_folder = '../diff_classifier/data/Nano_DDS_Figures'\n",
    "rfolder = '08_07_18_P35_PNN_tracking'\n",
    "ires = 512\n",
    "frames = 651\n",
    "aws.download_s3('{}/{}.tif'.format(rfolder, prefix), '{}/{}.tif'.format(local_folder, prefix), bucket_name='hpontes.data')\n",
    "#filename = '{}.tif'.format(prefix)\n",
    "#local_name = op.join(local_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{}.tif'.format(prefix)\n",
    "local_name = op.join(local_folder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = sio.imread(local_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 2047.5, 2043.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.imshow(example[649, :, :], cmap='gray')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting images prior to analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an upper limit on image resolution depending on the available RAM on your machine.  I typically limit analyses to images less than 1024 x 1024 pixels, and usually use 512 x 512 pixel images to be consistent.  The large images from the microscope in our lab are 2048 x 2044, resulting in 16 512 x 512 images, with a black band of 0s of the bottom 4 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.imagej as ij\n",
    "\n",
    "names = ij.partition_im(local_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        names.append('{}_{}_{}'.format(prefix, row, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    aws.upload_s3('{}/{}.tif'.format(local_folder, name), '{}/{}.tif'.format(rfolder, name), bucket_name='hpontes.data')\n",
    "    print(\"Done with splitting.  Should output file of name {}\".format(local_folder+'/'+name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        names.append('{}_{}_{}.tif'.format(prefix, i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle tracking is performed with a Trackmate script as exemplified [here](https://imagej.net/Scripting_TrackMate).  The function track allows the user to select relevant tracking parameters.  Current iteration of the code limits tracking to the DoG detector and the Sparse LAP tracker.  The following tracking parameters must be specified for tracking:\n",
    "* **template**: location of template file containing Trackmate script. Default Trackmate script is used if set to None.\n",
    "* **fiji_bin**: location of Fiji bin.  Fiji is required for tracking.\n",
    "\n",
    "* **radius**: estimated particle radius in pixels. Can take half values.\n",
    "* **threshold**: detection cutoff.  Default is 0.\n",
    "* **do_median_filtering**: if set to True, will perform a median filter before tracking.\n",
    "* **quality**: quality cutoff level.\n",
    "* **x**: Upper x coordinate threshold.  If used, will exclude spots that have x coordinate greater than specified value.\n",
    "* **y**: Upper y coordinate threshold.\n",
    "* **ylo**: Lower y coordinate threshold.\n",
    "* **median_intensity**: currently not used.\n",
    "* **snr**: Signal-to-noise ratio cutoff.  Default is 0.\n",
    "* **linking_max_distance**: max distance in pixels a particle can travel in a single frame.\n",
    "* **gap_closing_max_distance**: max distance in pixels a particle can travel and be picked up by detector when a frame is skipped.\n",
    "* **max_frame_gap**: number of frames a particle can disappear and still be linked.\n",
    "* **track_displacement**: lower duration cutoff in frames. Tracks must be this many frames long to be included in final dataset.\n",
    "\n",
    "Quality is the trickiest parameter to select, and can make or break a successful tracking iteration.  The key benefit to this software package is the ability to track large numbers of files at once.  If your files are similar enough, you can get away with using a single quality value for all videos.  In this case, I used two quality values depending on the mean intensity of the input image, which I found correlated with the best quality values for the tracking settings. In another notebook, I use a training dataset to predict quality based on the input mean intensity of the image to be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names[0].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(names[0].split('_'))\n",
    "print(names[0].split('.')[0].split('_')[4])\n",
    "print(names[0].split('.')[0].split('_')[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.imagej as ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ij.track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "    local_im = op.join(local_folder, name)\n",
    "    \n",
    "    row = int(name.split('.')[0].split('_')[4])\n",
    "    col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "#     try:\n",
    "#         aws.download_s3(remote_folder+'/'+outfile, outfile)\n",
    "#     except:\n",
    "#    test_intensity = ij.mean_intensity(local_im)\n",
    "#    if test_intensity > 500:\n",
    "#        quality = 245\n",
    "#    else:\n",
    "#        quality = 4.5\n",
    "\n",
    "    quality = 3.38\n",
    "    \n",
    "    if row==3:\n",
    "        y = 485\n",
    "    else:\n",
    "        y = 511\n",
    "\n",
    "    ij.track(local_im, outfile, template=None, fiji_bin=None, radius=5.0, threshold=0.0, \n",
    "          do_median_filtering=False, quality=quality, xdims=(0, 511), ydims=(1, y), snr=0.0, \n",
    "          linking_max_distance=10.0, gap_closing_max_distance=12.0, max_frame_gap=2, track_displacement=11.8)\n",
    "\n",
    "    print(\"Done with tracking.  Should output file of name {}\".format(local_folder+'/'+outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('../diff_classifier/data/Nano_DDS_Figures'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading trajectory csv files up to s3\n",
    "for name in names:\n",
    "    outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "    aws.upload_s3(outfile, op.join(rfolder, outfile), bucket_name='hpontes.data')\n",
    "    print(\"Done with uploading.  Should output file of name {}\".format(local_folder+'/'+outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating MSDs and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package uses pandas dataframes to perform MSD and feature calculations.  The first dataframe contains the xy coordinates and the MSDs of each trajectory at each frame.  The second dataframe contains a single row for each trajectory and associated features of each trajectory.\n",
    "\n",
    "This bit of code also only creates a single dataframe for each original input video, rather than creating one for each sub-video after splitting up the images.  They're a lot easier to keep track of that way and minimized total files created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prefix)\n",
    "print(names)\n",
    "print(outfile)\n",
    "print(outfile.split('.')[0].split('_'))\n",
    "print(outfile.split('.')[0].split('_')[5])\n",
    "print(outfile.split('.')[0].split('_')[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.utils as ut\n",
    "import diff_classifier.msd as msd\n",
    "import diff_classifier.features as ft\n",
    "\n",
    "msd_file = 'msd_{}.csv'.format(prefix)\n",
    "ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "counter = 0\n",
    "for name in names:\n",
    "    outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "    row = int(outfile.split('.')[0].split('_')[5])\n",
    "    col = int(outfile.split('.')[0].split('_')[6])\n",
    "\n",
    "    filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "    local_name = filename\n",
    "\n",
    "    if counter == 0:\n",
    "        to_add = ut.csv_to_pd(local_name)\n",
    "        to_add['X'] = to_add['X'] + ires*col\n",
    "        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "        merged = msd.all_msds2(to_add, frames=frames)\n",
    "    else: \n",
    "\n",
    "        if merged.shape[0] > 0:\n",
    "            to_add = ut.csv_to_pd(local_name)\n",
    "            to_add['X'] = to_add['X'] + ires*col\n",
    "            to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "            to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "        else:\n",
    "            to_add = ut.csv_to_pd(local_name)\n",
    "            to_add['X'] = to_add['X'] + ires*col\n",
    "            to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "            to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "        merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "        print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "    counter = counter + 1\n",
    "\n",
    "merged.to_csv(msd_file)\n",
    "#aws.upload_s3(msd_file, op.join(rfolder, msd_file), bucket_name='mckenna.data')\n",
    "merged_ft = ft.calculate_features(merged)\n",
    "merged_ft.to_csv(ft_file)\n",
    "#aws.upload_s3(ft_file, op.join(rfolder, ft_file), bucket_name='mckenna.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws.upload_s3(msd_file, op.join(rfolder, msd_file), bucket_name='hpontes.data')\n",
    "aws.upload_s3(ft_file, op.join(rfolder, ft_file), bucket_name='hpontes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prefix)\n",
    "print(msd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ft_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff_classifier includes several visualization tools for trajectories and features.  First is a map of the trajectories as they appear in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.heatmaps as hm\n",
    "\n",
    "\n",
    "hm.plot_trajectories(prefix, remote_folder=rfolder, bucket='hpontes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.heatmaps_Nano_DDS as hmdd\n",
    "\n",
    "hmdd.plot_trajectories(prefix, remote_folder=rfolder, upload = True, bucket='hpontes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged = pd.read_csv('msd_{}.csv'.format(prefix))\n",
    "particles = int(max(merged['Track_ID']))\n",
    "ires = 512\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "for part in range(0, particles):\n",
    "    x = merged[merged['Track_ID'] == part]['X']\n",
    "    y = merged[merged['Track_ID'] == part]['Y']\n",
    "    plt.plot(x, y, linewidth=2)\n",
    "\n",
    "plt.xlim(ires*1, ires*4)\n",
    "plt.ylim(ires*2, ires*4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the user can plot heatmaps of the trajectories with colormaps scaled to a particular feature.  In this case, the heatmap is scaled to the boundedness of the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature = 'boundedness'\n",
    "hm.plot_heatmap(prefix, feature=feature, vmin=merged_ft[feature].quantile(0.1), vmax=merged_ft[feature].quantile(0.9), remote_folder=rfolder, bucket='mckenna.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information can also be displayed in the form of a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_scatterplot(prefix, feature=feature, vmin=merged_ft[feature].quantile(0.1), vmax=merged_ft[feature].quantile(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can also plot a distribution of the diffusion coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_histogram(prefix, remote_folder=rfolder, bucket='mckenna.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful check for data quality is the number of particles per frame.  Inevitably, this is a decreating function with respect to time lag.  However, if the linking step was performed successfully, the curve shouldn't be too steep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_particles_in_frame(prefix, y_range=10000, remote_folder=rfolder, bucket='mckenna.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the user can plot the MSDs of the individual trajectories.  This function also calculates the geometrically averaged MSDs and the standard deviation of the logs of the MSDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmean1, gSEM1 = hm.plot_individual_msds(prefix, x_range= 2, umppx=0.07, alpha=0.05, remote_folder=rfolder, upload=True, bucket='hpontes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfolder)\n",
    "print(prefix)\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws.upload_s3('./geomean_{}.csv'.format(prefix), rfolder+'/'+'geomean_'+prefix+'.csv', bucket_name='hpontes.data')\n",
    "aws.upload_s3('./geoSEM_{}.csv'.format(prefix), rfolder+'/'+'geoSEM_'+prefix+'.csv', bucket_name='hpontes.data')\n",
    "aws.upload_s3('./msds_{}.png'.format(prefix), rfolder+'/'+'msds_'+prefix+'.png', bucket_name='hpontes.data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
