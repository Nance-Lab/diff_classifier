{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use Cloudknot to parallelize a tracking method.  Example Cloudknot functions are provided in the knotlet module, but the user must build his/her own functions for this step to work properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import cloudknot as ck\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I define the nomenclature I use to name my files, as well as specify exceptions (files that weren't generated or are missing and will be skipped in the analysis).  In this case, I was analyzing data collected in tissue slices.  Videos are named according to the pup number, the slice number, the hemisphere, and the video number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '01_18_Experiment'\n",
    "\n",
    "missing = []\n",
    "for i in range(10, 15):\n",
    "    missing.append(\"P1_S2_R_00{}\".format(i))\n",
    "\n",
    "for i in range(10, 15):\n",
    "    missing.append(\"P2_S3_L_00{}\".format(i))\n",
    "    \n",
    "for i in range(0, 15):\n",
    "    missing.append(\"P3_S3_L_{}\".format(\"%04d\" % i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = {}\n",
    "knot = {}\n",
    "result_futures = {}\n",
    "start_knot = 812\n",
    "\n",
    "pups = [\"P1\"]\n",
    "slices = [\"S1\"]\n",
    "folder = '01_18_Experiment'\n",
    "\n",
    "hemis = [\"R\"]\n",
    "vids = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below is sent to each individual machine the user calls upon.  A single video is sent to each machine for analysis, and the resulting outputs are uploaded to S3.  This case uses files that are only temporarily stored in a private bucket.  \n",
    "\n",
    "The following function is broken down into four separate sections performing different tasks of the analysis:\n",
    "\n",
    "* **parameter prediction**: A regression tool is used to predict the quality tracking parameter used by Trackmate based off a training dataset of images whose qualities were assessed manually beforehand.  If analyzing a large number of samples, the user should build a similar training dataset.\n",
    "\n",
    "* **splitting section**: Splits videos to be analyzed into smaller chunks to make analysis feasible.\n",
    "\n",
    "* **tracking section**: Tracks the videos using a Trackmate script.\n",
    "\n",
    "* **MSDs and features calculations**: Calculates MSDs and relevant features and outputs associated files and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_split_track_msds(prefix):\n",
    "    \"\"\"\n",
    "    1. Checks to see if features file exists.\n",
    "    2. If not, checks to see if image partitioning has occured.\n",
    "    3. If yes, checks to see if tracking has occured.\n",
    "    4. Regardless, tracks, calculates MSDs and features.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    mpl.use('Agg')\n",
    "    import diff_classifier.aws as aws\n",
    "    import diff_classifier.utils as ut\n",
    "    import diff_classifier.msd as msd\n",
    "    import diff_classifier.features as ft\n",
    "    import diff_classifier.imagej as ij\n",
    "    import diff_classifier.heatmaps as hm\n",
    "\n",
    "    from scipy.spatial import Voronoi\n",
    "    import scipy.stats as stats\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    import matplotlib.cm as cm\n",
    "    import os\n",
    "    import os.path as op\n",
    "    import numpy as np\n",
    "    import numpy.ma as ma\n",
    "    import pandas as pd\n",
    "    import boto3\n",
    "    import random\n",
    "\n",
    "    #Building predictor for tracking parameters\n",
    "    ###############################################################################################\n",
    "    \n",
    "    folder = '01_18_Experiment'\n",
    "    missing = []\n",
    "    for i in range(10, 15):\n",
    "        missing.append(\"P1_S2_R_00{}\".format(i))\n",
    "\n",
    "    for i in range(10, 15):\n",
    "        missing.append(\"P2_S3_L_00{}\".format(i))\n",
    "\n",
    "    for i in range(0, 15):\n",
    "        missing.append(\"P3_S3_L_{}\".format(\"%04d\" % i))\n",
    "\n",
    "    pups = [\"P3\", \"P2\", \"P1\"]\n",
    "    slices = [\"S1\", \"S2\", \"S3\"]\n",
    "    folder = '01_18_Experiment'\n",
    "\n",
    "    hemis = [\"R\", \"L\"]\n",
    "    vids = 15\n",
    "    tnum=20 #number of training datasets\n",
    "    \n",
    "    to_track = []\n",
    "    for pup in pups:\n",
    "        for hemi in hemis:\n",
    "                for slic in slices:\n",
    "                    for vid in range(0, vids):\n",
    "                                pref = \"{}_{}_{}_{}\".format(pup, slic, hemi, \"%04d\" % vid)\n",
    "                                if not pref in missing:\n",
    "                                    for row in range(0, 4):\n",
    "                                        for col in range(0, 4):\n",
    "                                            to_track.append(\"{}_{}_{}\".format(pref, row, col))\n",
    "\n",
    "    y = np.array([1.5, 833.6, 9.24, 4.5, 3.3, 3.4, 2.85, 2.75, 3.7, 2.45,\n",
    "                  2.6, 3.1, 3.25, 2.85, 2.05, 2.9, 5.4, 5.0, 2.7, 530])\n",
    "    \n",
    "    # Creates regression object based of training dataset composed of input images and manually\n",
    "    # calculated quality cutoffs from tracking with GUI interface.\n",
    "    regress = ij.regress_sys(folder, to_track, y, tnum, have_output=True)\n",
    "\n",
    "    #Splitting section\n",
    "    ###############################################################################################\n",
    "    remote_folder = \"01_18_Experiment/{}\".format(prefix.split('_')[0])\n",
    "    local_folder = os.getcwd()\n",
    "    ires = 512\n",
    "    frames = 651\n",
    "    filename = '{}.tif'.format(prefix)\n",
    "    remote_name = remote_folder+'/'+filename\n",
    "    local_name = local_folder+'/'+filename\n",
    "\n",
    "    msd_file = 'msd_{}.csv'.format(prefix)\n",
    "    ft_file = 'features_{}.csv'.format(prefix)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    names = []\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}.tif'.format(prefix, i, j))\n",
    "\n",
    "    try:\n",
    "        obj = s3.head_object(Bucket='ccurtis7.pup', Key=remote_folder+'/'+ft_file)\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            for name in names:\n",
    "                aws.download_s3(remote_folder+'/'+name, name)\n",
    "        except:\n",
    "            aws.download_s3(remote_name, local_name)\n",
    "            names = ij.partition_im(local_name)\n",
    "            for name in names:\n",
    "                aws.upload_s3(name, remote_folder+'/'+name)\n",
    "                print(\"Done with splitting.  Should output file of name {}\".format(remote_folder+'/'+name))\n",
    "\n",
    "        #Tracking section\n",
    "        ################################################################################################\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = op.join(local_folder, name)\n",
    "\n",
    "            row = int(name.split('.')[0].split('_')[4])\n",
    "            col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "            try:\n",
    "                aws.download_s3(remote_folder+'/'+outfile, outfile)\n",
    "            except:\n",
    "\n",
    "                quality = ij.regress_tracking_params(regress, name.split('.')[0], regmethod='PassiveAggressiveRegressor')\n",
    "                \n",
    "                if row==3:\n",
    "                    y = 485\n",
    "                else:\n",
    "                    y = 511\n",
    "\n",
    "                ij.track(local_im, outfile, template=None, fiji_bin=None, radius=4.5, threshold=0.,\n",
    "                         do_median_filtering=True, quality=quality, x=511, y=y, ylo=1, median_intensity=300.0, snr=0.0,\n",
    "                         linking_max_distance=8.0, gap_closing_max_distance=10.0, max_frame_gap=2,\n",
    "                         track_displacement=10.0)\n",
    "\n",
    "                aws.upload_s3(outfile, remote_folder+'/'+outfile)\n",
    "            print(\"Done with tracking.  Should output file of name {}\".format(remote_folder+'/'+outfile))\n",
    "\n",
    "\n",
    "        #MSD and features section\n",
    "        #################################################################################################\n",
    "        files_to_big = False\n",
    "        size_limit = 10\n",
    "\n",
    "        for name in names:\n",
    "            outfile = 'Traj_' + name.split('.')[0] + '.csv'\n",
    "            local_im = name\n",
    "            file_size_MB = op.getsize(local_im)/1000000\n",
    "            if file_size_MB > size_limit:\n",
    "                file_to_big = True\n",
    "\n",
    "        if files_to_big:\n",
    "            print('One or more of the {} trajectory files exceeds {}MB in size.  Will not continue with MSD calculations.'.format(\n",
    "                  prefix, size_limit))\n",
    "        else:\n",
    "            counter = 0\n",
    "            for name in names:\n",
    "                row = int(name.split('.')[0].split('_')[4])\n",
    "                col = int(name.split('.')[0].split('_')[5])\n",
    "\n",
    "                filename = \"Traj_{}_{}_{}.csv\".format(prefix, row, col)\n",
    "                local_name = local_folder+'/'+filename\n",
    "\n",
    "                if counter == 0:\n",
    "                    to_add = ut.csv_to_pd(local_name)\n",
    "                    to_add['X'] = to_add['X'] + ires*col\n",
    "                    to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                    merged = msd.all_msds2(to_add, frames=frames)\n",
    "                else:\n",
    "\n",
    "                    if merged.shape[0] > 0:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID'] + max(merged['Track_ID']) + 1\n",
    "                    else:\n",
    "                        to_add = ut.csv_to_pd(local_name)\n",
    "                        to_add['X'] = to_add['X'] + ires*col\n",
    "                        to_add['Y'] = ires - to_add['Y'] + ires*(3-row)\n",
    "                        to_add['Track_ID'] = to_add['Track_ID']\n",
    "\n",
    "                    merged = merged.append(msd.all_msds2(to_add, frames=frames))\n",
    "                    print('Done calculating MSDs for row {} and col {}'.format(row, col))\n",
    "                counter = counter + 1\n",
    "\n",
    "            merged.to_csv(msd_file)\n",
    "            aws.upload_s3(msd_file, remote_folder+'/'+msd_file)\n",
    "            merged_ft = ft.calculate_features(merged)\n",
    "            merged_ft.to_csv(ft_file)\n",
    "\n",
    "            aws.upload_s3(ft_file, remote_folder+'/'+ft_file)\n",
    "\n",
    "            #Plots\n",
    "            features = ('AR', 'D_fit', 'alpha', 'MSD_ratio', 'Track_ID', 'X', 'Y', 'asymmetry1', 'asymmetry2', 'asymmetry3',\n",
    "                        'boundedness', 'efficiency', 'elongation', 'fractal_dim', 'frames', 'kurtosis', 'straightness', 'trappedness')\n",
    "            vmin = (1.36, 0.015, 0.72, -0.09, 0, 0, 0, 0.5, 0.049, 0.089, 0.0069, 0.65, 0.26, 1.28, 0, 1.66, 0.087, -0.225)\n",
    "            vmax = (3.98, 2.6, 2.3, 0.015, max(merged_ft['Track_ID']), 2048, 2048, 0.99, 0.415, 0.53,\n",
    "                    0.062, 3.44, 0.75, 1.79, 650, 3.33, 0.52, -0.208)\n",
    "            die = {'features': features,\n",
    "                   'vmin': vmin,\n",
    "                   'vmax': vmax}\n",
    "            di = pd.DataFrame(data=die)\n",
    "            for i in range(0, di.shape[0]):\n",
    "                hm.plot_heatmap(prefix, feature=di['features'][i], vmin=di['vmin'][i], vmax=di['vmax'][i])\n",
    "                hm.plot_scatterplot(prefix, feature=di['features'][i], vmin=di['vmin'][i], vmax=di['vmax'][i])\n",
    "\n",
    "            hm.plot_trajectories(prefix)\n",
    "            try:\n",
    "                hm.plot_histogram(prefix)\n",
    "            except ValueError:\n",
    "                print(\"Couldn't plot histogram.\")\n",
    "            hm.plot_particles_in_frame(prefix)\n",
    "            gmean1, gSEM1 = hm.plot_individual_msds(prefix, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloudknot requires a Docker image to load on each machine that is used.  This image has all the required dependencies for the code to run.  The Docker image created is available as 'arokem/python3-fiji:0.3'.  It essentially just includes a Fiji install in the correct location, and points to the correct Github installs.\n",
    "\n",
    "Note: Use \"docker system prune\" to clear existing Dockers before creating a new Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=download_split_track_msds, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-3]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Docker Image\n",
    "my_image.build(\"0.1\", image_name=\"test_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual location where the commands are sent to AWS to start machines and begin the analysis.  The meat of is in the function \"Knot.\"  The user specifies a few essentials:\n",
    "\n",
    "* **name**: The user-defined name of the knot of machines to be started. Used to identify jobs in AWS.\n",
    "* **docker_image**: The Docker image used to initialize each machine.\n",
    "* **memory**: desired memory of each machine to be used.\n",
    "* **resource_type**: in order to get the cheapest machines, I set this to SPOT so we can bid on machines.\n",
    "* **bid_percentage**: in order to ensure I get a machine in each case, I set to 100%.  You can lower this.\n",
    "* **image_id**:\n",
    "* **pars_policies**: I give each machine access to the required S3 bucket here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pup in pups:\n",
    "    for hemi in hemis:\n",
    "\n",
    "            run_name = '{}_{}'.format(pup, hemi)\n",
    "            to_track[run_name] = []\n",
    "            for slic in slices:\n",
    "                for vid in range(0, vids):\n",
    "                            prefix = \"{}_{}_{}_{}\".format(pup, slic, hemi, \"%04d\" % vid)\n",
    "                            if not prefix in missing:\n",
    "                                to_track[run_name].append(prefix)\n",
    "\n",
    "            test_length = len(to_track[run_name])\n",
    "            print('Number of nodes to be loaded: {}'.format(test_length))\n",
    "            \n",
    "            knot[run_name] = ck.Knot(name='download_and_track_{}'.format(start_knot),\n",
    "                           docker_image = my_image,\n",
    "                           memory = 32000,\n",
    "                           resource_type = \"SPOT\",\n",
    "                           bid_percentage = 100,\n",
    "                           image_id = 'ami-6d8a7510',\n",
    "                           pars_policies=('AmazonS3FullAccess',))\n",
    "            result_futures[run_name] = knot[run_name].map(to_track[run_name])\n",
    "            start_knot = start_knot + 1\n",
    "            print('Next knot name: {}'.format(start_knot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To completely shut down all resources started after the analysis, it is good practice to clobber them using the clobber function.  The user can do this manually in the AWS Batch interface as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in knot:\n",
    "    knot[key].clobber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    self.events.trigger('post_run_cell')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/events.py\", line 74, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/pylab/backend_inline.py\", line 160, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ubuntu/.local/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/ubuntu/.local/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "download_split_track_msds('P2_S2_R_0005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
