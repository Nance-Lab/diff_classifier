{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.pca as pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip filename features_100nm_XY01.csv\n",
      "features_100nm_XY02.csv size: (18207, 67)\n",
      "features_100nm_XY03.csv size: (18178, 67)\n",
      "features_100nm_XY04.csv size: (20452, 67)\n",
      "features_100nm_XY05.csv size: (19292, 67)\n",
      "skip filename features_100nm_XY06.csv\n",
      "skip filename features_100nm_XY07.csv\n",
      "skip filename features_100nm_XY08.csv\n",
      "features_100nm_XY09.csv size: (21766, 67)\n",
      "skip filename features_100nm_XY10.csv\n",
      "features_200nm_XY01.csv size: (6072, 67)\n",
      "features_200nm_XY02.csv size: (6483, 67)\n",
      "features_200nm_XY03.csv size: (6513, 67)\n",
      "features_200nm_XY04.csv size: (6412, 67)\n",
      "features_200nm_XY05.csv size: (5249, 67)\n",
      "features_200nm_XY06.csv size: (5248, 67)\n",
      "features_200nm_XY07.csv size: (4674, 67)\n",
      "features_200nm_XY08.csv size: (5585, 67)\n",
      "features_200nm_XY09.csv size: (5700, 67)\n",
      "features_200nm_XY10.csv size: (5198, 67)\n",
      "features_500nm_XY01.csv size: (5325, 67)\n",
      "features_500nm_XY02.csv size: (4709, 67)\n",
      "features_500nm_XY03.csv size: (4125, 67)\n",
      "features_500nm_XY04.csv size: (1606, 67)\n",
      "features_500nm_XY05.csv size: (5067, 67)\n",
      "features_500nm_XY06.csv size: (6013, 67)\n",
      "features_500nm_XY07.csv size: (10970, 67)\n",
      "features_500nm_XY08.csv size: (7364, 67)\n",
      "features_500nm_XY09.csv size: (5843, 67)\n",
      "features_500nm_XY10.csv size: (4019, 67)\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "remote_folder = 'Gel_Studies/08_14_18_gel_validation' #Folder in AWS S3 containing files to be analyzed\n",
    "bucket = 'ccurtis.data'\n",
    "vids = 10\n",
    "sizes = ['100', '200', '500']\n",
    "nonnum = ['Particle Size', 'Video Number', 'Track_ID', 'Mean Mean_Intensity', 'Std Mean_Intensity']\n",
    "featofvar = 'Particle Size'\n",
    "\n",
    "counter = 0\n",
    "for size in sizes:\n",
    "    for num in range(1, vids+1):\n",
    "        try:\n",
    "            filename = 'features_{}nm_XY{}.csv'.format(size, '%02d' % num)\n",
    "            aws.download_s3('{}/{}'.format(remote_folder, filename), filename, bucket_name='ccurtis.data')\n",
    "            fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "            print('{} size: {}'.format(filename, fstats.shape))\n",
    "            fstats['Particle Size'] = pd.Series(fstats.shape[0]*[size], index=fstats.index)\n",
    "            fstats['Video Number'] = pd.Series(fstats.shape[0]*[num], index=fstats.index)\n",
    "            counter = counter + 1\n",
    "            if counter == 1:\n",
    "                fstats_tot = fstats\n",
    "            else:\n",
    "                fstats_tot = fstats_tot.append(fstats, ignore_index=True)\n",
    "        except:\n",
    "            print('skip filename {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA analyses with too many datapoints fail. You get rows with lots of NAs. I'm going to try making a subset of the data first\n",
    "#and then do a PCA analysis on that.\n",
    "\n",
    "#include all in analysis\n",
    "import random\n",
    "subset = np.sort(np.array(random.sample(range(fstats_tot.shape[0]), 500000)))\n",
    "fstats_sub = fstats_tot.loc[subset, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97895, 69)\n",
      "(57134, 69)\n",
      "(55041, 69)\n"
     ]
    }
   ],
   "source": [
    "for typ in fstats_tot['Particle Size'].unique():\n",
    "    fstats_type = fstats_tot[fstats_tot['Particle Size']==typ].reset_index(drop=True)\n",
    "    print(fstats_type.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97895, 69)\n",
      "(57134, 69)\n",
      "(55041, 69)\n"
     ]
    }
   ],
   "source": [
    "#with equal sample sizes for each particle type\n",
    "import random\n",
    "counter = 0\n",
    "for typ in fstats_tot['Particle Size'].unique():\n",
    "    fstats_type = fstats_tot[fstats_tot['Particle Size']==typ].reset_index(drop=True)\n",
    "    print(fstats_type.shape)\n",
    "    subset = np.sort(np.array(random.sample(range(fstats_type.shape[0]), 55000)))\n",
    "    if counter == 0:\n",
    "        fstats_sub = fstats_type.loc[subset, :].reset_index(drop=True)\n",
    "    else:\n",
    "        fstats_sub = fstats_sub.append(fstats_type.loc[subset, :].reset_index(drop=True), ignore_index=True)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "fstats_num = fstats_sub.drop(nonnum, axis=1)\n",
    "fstats_raw = fstats_num.as_matrix()\n",
    "#fstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.pca_analysis function provides a completely contained PCA analysis of the input trajectory features dataset. It includes options to impute NaN values (fill in with average values or drop them), and to scale features. Read the docstring for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance:\n",
      "0 component: 0.26456811998350804\n",
      "1 component: 0.3910484322933804\n",
      "2 component: 0.4607176185277395\n",
      "3 component: 0.516093472861476\n",
      "4 component: 0.5613799711978695\n",
      "5 component: 0.5941750536673687\n",
      "6 component: 0.6251942133120792\n",
      "7 component: 0.6556791619009862\n",
      "8 component: 0.6838689881887454\n",
      "9 component: 0.7103888195561195\n",
      "10 component: 0.7336712357280057\n",
      "11 component: 0.7544367580800567\n",
      "12 component: 0.7735633061254458\n",
      "13 component: 0.7913058624090679\n",
      "14 component: 0.8070761980081702\n"
     ]
    }
   ],
   "source": [
    "pcadataset = pca.pca_analysis(fstats_tot, dropcols=nonnum, n_components=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.kmo function calculates the Kaiser-Meyer-Olkin statistic, a measure of sampling adequacy. Check the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmostat = pca.kmo(pcadataset.scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stat\n",
    "stat.bartlett(pcadataset.scaled[0, :], pcadataset.scaled[1, :], pcadataset.scaled[2, :], pcadataset.scaled[3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstr = ''\n",
    "for i in range(pcadataset.scaled.shape[0]-1):\n",
    "    newstr = newstr + 'pcadataset.scaled[{}, :], '.format(i)\n",
    "\n",
    "newstr = 'stat.bartlett(' + newstr + 'pcadataset.scaled[{}, :])'.format(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = eval(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can then compare average principle component values between subgroups of the data. In this case, all particles were taken from the same sample, so there are no experimental subgroups. I chose to compare short trajectories to long trajectories, as I would expect differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ncomp = 14\n",
    "dicti = {}\n",
    "#test = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==200].as_matrix()), axis=0))[-6:]\n",
    "#test1 = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==500].as_matrix()), axis=0))[-6:]\n",
    "dicti[0] = np.nanmean(pcadataset.final[pcadataset.final['Particle Size']=='100'].values[:, -ncomp:], axis=0)\n",
    "dicti[1] = np.nanmean(pcadataset.final[pcadataset.final['Particle Size']=='200'].values[:, -ncomp:], axis=0)\n",
    "dicti[2] = np.nanmean(pcadataset.final[pcadataset.final['Particle Size']=='500'].values[:, -ncomp:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.plot_pca(dicti, savefig=True, labels=['100nm', '200nm', '500nm'], rticks=np.linspace(-4, 4, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable pcadataset.prcomps shows the user the major contributions to each of the new principle components. When observing the graph above, users can see that there are some differences between short trajectories and long trajectories in component 0 (asymmetry1 being the major contributor) and component 1 (elongation being the major contributor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.prcomps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['100', '200', '500']\n",
    "feats = pca.feature_violin(pcadataset.final, label='Particle Size', lvals=labels, fsubset=14, yrange=[-12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats1 = pca.feature_plot_3D(pcadataset.final, label='Particle Size', lvals=labels, randcount=400, ylim=[-12, 12],\n",
    "                              xlim=[-12, 12], zlim=[-12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp = 14\n",
    "trainp = np.array([])\n",
    "testp = np.array([])\n",
    "\n",
    "for i in range(0, 20):\n",
    "    KNNmod, X, y = pca.build_model(pcadataset.final, 'Particle Size', labels, equal_sampling=True,\n",
    "                                       tsize=500, input_cols=ncomp, model='MLP', NNhidden_layer=(6, 2))\n",
    "    trainp = np.append(trainp, pca.predict_model(KNNmod, X, y))\n",
    "    \n",
    "    X2 = pcadataset.final.values[:, -ncomp:]\n",
    "    y2 = pcadataset.final['Particle Size'].values\n",
    "    testp = np.append(testp, pca.predict_model(KNNmod, X2, y2))\n",
    "    print('Run {}: {}'.format(i, testp[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} +/ {}'.format(np.mean(trainp), np.std(trainp)))\n",
    "print('{} +/ {}'.format(np.mean(testp), np.std(testp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "Iteration 1, loss = 0.25054738\n",
      "Iteration 2, loss = 0.18236173\n",
      "Iteration 3, loss = 0.16504100\n",
      "Iteration 4, loss = 0.15503288\n",
      "Iteration 5, loss = 0.14841344\n",
      "Iteration 6, loss = 0.14321087\n",
      "Iteration 7, loss = 0.13970694\n",
      "Iteration 8, loss = 0.13643168\n",
      "Iteration 9, loss = 0.13311474\n",
      "Iteration 10, loss = 0.13144194\n",
      "Iteration 11, loss = 0.12827315\n",
      "Iteration 12, loss = 0.12654112\n",
      "Iteration 13, loss = 0.12461385\n",
      "Iteration 14, loss = 0.12327265\n",
      "Iteration 15, loss = 0.12191221\n",
      "Iteration 16, loss = 0.12082347\n",
      "Iteration 17, loss = 0.11936773\n",
      "Iteration 18, loss = 0.11757675\n",
      "Iteration 19, loss = 0.11683063\n",
      "Iteration 20, loss = 0.11549540\n",
      "Iteration 21, loss = 0.11472881\n",
      "Iteration 22, loss = 0.11330495\n",
      "Iteration 23, loss = 0.11217252\n",
      "Iteration 24, loss = 0.11104188\n",
      "Iteration 25, loss = 0.10929297\n",
      "Iteration 26, loss = 0.10872444\n",
      "Iteration 27, loss = 0.10784758\n",
      "Iteration 28, loss = 0.10738449\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25353292\n",
      "Iteration 2, loss = 0.18528011\n",
      "Iteration 3, loss = 0.16795163\n",
      "Iteration 4, loss = 0.15817257\n",
      "Iteration 5, loss = 0.15105729\n",
      "Iteration 6, loss = 0.14570837\n",
      "Iteration 7, loss = 0.14212300\n",
      "Iteration 8, loss = 0.13852881\n",
      "Iteration 9, loss = 0.13430301\n",
      "Iteration 10, loss = 0.13171489\n",
      "Iteration 11, loss = 0.13036574\n",
      "Iteration 12, loss = 0.12766457\n",
      "Iteration 13, loss = 0.12564467\n",
      "Iteration 14, loss = 0.12376931\n",
      "Iteration 15, loss = 0.12186452\n",
      "Iteration 16, loss = 0.11997132\n",
      "Iteration 17, loss = 0.11873915\n",
      "Iteration 18, loss = 0.11780870\n",
      "Iteration 19, loss = 0.11575459\n",
      "Iteration 20, loss = 0.11424567\n",
      "Iteration 21, loss = 0.11398530\n",
      "Iteration 22, loss = 0.11209707\n",
      "Iteration 23, loss = 0.11084166\n",
      "Iteration 24, loss = 0.10947137\n",
      "Iteration 25, loss = 0.10774286\n",
      "Iteration 26, loss = 0.10732831\n",
      "Iteration 27, loss = 0.10522672\n",
      "Iteration 28, loss = 0.10495325\n",
      "Iteration 29, loss = 0.10371184\n",
      "Iteration 30, loss = 0.10306707\n",
      "Iteration 31, loss = 0.10180694\n",
      "Iteration 32, loss = 0.10076747\n",
      "Iteration 33, loss = 0.10008528\n",
      "Iteration 34, loss = 0.09945952\n",
      "Iteration 35, loss = 0.09796294\n",
      "Iteration 36, loss = 0.09708255\n",
      "Iteration 37, loss = 0.09645178\n",
      "Iteration 38, loss = 0.09583107\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25054949\n",
      "Iteration 2, loss = 0.18480421\n",
      "Iteration 3, loss = 0.16747691\n",
      "Iteration 4, loss = 0.15780483\n",
      "Iteration 5, loss = 0.15103335\n",
      "Iteration 6, loss = 0.14616735\n",
      "Iteration 7, loss = 0.14281869\n",
      "Iteration 8, loss = 0.13945512\n",
      "Iteration 9, loss = 0.13556631\n",
      "Iteration 10, loss = 0.13359118\n",
      "Iteration 11, loss = 0.13161328\n",
      "Iteration 12, loss = 0.12920904\n",
      "Iteration 13, loss = 0.12745665\n",
      "Iteration 14, loss = 0.12587098\n",
      "Iteration 15, loss = 0.12394025\n",
      "Iteration 16, loss = 0.12264521\n",
      "Iteration 17, loss = 0.12166397\n",
      "Iteration 18, loss = 0.12004910\n",
      "Iteration 19, loss = 0.11817653\n",
      "Iteration 20, loss = 0.11797275\n",
      "Iteration 21, loss = 0.11584778\n",
      "Iteration 22, loss = 0.11453220\n",
      "Iteration 23, loss = 0.11284500\n",
      "Iteration 24, loss = 0.11327101\n",
      "Iteration 25, loss = 0.11207796\n",
      "Iteration 26, loss = 0.10980989\n",
      "Iteration 27, loss = 0.10996696\n",
      "Iteration 28, loss = 0.10785022\n",
      "Iteration 29, loss = 0.10699558\n",
      "Iteration 30, loss = 0.10590706\n",
      "Iteration 31, loss = 0.10613799\n",
      "Iteration 32, loss = 0.10428202\n",
      "Iteration 33, loss = 0.10348365\n",
      "Iteration 34, loss = 0.10319380\n",
      "Iteration 35, loss = 0.10237834\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25670478\n",
      "Iteration 2, loss = 0.18575883\n",
      "Iteration 3, loss = 0.16867579\n",
      "Iteration 4, loss = 0.15820839\n",
      "Iteration 5, loss = 0.15130473\n",
      "Iteration 6, loss = 0.14618261\n",
      "Iteration 7, loss = 0.14202713\n",
      "Iteration 8, loss = 0.13844352\n",
      "Iteration 9, loss = 0.13532094\n",
      "Iteration 10, loss = 0.13274988\n",
      "Iteration 11, loss = 0.13066949\n",
      "Iteration 12, loss = 0.12817559\n",
      "Iteration 13, loss = 0.12679905\n",
      "Iteration 14, loss = 0.12493286\n",
      "Iteration 15, loss = 0.12376923\n",
      "Iteration 16, loss = 0.12259621\n",
      "Iteration 17, loss = 0.11972436\n",
      "Iteration 18, loss = 0.11910054\n",
      "Iteration 19, loss = 0.11585521\n",
      "Iteration 20, loss = 0.11465840\n",
      "Iteration 21, loss = 0.11452457\n",
      "Iteration 22, loss = 0.11262394\n",
      "Iteration 23, loss = 0.11150597\n",
      "Iteration 24, loss = 0.11035721\n",
      "Iteration 25, loss = 0.10999971\n",
      "Iteration 26, loss = 0.10866790\n",
      "Iteration 27, loss = 0.10631607\n",
      "Iteration 28, loss = 0.10716683\n",
      "Iteration 29, loss = 0.10499261\n",
      "Iteration 30, loss = 0.10451977\n",
      "Iteration 31, loss = 0.10397557\n",
      "Iteration 32, loss = 0.10229671\n",
      "Iteration 33, loss = 0.10147472\n",
      "Iteration 34, loss = 0.10076295\n",
      "Iteration 35, loss = 0.10021663\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25321257\n",
      "Iteration 2, loss = 0.18320023\n",
      "Iteration 3, loss = 0.16600177\n",
      "Iteration 4, loss = 0.15664404\n",
      "Iteration 5, loss = 0.14933176\n",
      "Iteration 6, loss = 0.14520462\n",
      "Iteration 7, loss = 0.13958536\n",
      "Iteration 8, loss = 0.13675046\n",
      "Iteration 9, loss = 0.13430301\n",
      "Iteration 10, loss = 0.13111553\n",
      "Iteration 11, loss = 0.12889628\n",
      "Iteration 12, loss = 0.12671232\n",
      "Iteration 13, loss = 0.12456099\n",
      "Iteration 14, loss = 0.12368277\n",
      "Iteration 15, loss = 0.12267955\n",
      "Iteration 16, loss = 0.12026121\n",
      "Iteration 17, loss = 0.11831613\n",
      "Iteration 18, loss = 0.11683068\n",
      "Iteration 19, loss = 0.11552981\n",
      "Iteration 20, loss = 0.11501233\n",
      "Iteration 21, loss = 0.11389862\n",
      "Iteration 22, loss = 0.11207593\n",
      "Iteration 23, loss = 0.11078377\n",
      "Iteration 24, loss = 0.10956599\n",
      "Iteration 25, loss = 0.10903811\n",
      "Iteration 26, loss = 0.10825196\n",
      "Iteration 27, loss = 0.10740095\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20514972\n",
      "Iteration 2, loss = 0.15904194\n",
      "Iteration 3, loss = 0.14911129\n",
      "Iteration 4, loss = 0.14018583\n",
      "Iteration 5, loss = 0.13518836\n",
      "Iteration 6, loss = 0.12977620\n",
      "Iteration 7, loss = 0.12810127\n",
      "Iteration 8, loss = 0.12606799\n",
      "Iteration 9, loss = 0.12255121\n",
      "Iteration 10, loss = 0.11758068\n",
      "Iteration 11, loss = 0.11638845\n",
      "Iteration 12, loss = 0.11499020\n",
      "Iteration 13, loss = 0.11254396\n",
      "Iteration 14, loss = 0.11110683\n",
      "Iteration 15, loss = 0.10891147\n",
      "Iteration 16, loss = 0.10596151\n",
      "Iteration 17, loss = 0.10708665\n",
      "Iteration 18, loss = 0.10189177\n",
      "Iteration 19, loss = 0.10332219\n",
      "Iteration 20, loss = 0.10077014\n",
      "Iteration 21, loss = 0.10079099\n",
      "Iteration 22, loss = 0.09893832\n",
      "Iteration 23, loss = 0.09655628\n",
      "Iteration 24, loss = 0.09509704\n",
      "Iteration 25, loss = 0.09664286\n",
      "Iteration 26, loss = 0.09602364\n",
      "Iteration 27, loss = 0.09321759\n",
      "Iteration 28, loss = 0.09326081\n",
      "Iteration 29, loss = 0.09222467\n",
      "Iteration 30, loss = 0.09018953\n",
      "Iteration 31, loss = 0.09284170\n",
      "Iteration 32, loss = 0.08875594\n",
      "Iteration 33, loss = 0.09369863\n",
      "Iteration 34, loss = 0.08831827\n",
      "Iteration 35, loss = 0.08657899\n",
      "Iteration 36, loss = 0.08778643\n",
      "Iteration 37, loss = 0.08621001\n",
      "Iteration 38, loss = 0.08774190\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20409048\n",
      "Iteration 2, loss = 0.16180008\n",
      "Iteration 3, loss = 0.14873083\n",
      "Iteration 4, loss = 0.14204757\n",
      "Iteration 5, loss = 0.13615670\n",
      "Iteration 6, loss = 0.13192732\n",
      "Iteration 7, loss = 0.12907584\n",
      "Iteration 8, loss = 0.12749184\n",
      "Iteration 9, loss = 0.12418076\n",
      "Iteration 10, loss = 0.11996845\n",
      "Iteration 11, loss = 0.11692952\n",
      "Iteration 12, loss = 0.11531270\n",
      "Iteration 13, loss = 0.11477172\n",
      "Iteration 14, loss = 0.11402788\n",
      "Iteration 15, loss = 0.10952831\n",
      "Iteration 16, loss = 0.10703073\n",
      "Iteration 17, loss = 0.10516393\n",
      "Iteration 18, loss = 0.10390787\n",
      "Iteration 19, loss = 0.10404884\n",
      "Iteration 20, loss = 0.10322842\n",
      "Iteration 21, loss = 0.09918998\n",
      "Iteration 22, loss = 0.09982210\n",
      "Iteration 23, loss = 0.09958523\n",
      "Iteration 24, loss = 0.09781603\n",
      "Iteration 25, loss = 0.09674489\n",
      "Iteration 26, loss = 0.09473314\n",
      "Iteration 27, loss = 0.09432309\n",
      "Iteration 28, loss = 0.09310575\n",
      "Iteration 29, loss = 0.08999197\n",
      "Iteration 30, loss = 0.09077362\n",
      "Iteration 31, loss = 0.09301890\n",
      "Iteration 32, loss = 0.09131882\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20744646\n",
      "Iteration 2, loss = 0.16198472\n",
      "Iteration 3, loss = 0.15111305\n",
      "Iteration 4, loss = 0.14421882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.13944336\n",
      "Iteration 6, loss = 0.13536899\n",
      "Iteration 7, loss = 0.13241964\n",
      "Iteration 8, loss = 0.12939042\n",
      "Iteration 9, loss = 0.12580249\n",
      "Iteration 10, loss = 0.12443062\n",
      "Iteration 11, loss = 0.12247168\n",
      "Iteration 12, loss = 0.12064826\n",
      "Iteration 13, loss = 0.11923097\n",
      "Iteration 14, loss = 0.11776167\n",
      "Iteration 15, loss = 0.11606307\n",
      "Iteration 16, loss = 0.11411704\n",
      "Iteration 17, loss = 0.11269806\n",
      "Iteration 18, loss = 0.11018172\n",
      "Iteration 19, loss = 0.10784147\n",
      "Iteration 20, loss = 0.10820775\n",
      "Iteration 21, loss = 0.10553736\n",
      "Iteration 22, loss = 0.10396691\n",
      "Iteration 23, loss = 0.10419894\n",
      "Iteration 24, loss = 0.10314862\n",
      "Iteration 25, loss = 0.09998507\n",
      "Iteration 26, loss = 0.10184477\n",
      "Iteration 27, loss = 0.09864337\n",
      "Iteration 28, loss = 0.09870547\n",
      "Iteration 29, loss = 0.09581197\n",
      "Iteration 30, loss = 0.09412246\n",
      "Iteration 31, loss = 0.09669805\n",
      "Iteration 32, loss = 0.09466465\n",
      "Iteration 33, loss = 0.09654565\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20540757\n",
      "Iteration 2, loss = 0.16220176\n",
      "Iteration 3, loss = 0.14997703\n",
      "Iteration 4, loss = 0.14187230\n",
      "Iteration 5, loss = 0.13691997\n",
      "Iteration 6, loss = 0.13291802\n",
      "Iteration 7, loss = 0.12861293\n",
      "Iteration 8, loss = 0.12838410\n",
      "Iteration 9, loss = 0.12229344\n",
      "Iteration 10, loss = 0.12092858\n",
      "Iteration 11, loss = 0.11871520\n",
      "Iteration 12, loss = 0.11584109\n",
      "Iteration 13, loss = 0.11488590\n",
      "Iteration 14, loss = 0.11103105\n",
      "Iteration 15, loss = 0.10958705\n",
      "Iteration 16, loss = 0.10873175\n",
      "Iteration 17, loss = 0.10692231\n",
      "Iteration 18, loss = 0.10590447\n",
      "Iteration 19, loss = 0.10611279\n",
      "Iteration 20, loss = 0.10626827\n",
      "Iteration 21, loss = 0.10348213\n",
      "Iteration 22, loss = 0.10121527\n",
      "Iteration 23, loss = 0.10372072\n",
      "Iteration 24, loss = 0.09893846\n",
      "Iteration 25, loss = 0.10178018\n",
      "Iteration 26, loss = 0.09631068\n",
      "Iteration 27, loss = 0.09476033\n",
      "Iteration 28, loss = 0.09447452\n",
      "Iteration 29, loss = 0.09623698\n",
      "Iteration 30, loss = 0.09107791\n",
      "Iteration 31, loss = 0.09300641\n",
      "Iteration 32, loss = 0.09156729\n",
      "Iteration 33, loss = 0.09267849\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20573641\n",
      "Iteration 2, loss = 0.16235391\n",
      "Iteration 3, loss = 0.15164645\n",
      "Iteration 4, loss = 0.14375566\n",
      "Iteration 5, loss = 0.13726020\n",
      "Iteration 6, loss = 0.13023828\n",
      "Iteration 7, loss = 0.12892822\n",
      "Iteration 8, loss = 0.12544427\n",
      "Iteration 9, loss = 0.12243821\n",
      "Iteration 10, loss = 0.11873205\n",
      "Iteration 11, loss = 0.11586544\n",
      "Iteration 12, loss = 0.11487238\n",
      "Iteration 13, loss = 0.11219899\n",
      "Iteration 14, loss = 0.11121945\n",
      "Iteration 15, loss = 0.11051188\n",
      "Iteration 16, loss = 0.10570788\n",
      "Iteration 17, loss = 0.10471429\n",
      "Iteration 18, loss = 0.10537952\n",
      "Iteration 19, loss = 0.10149094\n",
      "Iteration 20, loss = 0.10032268\n",
      "Iteration 21, loss = 0.10021758\n",
      "Iteration 22, loss = 0.09620198\n",
      "Iteration 23, loss = 0.09826963\n",
      "Iteration 24, loss = 0.09784195\n",
      "Iteration 25, loss = 0.09656507\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19840146\n",
      "Iteration 2, loss = 0.16229656\n",
      "Iteration 3, loss = 0.15319179\n",
      "Iteration 4, loss = 0.14633944\n",
      "Iteration 5, loss = 0.14491550\n",
      "Iteration 6, loss = 0.13802292\n",
      "Iteration 7, loss = 0.13728690\n",
      "Iteration 8, loss = 0.13115265\n",
      "Iteration 9, loss = 0.13074026\n",
      "Iteration 10, loss = 0.12907869\n",
      "Iteration 11, loss = 0.12525707\n",
      "Iteration 12, loss = 0.12415479\n",
      "Iteration 13, loss = 0.12247343\n",
      "Iteration 14, loss = 0.12356476\n",
      "Iteration 15, loss = 0.11963578\n",
      "Iteration 16, loss = 0.11680627\n",
      "Iteration 17, loss = 0.11608650\n",
      "Iteration 18, loss = 0.11465111\n",
      "Iteration 19, loss = 0.11674341\n",
      "Iteration 20, loss = 0.11296881\n",
      "Iteration 21, loss = 0.10854133\n",
      "Iteration 22, loss = 0.11438752\n",
      "Iteration 23, loss = 0.11220984\n",
      "Iteration 24, loss = 0.10997449\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20143990\n",
      "Iteration 2, loss = 0.16052452\n",
      "Iteration 3, loss = 0.15108556\n",
      "Iteration 4, loss = 0.14494753\n",
      "Iteration 5, loss = 0.14007873\n",
      "Iteration 6, loss = 0.13610106\n",
      "Iteration 7, loss = 0.13452428\n",
      "Iteration 8, loss = 0.13259688\n",
      "Iteration 9, loss = 0.12942288\n",
      "Iteration 10, loss = 0.12614003\n",
      "Iteration 11, loss = 0.12609048\n",
      "Iteration 12, loss = 0.12568643\n",
      "Iteration 13, loss = 0.12668777\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20334228\n",
      "Iteration 2, loss = 0.16450314\n",
      "Iteration 3, loss = 0.15512079\n",
      "Iteration 4, loss = 0.14684026\n",
      "Iteration 5, loss = 0.14343887\n",
      "Iteration 6, loss = 0.13924562\n",
      "Iteration 7, loss = 0.13709730\n",
      "Iteration 8, loss = 0.13430712\n",
      "Iteration 9, loss = 0.13350251\n",
      "Iteration 10, loss = 0.12798415\n",
      "Iteration 11, loss = 0.12908344\n",
      "Iteration 12, loss = 0.12724089\n",
      "Iteration 13, loss = 0.12501315\n",
      "Iteration 14, loss = 0.12281861\n",
      "Iteration 15, loss = 0.12130321\n",
      "Iteration 16, loss = 0.11910805\n",
      "Iteration 17, loss = 0.11899620\n",
      "Iteration 18, loss = 0.11949585\n",
      "Iteration 19, loss = 0.11915940\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20243819\n",
      "Iteration 2, loss = 0.16408416\n",
      "Iteration 3, loss = 0.15485018\n",
      "Iteration 4, loss = 0.14652769\n",
      "Iteration 5, loss = 0.14340318\n",
      "Iteration 6, loss = 0.14111771\n",
      "Iteration 7, loss = 0.13554308\n",
      "Iteration 8, loss = 0.13042719\n",
      "Iteration 9, loss = 0.12974631\n",
      "Iteration 10, loss = 0.12790147\n",
      "Iteration 11, loss = 0.12440798\n",
      "Iteration 12, loss = 0.12361400\n",
      "Iteration 13, loss = 0.12272764\n",
      "Iteration 14, loss = 0.12153220\n",
      "Iteration 15, loss = 0.12374278\n",
      "Iteration 16, loss = 0.11855991\n",
      "Iteration 17, loss = 0.11891816\n",
      "Iteration 18, loss = 0.11811908\n",
      "Iteration 19, loss = 0.12300148\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20270382\n",
      "Iteration 2, loss = 0.16164296\n",
      "Iteration 3, loss = 0.15255603\n",
      "Iteration 4, loss = 0.14580293\n",
      "Iteration 5, loss = 0.13927118\n",
      "Iteration 6, loss = 0.13743243\n",
      "Iteration 7, loss = 0.13292569\n",
      "Iteration 8, loss = 0.13187024\n",
      "Iteration 9, loss = 0.12850977\n",
      "Iteration 10, loss = 0.12914385\n",
      "Iteration 11, loss = 0.12234542\n",
      "Iteration 12, loss = 0.12367006\n",
      "Iteration 13, loss = 0.11957908\n",
      "Iteration 14, loss = 0.11952514\n",
      "Iteration 15, loss = 0.11477531\n",
      "Iteration 16, loss = 0.11777106\n",
      "Iteration 17, loss = 0.11833028\n",
      "Iteration 18, loss = 0.11381425\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36691212\n",
      "Iteration 2, loss = 0.24445098\n",
      "Iteration 3, loss = 0.21906812\n",
      "Iteration 4, loss = 0.20416154\n",
      "Iteration 5, loss = 0.19429811\n",
      "Iteration 6, loss = 0.18700966\n",
      "Iteration 7, loss = 0.18083582\n",
      "Iteration 8, loss = 0.17569117\n",
      "Iteration 9, loss = 0.17135477\n",
      "Iteration 10, loss = 0.16720411\n",
      "Iteration 11, loss = 0.16399952\n",
      "Iteration 12, loss = 0.16111027\n",
      "Iteration 13, loss = 0.15852168\n",
      "Iteration 14, loss = 0.15599689\n",
      "Iteration 15, loss = 0.15391152\n",
      "Iteration 16, loss = 0.15203314\n",
      "Iteration 17, loss = 0.15000898\n",
      "Iteration 18, loss = 0.14845505\n",
      "Iteration 19, loss = 0.14650869\n",
      "Iteration 20, loss = 0.14562248\n",
      "Iteration 21, loss = 0.14389082\n",
      "Iteration 22, loss = 0.14245708\n",
      "Iteration 23, loss = 0.14115244\n",
      "Iteration 24, loss = 0.14030049\n",
      "Iteration 25, loss = 0.13902499\n",
      "Iteration 26, loss = 0.13807539\n",
      "Iteration 27, loss = 0.13694475\n",
      "Iteration 28, loss = 0.13611624\n",
      "Iteration 29, loss = 0.13500558\n",
      "Iteration 30, loss = 0.13431100\n",
      "Iteration 31, loss = 0.13321482\n",
      "Iteration 32, loss = 0.13279764\n",
      "Iteration 33, loss = 0.13175690\n",
      "Iteration 34, loss = 0.13108178\n",
      "Iteration 35, loss = 0.13057258\n",
      "Iteration 36, loss = 0.12940082\n",
      "Iteration 37, loss = 0.12881997\n",
      "Iteration 38, loss = 0.12843750\n",
      "Iteration 39, loss = 0.12792820\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37248939\n",
      "Iteration 2, loss = 0.24762530\n",
      "Iteration 3, loss = 0.22238667\n",
      "Iteration 4, loss = 0.20746004\n",
      "Iteration 5, loss = 0.19678706\n",
      "Iteration 6, loss = 0.18922798\n",
      "Iteration 7, loss = 0.18292624\n",
      "Iteration 8, loss = 0.17724795\n",
      "Iteration 9, loss = 0.17271186\n",
      "Iteration 10, loss = 0.16887108\n",
      "Iteration 11, loss = 0.16549227\n",
      "Iteration 12, loss = 0.16234011\n",
      "Iteration 13, loss = 0.15949354\n",
      "Iteration 14, loss = 0.15681882\n",
      "Iteration 15, loss = 0.15452229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.15280372\n",
      "Iteration 17, loss = 0.15073838\n",
      "Iteration 18, loss = 0.14873853\n",
      "Iteration 19, loss = 0.14725079\n",
      "Iteration 20, loss = 0.14567698\n",
      "Iteration 21, loss = 0.14421754\n",
      "Iteration 22, loss = 0.14285180\n",
      "Iteration 23, loss = 0.14148313\n",
      "Iteration 24, loss = 0.14024714\n",
      "Iteration 25, loss = 0.13889672\n",
      "Iteration 26, loss = 0.13817828\n",
      "Iteration 27, loss = 0.13703391\n",
      "Iteration 28, loss = 0.13633068\n",
      "Iteration 29, loss = 0.13510597\n",
      "Iteration 30, loss = 0.13398805\n",
      "Iteration 31, loss = 0.13306014\n",
      "Iteration 32, loss = 0.13243624\n",
      "Iteration 33, loss = 0.13164675\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36609016\n",
      "Iteration 2, loss = 0.24986886\n",
      "Iteration 3, loss = 0.22435209\n",
      "Iteration 4, loss = 0.20949986\n",
      "Iteration 5, loss = 0.19891032\n",
      "Iteration 6, loss = 0.19090286\n",
      "Iteration 7, loss = 0.18444138\n",
      "Iteration 8, loss = 0.17928359\n",
      "Iteration 9, loss = 0.17468068\n",
      "Iteration 10, loss = 0.17065730\n",
      "Iteration 11, loss = 0.16736507\n",
      "Iteration 12, loss = 0.16424966\n",
      "Iteration 13, loss = 0.16119655\n",
      "Iteration 14, loss = 0.15915135\n",
      "Iteration 15, loss = 0.15656750\n",
      "Iteration 16, loss = 0.15443272\n",
      "Iteration 17, loss = 0.15253658\n",
      "Iteration 18, loss = 0.15081704\n",
      "Iteration 19, loss = 0.14921645\n",
      "Iteration 20, loss = 0.14759116\n",
      "Iteration 21, loss = 0.14603723\n",
      "Iteration 22, loss = 0.14466921\n",
      "Iteration 23, loss = 0.14364962\n",
      "Iteration 24, loss = 0.14226456\n",
      "Iteration 25, loss = 0.14092844\n",
      "Iteration 26, loss = 0.13982146\n",
      "Iteration 27, loss = 0.13898429\n",
      "Iteration 28, loss = 0.13791789\n",
      "Iteration 29, loss = 0.13679352\n",
      "Iteration 30, loss = 0.13614243\n",
      "Iteration 31, loss = 0.13512569\n",
      "Iteration 32, loss = 0.13432470\n",
      "Iteration 33, loss = 0.13350487\n",
      "Iteration 34, loss = 0.13240434\n",
      "Iteration 35, loss = 0.13193440\n",
      "Iteration 36, loss = 0.13057922\n",
      "Iteration 37, loss = 0.13030054\n",
      "Iteration 38, loss = 0.12983487\n",
      "Iteration 39, loss = 0.12870797\n",
      "Iteration 40, loss = 0.12800244\n",
      "Iteration 41, loss = 0.12740286\n",
      "Iteration 42, loss = 0.12702760\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35955213\n",
      "Iteration 2, loss = 0.24815346\n",
      "Iteration 3, loss = 0.22263158\n",
      "Iteration 4, loss = 0.20788114\n",
      "Iteration 5, loss = 0.19726663\n",
      "Iteration 6, loss = 0.18996307\n",
      "Iteration 7, loss = 0.18314887\n",
      "Iteration 8, loss = 0.17830547\n",
      "Iteration 9, loss = 0.17364332\n",
      "Iteration 10, loss = 0.17019673\n",
      "Iteration 11, loss = 0.16643327\n",
      "Iteration 12, loss = 0.16368273\n",
      "Iteration 13, loss = 0.16090574\n",
      "Iteration 14, loss = 0.15846121\n",
      "Iteration 15, loss = 0.15593610\n",
      "Iteration 16, loss = 0.15414632\n",
      "Iteration 17, loss = 0.15297325\n",
      "Iteration 18, loss = 0.15055579\n",
      "Iteration 19, loss = 0.14889833\n",
      "Iteration 20, loss = 0.14754653\n",
      "Iteration 21, loss = 0.14598009\n",
      "Iteration 22, loss = 0.14457671\n",
      "Iteration 23, loss = 0.14361237\n",
      "Iteration 24, loss = 0.14209982\n",
      "Iteration 25, loss = 0.14099076\n",
      "Iteration 26, loss = 0.14002959\n",
      "Iteration 27, loss = 0.13888348\n",
      "Iteration 28, loss = 0.13787407\n",
      "Iteration 29, loss = 0.13727566\n",
      "Iteration 30, loss = 0.13600646\n",
      "Iteration 31, loss = 0.13512109\n",
      "Iteration 32, loss = 0.13455816\n",
      "Iteration 33, loss = 0.13346388\n",
      "Iteration 34, loss = 0.13264169\n",
      "Iteration 35, loss = 0.13211220\n",
      "Iteration 36, loss = 0.13129222\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35377593\n",
      "Iteration 2, loss = 0.24358927\n",
      "Iteration 3, loss = 0.21892648\n",
      "Iteration 4, loss = 0.20457575\n",
      "Iteration 5, loss = 0.19452311\n",
      "Iteration 6, loss = 0.18713292\n",
      "Iteration 7, loss = 0.18111741\n",
      "Iteration 8, loss = 0.17620116\n",
      "Iteration 9, loss = 0.17189996\n",
      "Iteration 10, loss = 0.16822022\n",
      "Iteration 11, loss = 0.16500798\n",
      "Iteration 12, loss = 0.16184309\n",
      "Iteration 13, loss = 0.15949722\n",
      "Iteration 14, loss = 0.15688692\n",
      "Iteration 15, loss = 0.15502179\n",
      "Iteration 16, loss = 0.15295581\n",
      "Iteration 17, loss = 0.15130138\n",
      "Iteration 18, loss = 0.14927686\n",
      "Iteration 19, loss = 0.14759115\n",
      "Iteration 20, loss = 0.14607887\n",
      "Iteration 21, loss = 0.14459893\n",
      "Iteration 22, loss = 0.14331701\n",
      "Iteration 23, loss = 0.14186970\n",
      "Iteration 24, loss = 0.14094407\n",
      "Iteration 25, loss = 0.13971488\n",
      "Iteration 26, loss = 0.13872835\n",
      "Iteration 27, loss = 0.13761964\n",
      "Iteration 28, loss = 0.13657321\n",
      "Iteration 29, loss = 0.13547910\n",
      "Iteration 30, loss = 0.13473754\n",
      "Iteration 31, loss = 0.13369860\n",
      "Iteration 32, loss = 0.13314296\n",
      "Iteration 33, loss = 0.13216654\n",
      "Iteration 34, loss = 0.13144051\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25966125\n",
      "Iteration 2, loss = 0.18168008\n",
      "Iteration 3, loss = 0.16537677\n",
      "Iteration 4, loss = 0.15452219\n",
      "Iteration 5, loss = 0.14655392\n",
      "Iteration 6, loss = 0.14262976\n",
      "Iteration 7, loss = 0.13757841\n",
      "Iteration 8, loss = 0.13461941\n",
      "Iteration 9, loss = 0.13151511\n",
      "Iteration 10, loss = 0.12886214\n",
      "Iteration 11, loss = 0.12597498\n",
      "Iteration 12, loss = 0.12340154\n",
      "Iteration 13, loss = 0.12245608\n",
      "Iteration 14, loss = 0.12030134\n",
      "Iteration 15, loss = 0.11898956\n",
      "Iteration 16, loss = 0.11653213\n",
      "Iteration 17, loss = 0.11561819\n",
      "Iteration 18, loss = 0.11470013\n",
      "Iteration 19, loss = 0.11279644\n",
      "Iteration 20, loss = 0.12169607\n",
      "Iteration 21, loss = 0.11127792\n",
      "Iteration 22, loss = 0.11060393\n",
      "Iteration 23, loss = 0.10896187\n",
      "Iteration 24, loss = 0.10728751\n",
      "Iteration 25, loss = 0.10592356\n",
      "Iteration 26, loss = 0.10439918\n",
      "Iteration 27, loss = 0.10349306\n",
      "Iteration 28, loss = 0.10262230\n",
      "Iteration 29, loss = 0.10127284\n",
      "Iteration 30, loss = 0.10253774\n",
      "Iteration 31, loss = 0.10403864\n",
      "Iteration 32, loss = 0.09813207\n",
      "Iteration 33, loss = 0.09790300\n",
      "Iteration 34, loss = 0.09693670\n",
      "Iteration 35, loss = 0.09495158\n",
      "Iteration 36, loss = 0.09410665\n",
      "Iteration 37, loss = 0.09366315\n",
      "Iteration 38, loss = 0.09218741\n",
      "Iteration 39, loss = 0.09203756\n",
      "Iteration 40, loss = 0.09138316\n",
      "Iteration 41, loss = 0.08940865\n",
      "Iteration 42, loss = 0.08719030\n",
      "Iteration 43, loss = 0.08776819\n",
      "Iteration 44, loss = 0.08635277\n",
      "Iteration 45, loss = 0.08729766\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25761535\n",
      "Iteration 2, loss = 0.18216276\n",
      "Iteration 3, loss = 0.16477221\n",
      "Iteration 4, loss = 0.15495918\n",
      "Iteration 5, loss = 0.14811506\n",
      "Iteration 6, loss = 0.14258612\n",
      "Iteration 7, loss = 0.13740816\n",
      "Iteration 8, loss = 0.13389765\n",
      "Iteration 9, loss = 0.13160483\n",
      "Iteration 10, loss = 0.12957920\n",
      "Iteration 11, loss = 0.12677317\n",
      "Iteration 12, loss = 0.12436063\n",
      "Iteration 13, loss = 0.12262165\n",
      "Iteration 14, loss = 0.12057517\n",
      "Iteration 15, loss = 0.12206390\n",
      "Iteration 16, loss = 0.11686613\n",
      "Iteration 17, loss = 0.11575807\n",
      "Iteration 18, loss = 0.11475815\n",
      "Iteration 19, loss = 0.11284375\n",
      "Iteration 20, loss = 0.11122482\n",
      "Iteration 21, loss = 0.11000867\n",
      "Iteration 22, loss = 0.10802606\n",
      "Iteration 23, loss = 0.11044097\n",
      "Iteration 24, loss = 0.10647382\n",
      "Iteration 25, loss = 0.10445784\n",
      "Iteration 26, loss = 0.10292994\n",
      "Iteration 27, loss = 0.10264146\n",
      "Iteration 28, loss = 0.10167947\n",
      "Iteration 29, loss = 0.10030547\n",
      "Iteration 30, loss = 0.09934669\n",
      "Iteration 31, loss = 0.09957237\n",
      "Iteration 32, loss = 0.09691874\n",
      "Iteration 33, loss = 0.09722245\n",
      "Iteration 34, loss = 0.09426704\n",
      "Iteration 35, loss = 0.09328099\n",
      "Iteration 36, loss = 0.09373468\n",
      "Iteration 37, loss = 0.09179946\n",
      "Iteration 38, loss = 0.09167875\n",
      "Iteration 39, loss = 0.09036951\n",
      "Iteration 40, loss = 0.08873816\n",
      "Iteration 41, loss = 0.08923802\n",
      "Iteration 42, loss = 0.08740913\n",
      "Iteration 43, loss = 0.08642387\n",
      "Iteration 44, loss = 0.08488219\n",
      "Iteration 45, loss = 0.08402835\n",
      "Iteration 46, loss = 0.08284099\n",
      "Iteration 47, loss = 0.08342219\n",
      "Iteration 48, loss = 0.08193393\n",
      "Iteration 49, loss = 0.07991458\n",
      "Iteration 50, loss = 0.07950278\n",
      "Iteration 51, loss = 0.07959601\n",
      "Iteration 52, loss = 0.07828762\n",
      "Iteration 53, loss = 0.07851214\n",
      "Iteration 54, loss = 0.07713405\n",
      "Iteration 55, loss = 0.07601278\n",
      "Iteration 56, loss = 0.07615542\n",
      "Iteration 57, loss = 0.07574744\n",
      "Iteration 58, loss = 0.07362061\n",
      "Iteration 59, loss = 0.07377699\n",
      "Iteration 60, loss = 0.07502211\n",
      "Iteration 61, loss = 0.07216033\n",
      "Iteration 62, loss = 0.07148948\n",
      "Iteration 63, loss = 0.06980068\n",
      "Iteration 64, loss = 0.06994822\n",
      "Iteration 65, loss = 0.06824147\n",
      "Iteration 66, loss = 0.07040735\n",
      "Iteration 67, loss = 0.06732446\n",
      "Iteration 68, loss = 0.06674661\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.26191126\n",
      "Iteration 2, loss = 0.18393519\n",
      "Iteration 3, loss = 0.16651969\n",
      "Iteration 4, loss = 0.15619565\n",
      "Iteration 5, loss = 0.14921579\n",
      "Iteration 6, loss = 0.14491918\n",
      "Iteration 7, loss = 0.14024210\n",
      "Iteration 8, loss = 0.13637587\n",
      "Iteration 9, loss = 0.13272773\n",
      "Iteration 10, loss = 0.13045855\n",
      "Iteration 11, loss = 0.12852012\n",
      "Iteration 12, loss = 0.12558858\n",
      "Iteration 13, loss = 0.12402452\n",
      "Iteration 14, loss = 0.12177813\n",
      "Iteration 15, loss = 0.11952543\n",
      "Iteration 16, loss = 0.11873914\n",
      "Iteration 17, loss = 0.11668980\n",
      "Iteration 18, loss = 0.11565666\n",
      "Iteration 19, loss = 0.11433375\n",
      "Iteration 20, loss = 0.11154202\n",
      "Iteration 21, loss = 0.11273792\n",
      "Iteration 22, loss = 0.10928185\n",
      "Iteration 23, loss = 0.10848260\n",
      "Iteration 24, loss = 0.10910280\n",
      "Iteration 25, loss = 0.10578949\n",
      "Iteration 26, loss = 0.10536928\n",
      "Iteration 27, loss = 0.10336957\n",
      "Iteration 28, loss = 0.10234112\n",
      "Iteration 29, loss = 0.10194729\n",
      "Iteration 30, loss = 0.10107773\n",
      "Iteration 31, loss = 0.09906916\n",
      "Iteration 32, loss = 0.09865211\n",
      "Iteration 33, loss = 0.09744059\n",
      "Iteration 34, loss = 0.09669943\n",
      "Iteration 35, loss = 0.09682334\n",
      "Iteration 36, loss = 0.09451129\n",
      "Iteration 37, loss = 0.09456358\n",
      "Iteration 38, loss = 0.09409685\n",
      "Iteration 39, loss = 0.09152733\n",
      "Iteration 40, loss = 0.09148979\n",
      "Iteration 41, loss = 0.09043705\n",
      "Iteration 42, loss = 0.08896358\n",
      "Iteration 43, loss = 0.09005744\n",
      "Iteration 44, loss = 0.08764091\n",
      "Iteration 45, loss = 0.08713643\n",
      "Iteration 46, loss = 0.08744281\n",
      "Iteration 47, loss = 0.08509463\n",
      "Iteration 48, loss = 0.08540024\n",
      "Iteration 49, loss = 0.08537655\n",
      "Iteration 50, loss = 0.08609094\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25225892\n",
      "Iteration 2, loss = 0.18257393\n",
      "Iteration 3, loss = 0.16535596\n",
      "Iteration 4, loss = 0.15543999\n",
      "Iteration 5, loss = 0.14772618\n",
      "Iteration 6, loss = 0.14314672\n",
      "Iteration 7, loss = 0.13767371\n",
      "Iteration 8, loss = 0.13551035\n",
      "Iteration 9, loss = 0.13203899\n",
      "Iteration 10, loss = 0.13149501\n",
      "Iteration 11, loss = 0.12685622\n",
      "Iteration 12, loss = 0.12525956\n",
      "Iteration 13, loss = 0.12222750\n",
      "Iteration 14, loss = 0.12146974\n",
      "Iteration 15, loss = 0.11946019\n",
      "Iteration 16, loss = 0.11813040\n",
      "Iteration 17, loss = 0.11581867\n",
      "Iteration 18, loss = 0.11431530\n",
      "Iteration 19, loss = 0.11262454\n",
      "Iteration 20, loss = 0.11209231\n",
      "Iteration 21, loss = 0.11100236\n",
      "Iteration 22, loss = 0.10957838\n",
      "Iteration 23, loss = 0.10703346\n",
      "Iteration 24, loss = 0.10595924\n",
      "Iteration 25, loss = 0.10443106\n",
      "Iteration 26, loss = 0.10341417\n",
      "Iteration 27, loss = 0.10288576\n",
      "Iteration 28, loss = 0.10070710\n",
      "Iteration 29, loss = 0.10031989\n",
      "Iteration 30, loss = 0.09889509\n",
      "Iteration 31, loss = 0.09704187\n",
      "Iteration 32, loss = 0.09579768\n",
      "Iteration 33, loss = 0.09642599\n",
      "Iteration 34, loss = 0.09570131\n",
      "Iteration 35, loss = 0.09384387\n",
      "Iteration 36, loss = 0.09356003\n",
      "Iteration 37, loss = 0.09168478\n",
      "Iteration 38, loss = 0.09221418\n",
      "Iteration 39, loss = 0.09036784\n",
      "Iteration 40, loss = 0.09048401\n",
      "Iteration 41, loss = 0.08782245\n",
      "Iteration 42, loss = 0.08736617\n",
      "Iteration 43, loss = 0.08633940\n",
      "Iteration 44, loss = 0.08549646\n",
      "Iteration 45, loss = 0.08363092\n",
      "Iteration 46, loss = 0.08381384\n",
      "Iteration 47, loss = 0.08291491\n",
      "Iteration 48, loss = 0.08343371\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26272090\n",
      "Iteration 2, loss = 0.18462746\n",
      "Iteration 3, loss = 0.16552339\n",
      "Iteration 4, loss = 0.15590933\n",
      "Iteration 5, loss = 0.14897915\n",
      "Iteration 6, loss = 0.14174452\n",
      "Iteration 7, loss = 0.13845645\n",
      "Iteration 8, loss = 0.13461496\n",
      "Iteration 9, loss = 0.13094202\n",
      "Iteration 10, loss = 0.12832576\n",
      "Iteration 11, loss = 0.12561915\n",
      "Iteration 12, loss = 0.12319548\n",
      "Iteration 13, loss = 0.12107002\n",
      "Iteration 14, loss = 0.11938340\n",
      "Iteration 15, loss = 0.11762025\n",
      "Iteration 16, loss = 0.11601093\n",
      "Iteration 17, loss = 0.11394279\n",
      "Iteration 18, loss = 0.11291200\n",
      "Iteration 19, loss = 0.11238675\n",
      "Iteration 20, loss = 0.11051648\n",
      "Iteration 21, loss = 0.10862714\n",
      "Iteration 22, loss = 0.10824355\n",
      "Iteration 23, loss = 0.10552096\n",
      "Iteration 24, loss = 0.10436851\n",
      "Iteration 25, loss = 0.10300344\n",
      "Iteration 26, loss = 0.10318868\n",
      "Iteration 27, loss = 0.10143347\n",
      "Iteration 28, loss = 0.10061166\n",
      "Iteration 29, loss = 0.09937672\n",
      "Iteration 30, loss = 0.09831725\n",
      "Iteration 31, loss = 0.09722362\n",
      "Iteration 32, loss = 0.09564258\n",
      "Iteration 33, loss = 0.09430903\n",
      "Iteration 34, loss = 0.09445819\n",
      "Iteration 35, loss = 0.09208025\n",
      "Iteration 36, loss = 0.09103756\n",
      "Iteration 37, loss = 0.09096023\n",
      "Iteration 38, loss = 0.08885275\n",
      "Iteration 39, loss = 0.08941543\n",
      "Iteration 40, loss = 0.08780200\n",
      "Iteration 41, loss = 0.08650160\n",
      "Iteration 42, loss = 0.08626743\n",
      "Iteration 43, loss = 0.08431268\n",
      "Iteration 44, loss = 0.08440794\n",
      "Iteration 45, loss = 0.08365580\n",
      "Iteration 46, loss = 0.08189652\n",
      "Iteration 47, loss = 0.08209591\n",
      "Iteration 48, loss = 0.08097780\n",
      "Iteration 49, loss = 0.07957962\n",
      "Iteration 50, loss = 0.08050230\n",
      "Iteration 51, loss = 0.07826411\n",
      "Iteration 52, loss = 0.07756775\n",
      "Iteration 53, loss = 0.07684389\n",
      "Iteration 54, loss = 0.07634854\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23379556\n",
      "Iteration 2, loss = 0.16673548\n",
      "Iteration 3, loss = 0.15237912\n",
      "Iteration 4, loss = 0.14558695\n",
      "Iteration 5, loss = 0.13773079\n",
      "Iteration 6, loss = 0.13296982\n",
      "Iteration 7, loss = 0.13076801\n",
      "Iteration 8, loss = 0.12673082\n",
      "Iteration 9, loss = 0.12270301\n",
      "Iteration 10, loss = 0.12232507\n",
      "Iteration 11, loss = 0.11859369\n",
      "Iteration 12, loss = 0.11766717\n",
      "Iteration 13, loss = 0.11553376\n",
      "Iteration 14, loss = 0.11096872\n",
      "Iteration 15, loss = 0.11143963\n",
      "Iteration 16, loss = 0.10948257\n",
      "Iteration 17, loss = 0.10741618\n",
      "Iteration 18, loss = 0.10348457\n",
      "Iteration 19, loss = 0.10412877\n",
      "Iteration 20, loss = 0.10413816\n",
      "Iteration 21, loss = 0.09966373\n",
      "Iteration 22, loss = 0.09691504\n",
      "Iteration 23, loss = 0.09531263\n",
      "Iteration 24, loss = 0.09545401\n",
      "Iteration 25, loss = 0.09406727\n",
      "Iteration 26, loss = 0.09288389\n",
      "Iteration 27, loss = 0.09152975\n",
      "Iteration 28, loss = 0.09233122\n",
      "Iteration 29, loss = 0.08861727\n",
      "Iteration 30, loss = 0.08686049\n",
      "Iteration 31, loss = 0.08587164\n",
      "Iteration 32, loss = 0.08614696\n",
      "Iteration 33, loss = 0.08434212\n",
      "Iteration 34, loss = 0.08307935\n",
      "Iteration 35, loss = 0.08362199\n",
      "Iteration 36, loss = 0.08043568\n",
      "Iteration 37, loss = 0.07932663\n",
      "Iteration 38, loss = 0.07789314\n",
      "Iteration 39, loss = 0.07607122\n",
      "Iteration 40, loss = 0.07752720\n",
      "Iteration 41, loss = 0.07337693\n",
      "Iteration 42, loss = 0.07485056\n",
      "Iteration 43, loss = 0.07315016\n",
      "Iteration 44, loss = 0.07473231\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22828185\n",
      "Iteration 2, loss = 0.16967793\n",
      "Iteration 3, loss = 0.15454483\n",
      "Iteration 4, loss = 0.14386981\n",
      "Iteration 5, loss = 0.13747452\n",
      "Iteration 6, loss = 0.13449221\n",
      "Iteration 7, loss = 0.12802865\n",
      "Iteration 8, loss = 0.12533365\n",
      "Iteration 9, loss = 0.12584586\n",
      "Iteration 10, loss = 0.12081516\n",
      "Iteration 11, loss = 0.11706214\n",
      "Iteration 12, loss = 0.11471871\n",
      "Iteration 13, loss = 0.11364574\n",
      "Iteration 14, loss = 0.11284416\n",
      "Iteration 15, loss = 0.10906734\n",
      "Iteration 16, loss = 0.10660851\n",
      "Iteration 17, loss = 0.10692157\n",
      "Iteration 18, loss = 0.10462169\n",
      "Iteration 19, loss = 0.10347780\n",
      "Iteration 20, loss = 0.10103613\n",
      "Iteration 21, loss = 0.09921713\n",
      "Iteration 22, loss = 0.09784754\n",
      "Iteration 23, loss = 0.09559537\n",
      "Iteration 24, loss = 0.09403614\n",
      "Iteration 25, loss = 0.09288872\n",
      "Iteration 26, loss = 0.09031421\n",
      "Iteration 27, loss = 0.09199709\n",
      "Iteration 28, loss = 0.08747814\n",
      "Iteration 29, loss = 0.08792417\n",
      "Iteration 30, loss = 0.08815946\n",
      "Iteration 31, loss = 0.08464078\n",
      "Iteration 32, loss = 0.08472685\n",
      "Iteration 33, loss = 0.08203495\n",
      "Iteration 34, loss = 0.08052386\n",
      "Iteration 35, loss = 0.08089249\n",
      "Iteration 36, loss = 0.07977187\n",
      "Iteration 37, loss = 0.07836095\n",
      "Iteration 38, loss = 0.07694341\n",
      "Iteration 39, loss = 0.07704132\n",
      "Iteration 40, loss = 0.08549508\n",
      "Iteration 41, loss = 0.07377486\n",
      "Iteration 42, loss = 0.07401966\n",
      "Iteration 43, loss = 0.07487826\n",
      "Iteration 44, loss = 0.07229693\n",
      "Iteration 45, loss = 0.07087289\n",
      "Iteration 46, loss = 0.06863496\n",
      "Iteration 47, loss = 0.06818169\n",
      "Iteration 48, loss = 0.06651694\n",
      "Iteration 49, loss = 0.06454211\n",
      "Iteration 50, loss = 0.06630361\n",
      "Iteration 51, loss = 0.06399255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.06414961\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22920103\n",
      "Iteration 2, loss = 0.17060587\n",
      "Iteration 3, loss = 0.15614342\n",
      "Iteration 4, loss = 0.14569188\n",
      "Iteration 5, loss = 0.14061978\n",
      "Iteration 6, loss = 0.13635350\n",
      "Iteration 7, loss = 0.13300708\n",
      "Iteration 8, loss = 0.12793497\n",
      "Iteration 9, loss = 0.12567535\n",
      "Iteration 10, loss = 0.12230200\n",
      "Iteration 11, loss = 0.11888299\n",
      "Iteration 12, loss = 0.11943133\n",
      "Iteration 13, loss = 0.11520628\n",
      "Iteration 14, loss = 0.11522017\n",
      "Iteration 15, loss = 0.11147925\n",
      "Iteration 16, loss = 0.11165016\n",
      "Iteration 17, loss = 0.10902872\n",
      "Iteration 18, loss = 0.10401608\n",
      "Iteration 19, loss = 0.10576214\n",
      "Iteration 20, loss = 0.10273472\n",
      "Iteration 21, loss = 0.10134641\n",
      "Iteration 22, loss = 0.10117323\n",
      "Iteration 23, loss = 0.09946899\n",
      "Iteration 24, loss = 0.09517527\n",
      "Iteration 25, loss = 0.09689806\n",
      "Iteration 26, loss = 0.09399868\n",
      "Iteration 27, loss = 0.09399974\n",
      "Iteration 28, loss = 0.09056481\n",
      "Iteration 29, loss = 0.08954383\n",
      "Iteration 30, loss = 0.08942753\n",
      "Iteration 31, loss = 0.08618730\n",
      "Iteration 32, loss = 0.08503754\n",
      "Iteration 33, loss = 0.08271632\n",
      "Iteration 34, loss = 0.08395967\n",
      "Iteration 35, loss = 0.08246872\n",
      "Iteration 36, loss = 0.08164321\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22819337\n",
      "Iteration 2, loss = 0.16778692\n",
      "Iteration 3, loss = 0.15569273\n",
      "Iteration 4, loss = 0.14755157\n",
      "Iteration 5, loss = 0.14106503\n",
      "Iteration 6, loss = 0.13626638\n",
      "Iteration 7, loss = 0.13137640\n",
      "Iteration 8, loss = 0.12790645\n",
      "Iteration 9, loss = 0.12539500\n",
      "Iteration 10, loss = 0.12759870\n",
      "Iteration 11, loss = 0.12332571\n",
      "Iteration 12, loss = 0.11843505\n",
      "Iteration 13, loss = 0.11639705\n",
      "Iteration 14, loss = 0.11448765\n",
      "Iteration 15, loss = 0.11187214\n",
      "Iteration 16, loss = 0.11068266\n",
      "Iteration 17, loss = 0.10756607\n",
      "Iteration 18, loss = 0.10618451\n",
      "Iteration 19, loss = 0.10618502\n",
      "Iteration 20, loss = 0.10316583\n",
      "Iteration 21, loss = 0.10217189\n",
      "Iteration 22, loss = 0.10012667\n",
      "Iteration 23, loss = 0.09808487\n",
      "Iteration 24, loss = 0.09626735\n",
      "Iteration 25, loss = 0.09673663\n",
      "Iteration 26, loss = 0.09634905\n",
      "Iteration 27, loss = 0.09416524\n",
      "Iteration 28, loss = 0.09100101\n",
      "Iteration 29, loss = 0.09113394\n",
      "Iteration 30, loss = 0.08909384\n",
      "Iteration 31, loss = 0.08815419\n",
      "Iteration 32, loss = 0.08654457\n",
      "Iteration 33, loss = 0.08606265\n",
      "Iteration 34, loss = 0.08622332\n",
      "Iteration 35, loss = 0.08449148\n",
      "Iteration 36, loss = 0.08041408\n",
      "Iteration 37, loss = 0.08628302\n",
      "Iteration 38, loss = 0.07976084\n",
      "Iteration 39, loss = 0.08111838\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22898646\n",
      "Iteration 2, loss = 0.16733905\n",
      "Iteration 3, loss = 0.15280480\n",
      "Iteration 4, loss = 0.14402636\n",
      "Iteration 5, loss = 0.13991211\n",
      "Iteration 6, loss = 0.13361499\n",
      "Iteration 7, loss = 0.12999967\n",
      "Iteration 8, loss = 0.12823038\n",
      "Iteration 9, loss = 0.12393249\n",
      "Iteration 10, loss = 0.12257016\n",
      "Iteration 11, loss = 0.11819819\n",
      "Iteration 12, loss = 0.11607691\n",
      "Iteration 13, loss = 0.11516444\n",
      "Iteration 14, loss = 0.11080664\n",
      "Iteration 15, loss = 0.10973616\n",
      "Iteration 16, loss = 0.10781814\n",
      "Iteration 17, loss = 0.10559058\n",
      "Iteration 18, loss = 0.10515995\n",
      "Iteration 19, loss = 0.10208359\n",
      "Iteration 20, loss = 0.10019756\n",
      "Iteration 21, loss = 0.09930407\n",
      "Iteration 22, loss = 0.09678555\n",
      "Iteration 23, loss = 0.09538285\n",
      "Iteration 24, loss = 0.09475126\n",
      "Iteration 25, loss = 0.09266193\n",
      "Iteration 26, loss = 0.09204009\n",
      "Iteration 27, loss = 0.09050261\n",
      "Iteration 28, loss = 0.08919746\n",
      "Iteration 29, loss = 0.08870269\n",
      "Iteration 30, loss = 0.08633670\n",
      "Iteration 31, loss = 0.08677006\n",
      "Iteration 32, loss = 0.08691725\n",
      "Iteration 33, loss = 0.08528499\n",
      "Iteration 34, loss = 0.08220438\n",
      "Iteration 35, loss = 0.08138651\n",
      "Iteration 36, loss = 0.08175826\n",
      "Iteration 37, loss = 0.07969898\n",
      "Iteration 38, loss = 0.07861899\n",
      "Iteration 39, loss = 0.08216654\n",
      "Iteration 40, loss = 0.07597485\n",
      "Iteration 41, loss = 0.07430272\n",
      "Iteration 42, loss = 0.07434017\n",
      "Iteration 43, loss = 0.07549591\n",
      "Iteration 44, loss = 0.07402547\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45875697\n",
      "Iteration 2, loss = 0.27413136\n",
      "Iteration 3, loss = 0.24634884\n",
      "Iteration 4, loss = 0.23051802\n",
      "Iteration 5, loss = 0.21951789\n",
      "Iteration 6, loss = 0.21102794\n",
      "Iteration 7, loss = 0.20447875\n",
      "Iteration 8, loss = 0.19903983\n",
      "Iteration 9, loss = 0.19425976\n",
      "Iteration 10, loss = 0.19018489\n",
      "Iteration 11, loss = 0.18674175\n",
      "Iteration 12, loss = 0.18352979\n",
      "Iteration 13, loss = 0.18071081\n",
      "Iteration 14, loss = 0.17816798\n",
      "Iteration 15, loss = 0.17584114\n",
      "Iteration 16, loss = 0.17366213\n",
      "Iteration 17, loss = 0.17141885\n",
      "Iteration 18, loss = 0.16949345\n",
      "Iteration 19, loss = 0.16783754\n",
      "Iteration 20, loss = 0.16610962\n",
      "Iteration 21, loss = 0.16500272\n",
      "Iteration 22, loss = 0.16298620\n",
      "Iteration 23, loss = 0.16172311\n",
      "Iteration 24, loss = 0.16020047\n",
      "Iteration 25, loss = 0.15889395\n",
      "Iteration 26, loss = 0.15786640\n",
      "Iteration 27, loss = 0.15666762\n",
      "Iteration 28, loss = 0.15544773\n",
      "Iteration 29, loss = 0.15457474\n",
      "Iteration 30, loss = 0.15348180\n",
      "Iteration 31, loss = 0.15258024\n",
      "Iteration 32, loss = 0.15166120\n",
      "Iteration 33, loss = 0.15056955\n",
      "Iteration 34, loss = 0.14980198\n",
      "Iteration 35, loss = 0.14899544\n",
      "Iteration 36, loss = 0.14815526\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44137380\n",
      "Iteration 2, loss = 0.28730989\n",
      "Iteration 3, loss = 0.25726829\n",
      "Iteration 4, loss = 0.23946433\n",
      "Iteration 5, loss = 0.22698840\n",
      "Iteration 6, loss = 0.21747785\n",
      "Iteration 7, loss = 0.20988200\n",
      "Iteration 8, loss = 0.20397066\n",
      "Iteration 9, loss = 0.19869978\n",
      "Iteration 10, loss = 0.19421382\n",
      "Iteration 11, loss = 0.19010217\n",
      "Iteration 12, loss = 0.18617059\n",
      "Iteration 13, loss = 0.18287695\n",
      "Iteration 14, loss = 0.18001581\n",
      "Iteration 15, loss = 0.17729294\n",
      "Iteration 16, loss = 0.17489252\n",
      "Iteration 17, loss = 0.17259600\n",
      "Iteration 18, loss = 0.17073689\n",
      "Iteration 19, loss = 0.16834032\n",
      "Iteration 20, loss = 0.16661258\n",
      "Iteration 21, loss = 0.16479718\n",
      "Iteration 22, loss = 0.16308837\n",
      "Iteration 23, loss = 0.16155229\n",
      "Iteration 24, loss = 0.16021611\n",
      "Iteration 25, loss = 0.15871267\n",
      "Iteration 26, loss = 0.15728281\n",
      "Iteration 27, loss = 0.15593188\n",
      "Iteration 28, loss = 0.15482439\n",
      "Iteration 29, loss = 0.15377497\n",
      "Iteration 30, loss = 0.15261126\n",
      "Iteration 31, loss = 0.15137241\n",
      "Iteration 32, loss = 0.15034302\n",
      "Iteration 33, loss = 0.14933039\n",
      "Iteration 34, loss = 0.14856283\n",
      "Iteration 35, loss = 0.14740416\n",
      "Iteration 36, loss = 0.14673093\n",
      "Iteration 37, loss = 0.14603907\n",
      "Iteration 38, loss = 0.14500187\n",
      "Iteration 39, loss = 0.14429173\n",
      "Iteration 40, loss = 0.14337997\n",
      "Iteration 41, loss = 0.14255369\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44311720\n",
      "Iteration 2, loss = 0.28910072\n",
      "Iteration 3, loss = 0.25775021\n",
      "Iteration 4, loss = 0.24020190\n",
      "Iteration 5, loss = 0.22844154\n",
      "Iteration 6, loss = 0.21958395\n",
      "Iteration 7, loss = 0.21231854\n",
      "Iteration 8, loss = 0.20635429\n",
      "Iteration 9, loss = 0.20128018\n",
      "Iteration 10, loss = 0.19698645\n",
      "Iteration 11, loss = 0.19298864\n",
      "Iteration 12, loss = 0.18936706\n",
      "Iteration 13, loss = 0.18635037\n",
      "Iteration 14, loss = 0.18360658\n",
      "Iteration 15, loss = 0.18094600\n",
      "Iteration 16, loss = 0.17878852\n",
      "Iteration 17, loss = 0.17630080\n",
      "Iteration 18, loss = 0.17431770\n",
      "Iteration 19, loss = 0.17247152\n",
      "Iteration 20, loss = 0.17054479\n",
      "Iteration 21, loss = 0.16872030\n",
      "Iteration 22, loss = 0.16706855\n",
      "Iteration 23, loss = 0.16554935\n",
      "Iteration 24, loss = 0.16410022\n",
      "Iteration 25, loss = 0.16273391\n",
      "Iteration 26, loss = 0.16144202\n",
      "Iteration 27, loss = 0.16029675\n",
      "Iteration 28, loss = 0.15884675\n",
      "Iteration 29, loss = 0.15777969\n",
      "Iteration 30, loss = 0.15677805\n",
      "Iteration 31, loss = 0.15548358\n",
      "Iteration 32, loss = 0.15461194\n",
      "Iteration 33, loss = 0.15339828\n",
      "Iteration 34, loss = 0.15266704\n",
      "Iteration 35, loss = 0.15157262\n",
      "Iteration 36, loss = 0.15090080\n",
      "Iteration 37, loss = 0.15020789\n",
      "Iteration 38, loss = 0.14921265\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42549378\n",
      "Iteration 2, loss = 0.27727741\n",
      "Iteration 3, loss = 0.25085324\n",
      "Iteration 4, loss = 0.23529373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.22445037\n",
      "Iteration 6, loss = 0.21601614\n",
      "Iteration 7, loss = 0.20906563\n",
      "Iteration 8, loss = 0.20320654\n",
      "Iteration 9, loss = 0.19843426\n",
      "Iteration 10, loss = 0.19432774\n",
      "Iteration 11, loss = 0.19062772\n",
      "Iteration 12, loss = 0.18718134\n",
      "Iteration 13, loss = 0.18413118\n",
      "Iteration 14, loss = 0.18130638\n",
      "Iteration 15, loss = 0.17894362\n",
      "Iteration 16, loss = 0.17654102\n",
      "Iteration 17, loss = 0.17451097\n",
      "Iteration 18, loss = 0.17233045\n",
      "Iteration 19, loss = 0.17037301\n",
      "Iteration 20, loss = 0.16877366\n",
      "Iteration 21, loss = 0.16724926\n",
      "Iteration 22, loss = 0.16557920\n",
      "Iteration 23, loss = 0.16439825\n",
      "Iteration 24, loss = 0.16279768\n",
      "Iteration 25, loss = 0.16154591\n",
      "Iteration 26, loss = 0.16043675\n",
      "Iteration 27, loss = 0.15922481\n",
      "Iteration 28, loss = 0.15779540\n",
      "Iteration 29, loss = 0.15688325\n",
      "Iteration 30, loss = 0.15559583\n",
      "Iteration 31, loss = 0.15463485\n",
      "Iteration 32, loss = 0.15360526\n",
      "Iteration 33, loss = 0.15282598\n",
      "Iteration 34, loss = 0.15165031\n",
      "Iteration 35, loss = 0.15095751\n",
      "Iteration 36, loss = 0.14999012\n",
      "Iteration 37, loss = 0.14922650\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45426996\n",
      "Iteration 2, loss = 0.28307966\n",
      "Iteration 3, loss = 0.25482691\n",
      "Iteration 4, loss = 0.23822931\n",
      "Iteration 5, loss = 0.22674205\n",
      "Iteration 6, loss = 0.21806358\n",
      "Iteration 7, loss = 0.21102729\n",
      "Iteration 8, loss = 0.20500955\n",
      "Iteration 9, loss = 0.20012052\n",
      "Iteration 10, loss = 0.19578264\n",
      "Iteration 11, loss = 0.19181829\n",
      "Iteration 12, loss = 0.18836499\n",
      "Iteration 13, loss = 0.18531945\n",
      "Iteration 14, loss = 0.18235812\n",
      "Iteration 15, loss = 0.17958220\n",
      "Iteration 16, loss = 0.17722462\n",
      "Iteration 17, loss = 0.17494020\n",
      "Iteration 18, loss = 0.17292131\n",
      "Iteration 19, loss = 0.17086273\n",
      "Iteration 20, loss = 0.16886791\n",
      "Iteration 21, loss = 0.16701009\n",
      "Iteration 22, loss = 0.16532352\n",
      "Iteration 23, loss = 0.16361453\n",
      "Iteration 24, loss = 0.16231884\n",
      "Iteration 25, loss = 0.16055005\n",
      "Iteration 26, loss = 0.15923639\n",
      "Iteration 27, loss = 0.15819310\n",
      "Iteration 28, loss = 0.15675728\n",
      "Iteration 29, loss = 0.15581049\n",
      "Iteration 30, loss = 0.15461204\n",
      "Iteration 31, loss = 0.15355697\n",
      "Iteration 32, loss = 0.15234538\n",
      "Iteration 33, loss = 0.15148737\n",
      "Iteration 34, loss = 0.15040451\n",
      "Iteration 35, loss = 0.14944041\n",
      "Iteration 36, loss = 0.14855151\n",
      "Iteration 37, loss = 0.14777489\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30242272\n",
      "Iteration 2, loss = 0.21222054\n",
      "Iteration 3, loss = 0.18676156\n",
      "Iteration 4, loss = 0.17315272\n",
      "Iteration 5, loss = 0.16494294\n",
      "Iteration 6, loss = 0.15843938\n",
      "Iteration 7, loss = 0.15303615\n",
      "Iteration 8, loss = 0.14809612\n",
      "Iteration 9, loss = 0.14488833\n",
      "Iteration 10, loss = 0.14234851\n",
      "Iteration 11, loss = 0.13920654\n",
      "Iteration 12, loss = 0.13680270\n",
      "Iteration 13, loss = 0.13429848\n",
      "Iteration 14, loss = 0.13161519\n",
      "Iteration 15, loss = 0.13175867\n",
      "Iteration 16, loss = 0.12976398\n",
      "Iteration 17, loss = 0.12698162\n",
      "Iteration 18, loss = 0.12608113\n",
      "Iteration 19, loss = 0.12667821\n",
      "Iteration 20, loss = 0.12259357\n",
      "Iteration 21, loss = 0.12169409\n",
      "Iteration 22, loss = 0.12929539\n",
      "Iteration 23, loss = 0.12020744\n",
      "Iteration 24, loss = 0.12002124\n",
      "Iteration 25, loss = 0.11805674\n",
      "Iteration 26, loss = 0.11706618\n",
      "Iteration 27, loss = 0.11607494\n",
      "Iteration 28, loss = 0.11482501\n",
      "Iteration 29, loss = 0.11375861\n",
      "Iteration 30, loss = 0.11364245\n",
      "Iteration 31, loss = 0.11235463\n",
      "Iteration 32, loss = 0.11139814\n",
      "Iteration 33, loss = 0.11113274\n",
      "Iteration 34, loss = 0.11039508\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29898908\n",
      "Iteration 2, loss = 0.20338598\n",
      "Iteration 3, loss = 0.18421324\n",
      "Iteration 4, loss = 0.17324967\n",
      "Iteration 5, loss = 0.16566061\n",
      "Iteration 6, loss = 0.15854186\n",
      "Iteration 7, loss = 0.15398222\n",
      "Iteration 8, loss = 0.14995791\n",
      "Iteration 9, loss = 0.14579876\n",
      "Iteration 10, loss = 0.14301896\n",
      "Iteration 11, loss = 0.13979400\n",
      "Iteration 12, loss = 0.13833344\n",
      "Iteration 13, loss = 0.13619579\n",
      "Iteration 14, loss = 0.13404650\n",
      "Iteration 15, loss = 0.13223072\n",
      "Iteration 16, loss = 0.13035634\n",
      "Iteration 17, loss = 0.12968147\n",
      "Iteration 18, loss = 0.12762089\n",
      "Iteration 19, loss = 0.12618722\n",
      "Iteration 20, loss = 0.12475845\n",
      "Iteration 21, loss = 0.12403150\n",
      "Iteration 22, loss = 0.12278115\n",
      "Iteration 23, loss = 0.12182826\n",
      "Iteration 24, loss = 0.12038080\n",
      "Iteration 25, loss = 0.11900950\n",
      "Iteration 26, loss = 0.11734602\n",
      "Iteration 27, loss = 0.11685619\n",
      "Iteration 28, loss = 0.11602063\n",
      "Iteration 29, loss = 0.11462519\n",
      "Iteration 30, loss = 0.11396844\n",
      "Iteration 31, loss = 0.11268773\n",
      "Iteration 32, loss = 0.11203702\n",
      "Iteration 33, loss = 0.11123348\n",
      "Iteration 34, loss = 0.11097519\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31468521\n",
      "Iteration 2, loss = 0.21404870\n",
      "Iteration 3, loss = 0.19301486\n",
      "Iteration 4, loss = 0.17970397\n",
      "Iteration 5, loss = 0.17068236\n",
      "Iteration 6, loss = 0.16343523\n",
      "Iteration 7, loss = 0.15770392\n",
      "Iteration 8, loss = 0.15386064\n",
      "Iteration 9, loss = 0.14901190\n",
      "Iteration 10, loss = 0.14596769\n",
      "Iteration 11, loss = 0.14276046\n",
      "Iteration 12, loss = 0.14102097\n",
      "Iteration 13, loss = 0.13783786\n",
      "Iteration 14, loss = 0.13602823\n",
      "Iteration 15, loss = 0.13539071\n",
      "Iteration 16, loss = 0.13217824\n",
      "Iteration 17, loss = 0.13103266\n",
      "Iteration 18, loss = 0.12979812\n",
      "Iteration 19, loss = 0.12755298\n",
      "Iteration 20, loss = 0.12582669\n",
      "Iteration 21, loss = 0.12627195\n",
      "Iteration 22, loss = 0.12427856\n",
      "Iteration 23, loss = 0.12241776\n",
      "Iteration 24, loss = 0.12100286\n",
      "Iteration 25, loss = 0.12036989\n",
      "Iteration 26, loss = 0.12199478\n",
      "Iteration 27, loss = 0.11793828\n",
      "Iteration 28, loss = 0.11666218\n",
      "Iteration 29, loss = 0.11643725\n",
      "Iteration 30, loss = 0.11512554\n",
      "Iteration 31, loss = 0.11400222\n",
      "Iteration 32, loss = 0.11330268\n",
      "Iteration 33, loss = 0.11279237\n",
      "Iteration 34, loss = 0.11130080\n",
      "Iteration 35, loss = 0.11055525\n",
      "Iteration 36, loss = 0.10983814\n",
      "Iteration 37, loss = 0.10896623\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.29871048\n",
      "Iteration 2, loss = 0.20603359\n",
      "Iteration 3, loss = 0.18623286\n",
      "Iteration 4, loss = 0.17854677\n",
      "Iteration 5, loss = 0.16701779\n",
      "Iteration 6, loss = 0.16202456\n",
      "Iteration 7, loss = 0.15651274\n",
      "Iteration 8, loss = 0.15152578\n",
      "Iteration 9, loss = 0.14872859\n",
      "Iteration 10, loss = 0.14487792\n",
      "Iteration 11, loss = 0.14213357\n",
      "Iteration 12, loss = 0.14085711\n",
      "Iteration 13, loss = 0.13801454\n",
      "Iteration 14, loss = 0.13506489\n",
      "Iteration 15, loss = 0.13396705\n",
      "Iteration 16, loss = 0.13212707\n",
      "Iteration 17, loss = 0.13016795\n",
      "Iteration 18, loss = 0.12871768\n",
      "Iteration 19, loss = 0.12723332\n",
      "Iteration 20, loss = 0.12578889\n",
      "Iteration 21, loss = 0.12557722\n",
      "Iteration 22, loss = 0.12410856\n",
      "Iteration 23, loss = 0.12430699\n",
      "Iteration 24, loss = 0.12266332\n",
      "Iteration 25, loss = 0.12025480\n",
      "Iteration 26, loss = 0.11902324\n",
      "Iteration 27, loss = 0.11878275\n",
      "Iteration 28, loss = 0.11779748\n",
      "Iteration 29, loss = 0.11743778\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30358085\n",
      "Iteration 2, loss = 0.20202900\n",
      "Iteration 3, loss = 0.18267351\n",
      "Iteration 4, loss = 0.17081716\n",
      "Iteration 5, loss = 0.16292282\n",
      "Iteration 6, loss = 0.15650703\n",
      "Iteration 7, loss = 0.15219278\n",
      "Iteration 8, loss = 0.14673101\n",
      "Iteration 9, loss = 0.14368498\n",
      "Iteration 10, loss = 0.14066944\n",
      "Iteration 11, loss = 0.13777248\n",
      "Iteration 12, loss = 0.13609161\n",
      "Iteration 13, loss = 0.13403640\n",
      "Iteration 14, loss = 0.13146678\n",
      "Iteration 15, loss = 0.12942431\n",
      "Iteration 16, loss = 0.12741428\n",
      "Iteration 17, loss = 0.12594344\n",
      "Iteration 18, loss = 0.12555682\n",
      "Iteration 19, loss = 0.12284093\n",
      "Iteration 20, loss = 0.12176599\n",
      "Iteration 21, loss = 0.12082124\n",
      "Iteration 22, loss = 0.11930225\n",
      "Iteration 23, loss = 0.11795083\n",
      "Iteration 24, loss = 0.11728442\n",
      "Iteration 25, loss = 0.11665858\n",
      "Iteration 26, loss = 0.11550687\n",
      "Iteration 27, loss = 0.11365818\n",
      "Iteration 28, loss = 0.11248259\n",
      "Iteration 29, loss = 0.11288079\n",
      "Iteration 30, loss = 0.11098624\n",
      "Iteration 31, loss = 0.11013922\n",
      "Iteration 32, loss = 0.10928638\n",
      "Iteration 33, loss = 0.10870598\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25142308\n",
      "Iteration 2, loss = 0.18319858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.16466074\n",
      "Iteration 4, loss = 0.15477439\n",
      "Iteration 5, loss = 0.14712892\n",
      "Iteration 6, loss = 0.14165965\n",
      "Iteration 7, loss = 0.13682059\n",
      "Iteration 8, loss = 0.13447323\n",
      "Iteration 9, loss = 0.12982637\n",
      "Iteration 10, loss = 0.12813931\n",
      "Iteration 11, loss = 0.12591232\n",
      "Iteration 12, loss = 0.12341964\n",
      "Iteration 13, loss = 0.12197355\n",
      "Iteration 14, loss = 0.12001041\n",
      "Iteration 15, loss = 0.11807766\n",
      "Iteration 16, loss = 0.11923167\n",
      "Iteration 17, loss = 0.11489237\n",
      "Iteration 18, loss = 0.11426404\n",
      "Iteration 19, loss = 0.11221951\n",
      "Iteration 20, loss = 0.11231641\n",
      "Iteration 21, loss = 0.10896754\n",
      "Iteration 22, loss = 0.10962992\n",
      "Iteration 23, loss = 0.10717302\n",
      "Iteration 24, loss = 0.10758015\n",
      "Iteration 25, loss = 0.10838634\n",
      "Iteration 26, loss = 0.10597275\n",
      "Iteration 27, loss = 0.10740557\n",
      "Iteration 28, loss = 0.10247768\n",
      "Iteration 29, loss = 0.10112693\n",
      "Iteration 30, loss = 0.09938739\n",
      "Iteration 31, loss = 0.09676407\n",
      "Iteration 32, loss = 0.09667989\n",
      "Iteration 33, loss = 0.09491963\n",
      "Iteration 34, loss = 0.09442093\n",
      "Iteration 35, loss = 0.09392109\n",
      "Iteration 36, loss = 0.09277776\n",
      "Iteration 37, loss = 0.09272305\n",
      "Iteration 38, loss = 0.09092738\n",
      "Iteration 39, loss = 0.09293271\n",
      "Iteration 40, loss = 0.09559889\n",
      "Iteration 41, loss = 0.08962996\n",
      "Iteration 42, loss = 0.08712558\n",
      "Iteration 43, loss = 0.08613094\n",
      "Iteration 44, loss = 0.08469360\n",
      "Iteration 45, loss = 0.08376547\n",
      "Iteration 46, loss = 0.08239772\n",
      "Iteration 47, loss = 0.08285691\n",
      "Iteration 48, loss = 0.08188876\n",
      "Iteration 49, loss = 0.08035301\n",
      "Iteration 50, loss = 0.08006449\n",
      "Iteration 51, loss = 0.08092453\n",
      "Iteration 52, loss = 0.07960149\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25467178\n",
      "Iteration 2, loss = 0.18151297\n",
      "Iteration 3, loss = 0.16239890\n",
      "Iteration 4, loss = 0.15272994\n",
      "Iteration 5, loss = 0.14518891\n",
      "Iteration 6, loss = 0.14033047\n",
      "Iteration 7, loss = 0.13567279\n",
      "Iteration 8, loss = 0.13221832\n",
      "Iteration 9, loss = 0.12982168\n",
      "Iteration 10, loss = 0.12762704\n",
      "Iteration 11, loss = 0.12445688\n",
      "Iteration 12, loss = 0.12279759\n",
      "Iteration 13, loss = 0.12375364\n",
      "Iteration 14, loss = 0.11860653\n",
      "Iteration 15, loss = 0.11691806\n",
      "Iteration 16, loss = 0.11874490\n",
      "Iteration 17, loss = 0.11447482\n",
      "Iteration 18, loss = 0.11248506\n",
      "Iteration 19, loss = 0.11152964\n",
      "Iteration 20, loss = 0.11001757\n",
      "Iteration 21, loss = 0.10820228\n",
      "Iteration 22, loss = 0.10664102\n",
      "Iteration 23, loss = 0.10565913\n",
      "Iteration 24, loss = 0.10455226\n",
      "Iteration 25, loss = 0.10263871\n",
      "Iteration 26, loss = 0.10432005\n",
      "Iteration 27, loss = 0.10155003\n",
      "Iteration 28, loss = 0.10276598\n",
      "Iteration 29, loss = 0.09884575\n",
      "Iteration 30, loss = 0.09667984\n",
      "Iteration 31, loss = 0.09761116\n",
      "Iteration 32, loss = 0.09640774\n",
      "Iteration 33, loss = 0.09399265\n",
      "Iteration 34, loss = 0.09510841\n",
      "Iteration 35, loss = 0.09347102\n",
      "Iteration 36, loss = 0.09102894\n",
      "Iteration 37, loss = 0.09047257\n",
      "Iteration 38, loss = 0.08991301\n",
      "Iteration 39, loss = 0.08818271\n",
      "Iteration 40, loss = 0.08631304\n",
      "Iteration 41, loss = 0.08627017\n",
      "Iteration 42, loss = 0.08467251\n",
      "Iteration 43, loss = 0.08616194\n",
      "Iteration 44, loss = 0.08344849\n",
      "Iteration 45, loss = 0.08248845\n",
      "Iteration 46, loss = 0.08142654\n",
      "Iteration 47, loss = 0.08101772\n",
      "Iteration 48, loss = 0.08137320\n",
      "Iteration 49, loss = 0.07910373\n",
      "Iteration 50, loss = 0.07815911\n",
      "Iteration 51, loss = 0.07637302\n",
      "Iteration 52, loss = 0.07800010\n",
      "Iteration 53, loss = 0.07666276\n",
      "Iteration 54, loss = 0.07415173\n",
      "Iteration 55, loss = 0.07507836\n",
      "Iteration 56, loss = 0.07309938\n",
      "Iteration 57, loss = 0.07289339\n",
      "Iteration 58, loss = 0.07232624\n",
      "Iteration 59, loss = 0.07115606\n",
      "Iteration 60, loss = 0.07024474\n",
      "Iteration 61, loss = 0.06893476\n",
      "Iteration 62, loss = 0.07063941\n",
      "Iteration 63, loss = 0.06787252\n",
      "Iteration 64, loss = 0.07030195\n",
      "Iteration 65, loss = 0.06863116\n",
      "Iteration 66, loss = 0.06579709\n",
      "Iteration 67, loss = 0.06541115\n",
      "Iteration 68, loss = 0.06387096\n",
      "Iteration 69, loss = 0.06355538\n",
      "Iteration 70, loss = 0.06203891\n",
      "Iteration 71, loss = 0.06285693\n",
      "Iteration 72, loss = 0.06202746\n",
      "Iteration 73, loss = 0.06207115\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26903384\n",
      "Iteration 2, loss = 0.18808896\n",
      "Iteration 3, loss = 0.16904034\n",
      "Iteration 4, loss = 0.15986388\n",
      "Iteration 5, loss = 0.15101577\n",
      "Iteration 6, loss = 0.14549659\n",
      "Iteration 7, loss = 0.14100761\n",
      "Iteration 8, loss = 0.13746844\n",
      "Iteration 9, loss = 0.13298779\n",
      "Iteration 10, loss = 0.13067776\n",
      "Iteration 11, loss = 0.12755272\n",
      "Iteration 12, loss = 0.12580840\n",
      "Iteration 13, loss = 0.12434975\n",
      "Iteration 14, loss = 0.12145652\n",
      "Iteration 15, loss = 0.12033056\n",
      "Iteration 16, loss = 0.11928670\n",
      "Iteration 17, loss = 0.11802492\n",
      "Iteration 18, loss = 0.11572136\n",
      "Iteration 19, loss = 0.11348190\n",
      "Iteration 20, loss = 0.11306035\n",
      "Iteration 21, loss = 0.11145181\n",
      "Iteration 22, loss = 0.11024892\n",
      "Iteration 23, loss = 0.10916051\n",
      "Iteration 24, loss = 0.10761902\n",
      "Iteration 25, loss = 0.11333912\n",
      "Iteration 26, loss = 0.10534077\n",
      "Iteration 27, loss = 0.10452271\n",
      "Iteration 28, loss = 0.10408630\n",
      "Iteration 29, loss = 0.10171239\n",
      "Iteration 30, loss = 0.10106722\n",
      "Iteration 31, loss = 0.09943159\n",
      "Iteration 32, loss = 0.09919633\n",
      "Iteration 33, loss = 0.09831214\n",
      "Iteration 34, loss = 0.09603967\n",
      "Iteration 35, loss = 0.09733715\n",
      "Iteration 36, loss = 0.09586713\n",
      "Iteration 37, loss = 0.09282274\n",
      "Iteration 38, loss = 0.09237372\n",
      "Iteration 39, loss = 0.10129068\n",
      "Iteration 40, loss = 0.09136925\n",
      "Iteration 41, loss = 0.09252249\n",
      "Iteration 42, loss = 0.09088063\n",
      "Iteration 43, loss = 0.09072194\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25636352\n",
      "Iteration 2, loss = 0.18319530\n",
      "Iteration 3, loss = 0.16711985\n",
      "Iteration 4, loss = 0.15591798\n",
      "Iteration 5, loss = 0.14876324\n",
      "Iteration 6, loss = 0.14495398\n",
      "Iteration 7, loss = 0.13997275\n",
      "Iteration 8, loss = 0.13848720\n",
      "Iteration 9, loss = 0.13441260\n",
      "Iteration 10, loss = 0.13061259\n",
      "Iteration 11, loss = 0.12908953\n",
      "Iteration 12, loss = 0.12504104\n",
      "Iteration 13, loss = 0.12458115\n",
      "Iteration 14, loss = 0.12088516\n",
      "Iteration 15, loss = 0.12029260\n",
      "Iteration 16, loss = 0.11859595\n",
      "Iteration 17, loss = 0.11765413\n",
      "Iteration 18, loss = 0.11446716\n",
      "Iteration 19, loss = 0.11408271\n",
      "Iteration 20, loss = 0.11278607\n",
      "Iteration 21, loss = 0.11394516\n",
      "Iteration 22, loss = 0.10996883\n",
      "Iteration 23, loss = 0.11283882\n",
      "Iteration 24, loss = 0.10795260\n",
      "Iteration 25, loss = 0.10617754\n",
      "Iteration 26, loss = 0.10560078\n",
      "Iteration 27, loss = 0.10328510\n",
      "Iteration 28, loss = 0.10338494\n",
      "Iteration 29, loss = 0.10170401\n",
      "Iteration 30, loss = 0.09986721\n",
      "Iteration 31, loss = 0.09931568\n",
      "Iteration 32, loss = 0.10096729\n",
      "Iteration 33, loss = 0.09770894\n",
      "Iteration 34, loss = 0.09479542\n",
      "Iteration 35, loss = 0.09502579\n",
      "Iteration 36, loss = 0.09427431\n",
      "Iteration 37, loss = 0.10182612\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25727019\n",
      "Iteration 2, loss = 0.18218384\n",
      "Iteration 3, loss = 0.16577264\n",
      "Iteration 4, loss = 0.15456225\n",
      "Iteration 5, loss = 0.14562978\n",
      "Iteration 6, loss = 0.14203209\n",
      "Iteration 7, loss = 0.13988217\n",
      "Iteration 8, loss = 0.13268934\n",
      "Iteration 9, loss = 0.13012309\n",
      "Iteration 10, loss = 0.12711467\n",
      "Iteration 11, loss = 0.12533898\n",
      "Iteration 12, loss = 0.12313416\n",
      "Iteration 13, loss = 0.12101855\n",
      "Iteration 14, loss = 0.11882613\n",
      "Iteration 15, loss = 0.12287501\n",
      "Iteration 16, loss = 0.11586296\n",
      "Iteration 17, loss = 0.11424707\n",
      "Iteration 18, loss = 0.11293246\n",
      "Iteration 19, loss = 0.11046128\n",
      "Iteration 20, loss = 0.10901243\n",
      "Iteration 21, loss = 0.10889317\n",
      "Iteration 22, loss = 0.10737546\n",
      "Iteration 23, loss = 0.10599227\n",
      "Iteration 24, loss = 0.10383161\n",
      "Iteration 25, loss = 0.10255482\n",
      "Iteration 26, loss = 0.10128031\n",
      "Iteration 27, loss = 0.09985170\n",
      "Iteration 28, loss = 0.09990115\n",
      "Iteration 29, loss = 0.09745983\n",
      "Iteration 30, loss = 0.09619070\n",
      "Iteration 31, loss = 0.09623704\n",
      "Iteration 32, loss = 0.09413953\n",
      "Iteration 33, loss = 0.10172326\n",
      "Iteration 34, loss = 0.09187391\n",
      "Iteration 35, loss = 0.09106213\n",
      "Iteration 36, loss = 0.09018671\n",
      "Iteration 37, loss = 0.09044092\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53612616\n",
      "Iteration 2, loss = 0.32537491\n",
      "Iteration 3, loss = 0.28879924\n",
      "Iteration 4, loss = 0.26881751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.25463414\n",
      "Iteration 6, loss = 0.24411888\n",
      "Iteration 7, loss = 0.23574311\n",
      "Iteration 8, loss = 0.22864107\n",
      "Iteration 9, loss = 0.22267359\n",
      "Iteration 10, loss = 0.21756984\n",
      "Iteration 11, loss = 0.21308599\n",
      "Iteration 12, loss = 0.20915102\n",
      "Iteration 13, loss = 0.20564412\n",
      "Iteration 14, loss = 0.20242099\n",
      "Iteration 15, loss = 0.19960553\n",
      "Iteration 16, loss = 0.19698924\n",
      "Iteration 17, loss = 0.19460881\n",
      "Iteration 18, loss = 0.19242954\n",
      "Iteration 19, loss = 0.19057649\n",
      "Iteration 20, loss = 0.18861747\n",
      "Iteration 21, loss = 0.18662626\n",
      "Iteration 22, loss = 0.18494434\n",
      "Iteration 23, loss = 0.18328237\n",
      "Iteration 24, loss = 0.18179606\n",
      "Iteration 25, loss = 0.18034025\n",
      "Iteration 26, loss = 0.17896630\n",
      "Iteration 27, loss = 0.17746597\n",
      "Iteration 28, loss = 0.17636627\n",
      "Iteration 29, loss = 0.17513809\n",
      "Iteration 30, loss = 0.17392398\n",
      "Iteration 31, loss = 0.17288674\n",
      "Iteration 32, loss = 0.17171857\n",
      "Iteration 33, loss = 0.17063169\n",
      "Iteration 34, loss = 0.16968668\n",
      "Iteration 35, loss = 0.16922058\n",
      "Iteration 36, loss = 0.16783683\n",
      "Iteration 37, loss = 0.16688917\n",
      "Iteration 38, loss = 0.16589795\n",
      "Iteration 39, loss = 0.16548792\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56212627\n",
      "Iteration 2, loss = 0.32934490\n",
      "Iteration 3, loss = 0.29041852\n",
      "Iteration 4, loss = 0.27066707\n",
      "Iteration 5, loss = 0.25677824\n",
      "Iteration 6, loss = 0.24685917\n",
      "Iteration 7, loss = 0.23852577\n",
      "Iteration 8, loss = 0.23186154\n",
      "Iteration 9, loss = 0.22607863\n",
      "Iteration 10, loss = 0.22098738\n",
      "Iteration 11, loss = 0.21663024\n",
      "Iteration 12, loss = 0.21276086\n",
      "Iteration 13, loss = 0.20917324\n",
      "Iteration 14, loss = 0.20606116\n",
      "Iteration 15, loss = 0.20312760\n",
      "Iteration 16, loss = 0.20044235\n",
      "Iteration 17, loss = 0.19790447\n",
      "Iteration 18, loss = 0.19557612\n",
      "Iteration 19, loss = 0.19340439\n",
      "Iteration 20, loss = 0.19134508\n",
      "Iteration 21, loss = 0.18976140\n",
      "Iteration 22, loss = 0.18775245\n",
      "Iteration 23, loss = 0.18607805\n",
      "Iteration 24, loss = 0.18453800\n",
      "Iteration 25, loss = 0.18282882\n",
      "Iteration 26, loss = 0.18146978\n",
      "Iteration 27, loss = 0.18016856\n",
      "Iteration 28, loss = 0.17885606\n",
      "Iteration 29, loss = 0.17738704\n",
      "Iteration 30, loss = 0.17613563\n",
      "Iteration 31, loss = 0.17505190\n",
      "Iteration 32, loss = 0.17369847\n",
      "Iteration 33, loss = 0.17282529\n",
      "Iteration 34, loss = 0.17158709\n",
      "Iteration 35, loss = 0.17062055\n",
      "Iteration 36, loss = 0.16950775\n",
      "Iteration 37, loss = 0.16840536\n",
      "Iteration 38, loss = 0.16759706\n",
      "Iteration 39, loss = 0.16681043\n",
      "Iteration 40, loss = 0.16580316\n",
      "Iteration 41, loss = 0.16491264\n",
      "Iteration 42, loss = 0.16405969\n",
      "Iteration 43, loss = 0.16324873\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58042503\n",
      "Iteration 2, loss = 0.34173575\n",
      "Iteration 3, loss = 0.30171503\n",
      "Iteration 4, loss = 0.28005274\n",
      "Iteration 5, loss = 0.26568162\n",
      "Iteration 6, loss = 0.25429669\n",
      "Iteration 7, loss = 0.24531447\n",
      "Iteration 8, loss = 0.23809722\n",
      "Iteration 9, loss = 0.23172993\n",
      "Iteration 10, loss = 0.22629997\n",
      "Iteration 11, loss = 0.22149368\n",
      "Iteration 12, loss = 0.21719460\n",
      "Iteration 13, loss = 0.21365994\n",
      "Iteration 14, loss = 0.21007942\n",
      "Iteration 15, loss = 0.20693117\n",
      "Iteration 16, loss = 0.20397874\n",
      "Iteration 17, loss = 0.20128213\n",
      "Iteration 18, loss = 0.19894185\n",
      "Iteration 19, loss = 0.19665538\n",
      "Iteration 20, loss = 0.19449367\n",
      "Iteration 21, loss = 0.19252893\n",
      "Iteration 22, loss = 0.19066630\n",
      "Iteration 23, loss = 0.18885830\n",
      "Iteration 24, loss = 0.18723911\n",
      "Iteration 25, loss = 0.18560154\n",
      "Iteration 26, loss = 0.18402336\n",
      "Iteration 27, loss = 0.18274901\n",
      "Iteration 28, loss = 0.18119441\n",
      "Iteration 29, loss = 0.17996681\n",
      "Iteration 30, loss = 0.17860607\n",
      "Iteration 31, loss = 0.17742311\n",
      "Iteration 32, loss = 0.17623371\n",
      "Iteration 33, loss = 0.17502504\n",
      "Iteration 34, loss = 0.17390380\n",
      "Iteration 35, loss = 0.17307047\n",
      "Iteration 36, loss = 0.17205777\n",
      "Iteration 37, loss = 0.17111467\n",
      "Iteration 38, loss = 0.16990772\n",
      "Iteration 39, loss = 0.16942410\n",
      "Iteration 40, loss = 0.16807430\n",
      "Iteration 41, loss = 0.16727022\n",
      "Iteration 42, loss = 0.16638765\n",
      "Iteration 43, loss = 0.16557545\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59192248\n",
      "Iteration 2, loss = 0.33198079\n",
      "Iteration 3, loss = 0.29289484\n",
      "Iteration 4, loss = 0.27172111\n",
      "Iteration 5, loss = 0.25741760\n",
      "Iteration 6, loss = 0.24693238\n",
      "Iteration 7, loss = 0.23897886\n",
      "Iteration 8, loss = 0.23196601\n",
      "Iteration 9, loss = 0.22625911\n",
      "Iteration 10, loss = 0.22116309\n",
      "Iteration 11, loss = 0.21720539\n",
      "Iteration 12, loss = 0.21289663\n",
      "Iteration 13, loss = 0.20958517\n",
      "Iteration 14, loss = 0.20698944\n",
      "Iteration 15, loss = 0.20362135\n",
      "Iteration 16, loss = 0.20114754\n",
      "Iteration 17, loss = 0.19826682\n",
      "Iteration 18, loss = 0.19607316\n",
      "Iteration 19, loss = 0.19388865\n",
      "Iteration 20, loss = 0.19188666\n",
      "Iteration 21, loss = 0.19007805\n",
      "Iteration 22, loss = 0.18833595\n",
      "Iteration 23, loss = 0.18648149\n",
      "Iteration 24, loss = 0.18509569\n",
      "Iteration 25, loss = 0.18372603\n",
      "Iteration 26, loss = 0.18213991\n",
      "Iteration 27, loss = 0.18071151\n",
      "Iteration 28, loss = 0.17958605\n",
      "Iteration 29, loss = 0.17813850\n",
      "Iteration 30, loss = 0.17689488\n",
      "Iteration 31, loss = 0.17588802\n",
      "Iteration 32, loss = 0.17463089\n",
      "Iteration 33, loss = 0.17386785\n",
      "Iteration 34, loss = 0.17259447\n",
      "Iteration 35, loss = 0.17151896\n",
      "Iteration 36, loss = 0.17054902\n",
      "Iteration 37, loss = 0.16972822\n",
      "Iteration 38, loss = 0.16866683\n",
      "Iteration 39, loss = 0.16786311\n",
      "Iteration 40, loss = 0.16718426\n",
      "Iteration 41, loss = 0.16616205\n",
      "Iteration 42, loss = 0.16531597\n",
      "Iteration 43, loss = 0.16450083\n",
      "Iteration 44, loss = 0.16373050\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61870848\n",
      "Iteration 2, loss = 0.33286567\n",
      "Iteration 3, loss = 0.29484060\n",
      "Iteration 4, loss = 0.27416435\n",
      "Iteration 5, loss = 0.25990861\n",
      "Iteration 6, loss = 0.24916050\n",
      "Iteration 7, loss = 0.24073384\n",
      "Iteration 8, loss = 0.23361670\n",
      "Iteration 9, loss = 0.22752296\n",
      "Iteration 10, loss = 0.22227470\n",
      "Iteration 11, loss = 0.21761320\n",
      "Iteration 12, loss = 0.21354754\n",
      "Iteration 13, loss = 0.20986621\n",
      "Iteration 14, loss = 0.20646588\n",
      "Iteration 15, loss = 0.20332752\n",
      "Iteration 16, loss = 0.20055447\n",
      "Iteration 17, loss = 0.19787352\n",
      "Iteration 18, loss = 0.19540645\n",
      "Iteration 19, loss = 0.19335782\n",
      "Iteration 20, loss = 0.19104953\n",
      "Iteration 21, loss = 0.18922223\n",
      "Iteration 22, loss = 0.18719259\n",
      "Iteration 23, loss = 0.18553332\n",
      "Iteration 24, loss = 0.18369029\n",
      "Iteration 25, loss = 0.18216254\n",
      "Iteration 26, loss = 0.18051143\n",
      "Iteration 27, loss = 0.17931402\n",
      "Iteration 28, loss = 0.17782090\n",
      "Iteration 29, loss = 0.17637667\n",
      "Iteration 30, loss = 0.17532538\n",
      "Iteration 31, loss = 0.17394730\n",
      "Iteration 32, loss = 0.17280822\n",
      "Iteration 33, loss = 0.17190543\n",
      "Iteration 34, loss = 0.17059924\n",
      "Iteration 35, loss = 0.16953349\n",
      "Iteration 36, loss = 0.16849720\n",
      "Iteration 37, loss = 0.16757788\n",
      "Iteration 38, loss = 0.16654022\n",
      "Iteration 39, loss = 0.16566528\n",
      "Iteration 40, loss = 0.16479162\n",
      "Iteration 41, loss = 0.16398901\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37796819\n",
      "Iteration 2, loss = 0.23372516\n",
      "Iteration 3, loss = 0.21305732\n",
      "Iteration 4, loss = 0.19814780\n",
      "Iteration 5, loss = 0.18866474\n",
      "Iteration 6, loss = 0.18172159\n",
      "Iteration 7, loss = 0.17555323\n",
      "Iteration 8, loss = 0.17032703\n",
      "Iteration 9, loss = 0.16729110\n",
      "Iteration 10, loss = 0.16499645\n",
      "Iteration 11, loss = 0.16085302\n",
      "Iteration 12, loss = 0.16022632\n",
      "Iteration 13, loss = 0.15360220\n",
      "Iteration 14, loss = 0.15121043\n",
      "Iteration 15, loss = 0.15299894\n",
      "Iteration 16, loss = 0.14610728\n",
      "Iteration 17, loss = 0.14485276\n",
      "Iteration 18, loss = 0.14317930\n",
      "Iteration 19, loss = 0.14134986\n",
      "Iteration 20, loss = 0.14001685\n",
      "Iteration 21, loss = 0.13899228\n",
      "Iteration 22, loss = 0.14098995\n",
      "Iteration 23, loss = 0.13591302\n",
      "Iteration 24, loss = 0.13866023\n",
      "Iteration 25, loss = 0.13336001\n",
      "Iteration 26, loss = 0.13237667\n",
      "Iteration 27, loss = 0.13102994\n",
      "Iteration 28, loss = 0.13110547\n",
      "Iteration 29, loss = 0.12907763\n",
      "Iteration 30, loss = 0.12825251\n",
      "Iteration 31, loss = 0.12710145\n",
      "Iteration 32, loss = 0.13054777\n",
      "Iteration 33, loss = 0.12683997\n",
      "Iteration 34, loss = 0.12525106\n",
      "Iteration 35, loss = 0.12484264\n",
      "Iteration 36, loss = 0.12384637\n",
      "Iteration 37, loss = 0.12260714\n",
      "Iteration 38, loss = 0.12184849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.12321057\n",
      "Iteration 40, loss = 0.12076983\n",
      "Iteration 41, loss = 0.11979447\n",
      "Iteration 42, loss = 0.11905130\n",
      "Iteration 43, loss = 0.11893189\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36245093\n",
      "Iteration 2, loss = 0.23251704\n",
      "Iteration 3, loss = 0.20958851\n",
      "Iteration 4, loss = 0.19556376\n",
      "Iteration 5, loss = 0.18747372\n",
      "Iteration 6, loss = 0.18003235\n",
      "Iteration 7, loss = 0.17313748\n",
      "Iteration 8, loss = 0.16813853\n",
      "Iteration 9, loss = 0.16416761\n",
      "Iteration 10, loss = 0.16073681\n",
      "Iteration 11, loss = 0.15793880\n",
      "Iteration 12, loss = 0.15477315\n",
      "Iteration 13, loss = 0.15247062\n",
      "Iteration 14, loss = 0.15025939\n",
      "Iteration 15, loss = 0.14789942\n",
      "Iteration 16, loss = 0.14568197\n",
      "Iteration 17, loss = 0.14490369\n",
      "Iteration 18, loss = 0.14598348\n",
      "Iteration 19, loss = 0.14043102\n",
      "Iteration 20, loss = 0.13960568\n",
      "Iteration 21, loss = 0.13787964\n",
      "Iteration 22, loss = 0.13676148\n",
      "Iteration 23, loss = 0.13547771\n",
      "Iteration 24, loss = 0.13457843\n",
      "Iteration 25, loss = 0.13290740\n",
      "Iteration 26, loss = 0.13333709\n",
      "Iteration 27, loss = 0.13102680\n",
      "Iteration 28, loss = 0.13097723\n",
      "Iteration 29, loss = 0.12923407\n",
      "Iteration 30, loss = 0.12800928\n",
      "Iteration 31, loss = 0.12755233\n",
      "Iteration 32, loss = 0.12946788\n",
      "Iteration 33, loss = 0.12610286\n",
      "Iteration 34, loss = 0.12412073\n",
      "Iteration 35, loss = 0.12379210\n",
      "Iteration 36, loss = 0.12400077\n",
      "Iteration 37, loss = 0.12193280\n",
      "Iteration 38, loss = 0.12158000\n",
      "Iteration 39, loss = 0.12083427\n",
      "Iteration 40, loss = 0.12062948\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35392072\n",
      "Iteration 2, loss = 0.23477020\n",
      "Iteration 3, loss = 0.21231377\n",
      "Iteration 4, loss = 0.19828856\n",
      "Iteration 5, loss = 0.18899749\n",
      "Iteration 6, loss = 0.18150848\n",
      "Iteration 7, loss = 0.17574836\n",
      "Iteration 8, loss = 0.17082522\n",
      "Iteration 9, loss = 0.16631208\n",
      "Iteration 10, loss = 0.16342126\n",
      "Iteration 11, loss = 0.16066977\n",
      "Iteration 12, loss = 0.15805956\n",
      "Iteration 13, loss = 0.15450386\n",
      "Iteration 14, loss = 0.15272611\n",
      "Iteration 15, loss = 0.15150214\n",
      "Iteration 16, loss = 0.14871370\n",
      "Iteration 17, loss = 0.14671297\n",
      "Iteration 18, loss = 0.14640414\n",
      "Iteration 19, loss = 0.14360176\n",
      "Iteration 20, loss = 0.14224808\n",
      "Iteration 21, loss = 0.14248470\n",
      "Iteration 22, loss = 0.13953978\n",
      "Iteration 23, loss = 0.13850896\n",
      "Iteration 24, loss = 0.13693118\n",
      "Iteration 25, loss = 0.13632968\n",
      "Iteration 26, loss = 0.13503592\n",
      "Iteration 27, loss = 0.13387240\n",
      "Iteration 28, loss = 0.13328547\n",
      "Iteration 29, loss = 0.13456721\n",
      "Iteration 30, loss = 0.13179016\n",
      "Iteration 31, loss = 0.13053464\n",
      "Iteration 32, loss = 0.12986428\n",
      "Iteration 33, loss = 0.12886847\n",
      "Iteration 34, loss = 0.12901673\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35152255\n",
      "Iteration 2, loss = 0.23550374\n",
      "Iteration 3, loss = 0.21281083\n",
      "Iteration 4, loss = 0.20028076\n",
      "Iteration 5, loss = 0.18960814\n",
      "Iteration 6, loss = 0.18223988\n",
      "Iteration 7, loss = 0.17643500\n",
      "Iteration 8, loss = 0.17139264\n",
      "Iteration 9, loss = 0.16742646\n",
      "Iteration 10, loss = 0.16373915\n",
      "Iteration 11, loss = 0.16000573\n",
      "Iteration 12, loss = 0.15733957\n",
      "Iteration 13, loss = 0.15499306\n",
      "Iteration 14, loss = 0.15259449\n",
      "Iteration 15, loss = 0.15042441\n",
      "Iteration 16, loss = 0.14833049\n",
      "Iteration 17, loss = 0.14667034\n",
      "Iteration 18, loss = 0.14496879\n",
      "Iteration 19, loss = 0.14388440\n",
      "Iteration 20, loss = 0.14237907\n",
      "Iteration 21, loss = 0.14019726\n",
      "Iteration 22, loss = 0.13904054\n",
      "Iteration 23, loss = 0.13747253\n",
      "Iteration 24, loss = 0.13637521\n",
      "Iteration 25, loss = 0.13557435\n",
      "Iteration 26, loss = 0.13440580\n",
      "Iteration 27, loss = 0.13395662\n",
      "Iteration 28, loss = 0.13254693\n",
      "Iteration 29, loss = 0.13098002\n",
      "Iteration 30, loss = 0.13073518\n",
      "Iteration 31, loss = 0.12935241\n",
      "Iteration 32, loss = 0.12916187\n",
      "Iteration 33, loss = 0.12760549\n",
      "Iteration 34, loss = 0.12671292\n",
      "Iteration 35, loss = 0.12535794\n",
      "Iteration 36, loss = 0.12498065\n",
      "Iteration 37, loss = 0.12465817\n",
      "Iteration 38, loss = 0.12346912\n",
      "Iteration 39, loss = 0.12250377\n",
      "Iteration 40, loss = 0.12265785\n",
      "Iteration 41, loss = 0.12114171\n",
      "Iteration 42, loss = 0.12041929\n",
      "Iteration 43, loss = 0.11977355\n",
      "Iteration 44, loss = 0.11923213\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35601101\n",
      "Iteration 2, loss = 0.23092728\n",
      "Iteration 3, loss = 0.20783247\n",
      "Iteration 4, loss = 0.19498765\n",
      "Iteration 5, loss = 0.18611162\n",
      "Iteration 6, loss = 0.17846086\n",
      "Iteration 7, loss = 0.17397071\n",
      "Iteration 8, loss = 0.16884312\n",
      "Iteration 9, loss = 0.16527396\n",
      "Iteration 10, loss = 0.16231730\n",
      "Iteration 11, loss = 0.15897736\n",
      "Iteration 12, loss = 0.15591574\n",
      "Iteration 13, loss = 0.15313544\n",
      "Iteration 14, loss = 0.15119216\n",
      "Iteration 15, loss = 0.15169845\n",
      "Iteration 16, loss = 0.14745036\n",
      "Iteration 17, loss = 0.14563834\n",
      "Iteration 18, loss = 0.14513144\n",
      "Iteration 19, loss = 0.14341217\n",
      "Iteration 20, loss = 0.14118021\n",
      "Iteration 21, loss = 0.13966982\n",
      "Iteration 22, loss = 0.13853295\n",
      "Iteration 23, loss = 0.13735440\n",
      "Iteration 24, loss = 0.13611256\n",
      "Iteration 25, loss = 0.13521433\n",
      "Iteration 26, loss = 0.13418950\n",
      "Iteration 27, loss = 0.13311043\n",
      "Iteration 28, loss = 0.13230543\n",
      "Iteration 29, loss = 0.13065317\n",
      "Iteration 30, loss = 0.13035398\n",
      "Iteration 31, loss = 0.12949378\n",
      "Iteration 32, loss = 0.12852089\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30616771\n",
      "Iteration 2, loss = 0.20496550\n",
      "Iteration 3, loss = 0.18550330\n",
      "Iteration 4, loss = 0.17363521\n",
      "Iteration 5, loss = 0.16565900\n",
      "Iteration 6, loss = 0.15822555\n",
      "Iteration 7, loss = 0.15251567\n",
      "Iteration 8, loss = 0.14875381\n",
      "Iteration 9, loss = 0.15593377\n",
      "Iteration 10, loss = 0.14196787\n",
      "Iteration 11, loss = 0.14326197\n",
      "Iteration 12, loss = 0.13748248\n",
      "Iteration 13, loss = 0.13441771\n",
      "Iteration 14, loss = 0.13255901\n",
      "Iteration 15, loss = 0.13275088\n",
      "Iteration 16, loss = 0.13137578\n",
      "Iteration 17, loss = 0.12823041\n",
      "Iteration 18, loss = 0.12651225\n",
      "Iteration 19, loss = 0.12445974\n",
      "Iteration 20, loss = 0.12308585\n",
      "Iteration 21, loss = 0.12245410\n",
      "Iteration 22, loss = 0.12108675\n",
      "Iteration 23, loss = 0.11982526\n",
      "Iteration 24, loss = 0.11874146\n",
      "Iteration 25, loss = 0.11770905\n",
      "Iteration 26, loss = 0.11714212\n",
      "Iteration 27, loss = 0.11613272\n",
      "Iteration 28, loss = 0.11521277\n",
      "Iteration 29, loss = 0.11389513\n",
      "Iteration 30, loss = 0.11401397\n",
      "Iteration 31, loss = 0.11210482\n",
      "Iteration 32, loss = 0.11118926\n",
      "Iteration 33, loss = 0.11167648\n",
      "Iteration 34, loss = 0.10981526\n",
      "Iteration 35, loss = 0.10993792\n",
      "Iteration 36, loss = 0.10735418\n",
      "Iteration 37, loss = 0.10724763\n",
      "Iteration 38, loss = 0.11425934\n",
      "Iteration 39, loss = 0.10607143\n",
      "Iteration 40, loss = 0.11077662\n",
      "Iteration 41, loss = 0.10448774\n",
      "Iteration 42, loss = 0.10383925\n",
      "Iteration 43, loss = 0.10320918\n",
      "Iteration 44, loss = 0.10220502\n",
      "Iteration 45, loss = 0.10283531\n",
      "Iteration 46, loss = 0.10481395\n",
      "Iteration 47, loss = 0.10166746\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30774846\n",
      "Iteration 2, loss = 0.20526517\n",
      "Iteration 3, loss = 0.18526518\n",
      "Iteration 4, loss = 0.17490014\n",
      "Iteration 5, loss = 0.16470094\n",
      "Iteration 6, loss = 0.16031280\n",
      "Iteration 7, loss = 0.15293988\n",
      "Iteration 8, loss = 0.14865546\n",
      "Iteration 9, loss = 0.14472597\n",
      "Iteration 10, loss = 0.14245255\n",
      "Iteration 11, loss = 0.13977011\n",
      "Iteration 12, loss = 0.13726317\n",
      "Iteration 13, loss = 0.13519309\n",
      "Iteration 14, loss = 0.13355163\n",
      "Iteration 15, loss = 0.13058878\n",
      "Iteration 16, loss = 0.13237070\n",
      "Iteration 17, loss = 0.12756141\n",
      "Iteration 18, loss = 0.12587410\n",
      "Iteration 19, loss = 0.12513279\n",
      "Iteration 20, loss = 0.12286991\n",
      "Iteration 21, loss = 0.12166579\n",
      "Iteration 22, loss = 0.12242925\n",
      "Iteration 23, loss = 0.12036753\n",
      "Iteration 24, loss = 0.11872048\n",
      "Iteration 25, loss = 0.11741140\n",
      "Iteration 26, loss = 0.11672573\n",
      "Iteration 27, loss = 0.11545810\n",
      "Iteration 28, loss = 0.11419368\n",
      "Iteration 29, loss = 0.11341701\n",
      "Iteration 30, loss = 0.11238495\n",
      "Iteration 31, loss = 0.11198093\n",
      "Iteration 32, loss = 0.11446577\n",
      "Iteration 33, loss = 0.11163789\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.33303115\n",
      "Iteration 2, loss = 0.21476523\n",
      "Iteration 3, loss = 0.19558307\n",
      "Iteration 4, loss = 0.18402831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.17302907\n",
      "Iteration 6, loss = 0.16602860\n",
      "Iteration 7, loss = 0.15997966\n",
      "Iteration 8, loss = 0.15571333\n",
      "Iteration 9, loss = 0.15121526\n",
      "Iteration 10, loss = 0.14712213\n",
      "Iteration 11, loss = 0.14989501\n",
      "Iteration 12, loss = 0.14284212\n",
      "Iteration 13, loss = 0.14109185\n",
      "Iteration 14, loss = 0.13642254\n",
      "Iteration 15, loss = 0.13638832\n",
      "Iteration 16, loss = 0.13276578\n",
      "Iteration 17, loss = 0.13017731\n",
      "Iteration 18, loss = 0.12900127\n",
      "Iteration 19, loss = 0.12757956\n",
      "Iteration 20, loss = 0.12632641\n",
      "Iteration 21, loss = 0.12547594\n",
      "Iteration 22, loss = 0.12447095\n",
      "Iteration 23, loss = 0.12207005\n",
      "Iteration 24, loss = 0.12177223\n",
      "Iteration 25, loss = 0.12377356\n",
      "Iteration 26, loss = 0.12188654\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31496236\n",
      "Iteration 2, loss = 0.20306615\n",
      "Iteration 3, loss = 0.18375045\n",
      "Iteration 4, loss = 0.17184495\n",
      "Iteration 5, loss = 0.16389638\n",
      "Iteration 6, loss = 0.15730774\n",
      "Iteration 7, loss = 0.15289466\n",
      "Iteration 8, loss = 0.14841756\n",
      "Iteration 9, loss = 0.14602102\n",
      "Iteration 10, loss = 0.14272357\n",
      "Iteration 11, loss = 0.13974791\n",
      "Iteration 12, loss = 0.13903206\n",
      "Iteration 13, loss = 0.13556290\n",
      "Iteration 14, loss = 0.13539768\n",
      "Iteration 15, loss = 0.13166352\n",
      "Iteration 16, loss = 0.13004730\n",
      "Iteration 17, loss = 0.12925613\n",
      "Iteration 18, loss = 0.13139132\n",
      "Iteration 19, loss = 0.12561724\n",
      "Iteration 20, loss = 0.12516690\n",
      "Iteration 21, loss = 0.12369423\n",
      "Iteration 22, loss = 0.12261101\n",
      "Iteration 23, loss = 0.12056070\n",
      "Iteration 24, loss = 0.11995622\n",
      "Iteration 25, loss = 0.12193461\n",
      "Iteration 26, loss = 0.11766617\n",
      "Iteration 27, loss = 0.11671580\n",
      "Iteration 28, loss = 0.11679981\n",
      "Iteration 29, loss = 0.11455162\n",
      "Iteration 30, loss = 0.11514560\n",
      "Iteration 31, loss = 0.11299882\n",
      "Iteration 32, loss = 0.11477216\n",
      "Iteration 33, loss = 0.11184512\n",
      "Iteration 34, loss = 0.11078169\n",
      "Iteration 35, loss = 0.10896086\n",
      "Iteration 36, loss = 0.10840956\n",
      "Iteration 37, loss = 0.10829702\n",
      "Iteration 38, loss = 0.10659723\n",
      "Iteration 39, loss = 0.10941761\n",
      "Iteration 40, loss = 0.10631265\n",
      "Iteration 41, loss = 0.10527580\n",
      "Iteration 42, loss = 0.10344799\n",
      "Iteration 43, loss = 0.10385323\n",
      "Iteration 44, loss = 0.10336169\n",
      "Iteration 45, loss = 0.10168011\n",
      "Iteration 46, loss = 0.12791941\n",
      "Iteration 47, loss = 0.10656378\n",
      "Iteration 48, loss = 0.10049769\n",
      "Iteration 49, loss = 0.09913558\n",
      "Iteration 50, loss = 0.10051305\n",
      "Iteration 51, loss = 0.09773637\n",
      "Iteration 52, loss = 0.09730819\n",
      "Iteration 53, loss = 0.09598272\n",
      "Iteration 54, loss = 0.09626568\n",
      "Iteration 55, loss = 0.09461227\n",
      "Iteration 56, loss = 0.09380688\n",
      "Iteration 57, loss = 0.09646218\n",
      "Iteration 58, loss = 0.09402107\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30893894\n",
      "Iteration 2, loss = 0.20529348\n",
      "Iteration 3, loss = 0.18461294\n",
      "Iteration 4, loss = 0.17539929\n",
      "Iteration 5, loss = 0.16466069\n",
      "Iteration 6, loss = 0.15794889\n",
      "Iteration 7, loss = 0.15255260\n",
      "Iteration 8, loss = 0.15235857\n",
      "Iteration 9, loss = 0.14965171\n",
      "Iteration 10, loss = 0.14178420\n",
      "Iteration 11, loss = 0.13876330\n",
      "Iteration 12, loss = 0.13709335\n",
      "Iteration 13, loss = 0.13412505\n",
      "Iteration 14, loss = 0.13231127\n",
      "Iteration 15, loss = 0.12976452\n",
      "Iteration 16, loss = 0.12922228\n",
      "Iteration 17, loss = 0.12748921\n",
      "Iteration 18, loss = 0.12688937\n",
      "Iteration 19, loss = 0.12457024\n",
      "Iteration 20, loss = 0.12568318\n",
      "Iteration 21, loss = 0.12252795\n",
      "Iteration 22, loss = 0.12244327\n",
      "Iteration 23, loss = 0.12002643\n",
      "Iteration 24, loss = 0.11877653\n",
      "Iteration 25, loss = 0.11889154\n",
      "Iteration 26, loss = 0.11585512\n",
      "Iteration 27, loss = 0.11891360\n",
      "Iteration 28, loss = 0.11485520\n",
      "Iteration 29, loss = 0.11321532\n",
      "Iteration 30, loss = 0.11149058\n",
      "Iteration 31, loss = 0.11534063\n",
      "Iteration 32, loss = 0.11058122\n",
      "Iteration 33, loss = 0.10983188\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27256419\n",
      "Iteration 2, loss = 0.20634871\n",
      "Iteration 3, loss = 0.18898473\n",
      "Iteration 4, loss = 0.17871434\n",
      "Iteration 5, loss = 0.17104469\n",
      "Iteration 6, loss = 0.16680160\n",
      "Iteration 7, loss = 0.16252107\n",
      "Iteration 8, loss = 0.15858828\n",
      "Iteration 9, loss = 0.15579730\n",
      "Iteration 10, loss = 0.15420562\n",
      "Iteration 11, loss = 0.15132586\n",
      "Iteration 12, loss = 0.14978594\n",
      "Iteration 13, loss = 0.14754098\n",
      "Iteration 14, loss = 0.14656922\n",
      "Iteration 15, loss = 0.14503460\n",
      "Iteration 16, loss = 0.14479564\n",
      "Iteration 17, loss = 0.14320161\n",
      "Iteration 18, loss = 0.14133881\n",
      "Iteration 19, loss = 0.14117830\n",
      "Iteration 20, loss = 0.14046210\n",
      "Iteration 21, loss = 0.13913415\n",
      "Iteration 22, loss = 0.13875342\n",
      "Iteration 23, loss = 0.13784327\n",
      "Iteration 24, loss = 0.13612932\n",
      "Iteration 25, loss = 0.13596303\n",
      "Iteration 26, loss = 0.13550020\n",
      "Iteration 27, loss = 0.13439001\n",
      "Iteration 28, loss = 0.13426688\n",
      "Iteration 29, loss = 0.13382672\n",
      "Iteration 30, loss = 0.13223758\n",
      "Iteration 31, loss = 0.13223440\n",
      "Iteration 32, loss = 0.13182933\n",
      "Iteration 33, loss = 0.13167171\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27213382\n",
      "Iteration 2, loss = 0.20649027\n",
      "Iteration 3, loss = 0.18874084\n",
      "Iteration 4, loss = 0.17794088\n",
      "Iteration 5, loss = 0.17209067\n",
      "Iteration 6, loss = 0.16623509\n",
      "Iteration 7, loss = 0.16271574\n",
      "Iteration 8, loss = 0.15918614\n",
      "Iteration 9, loss = 0.15595977\n",
      "Iteration 10, loss = 0.15401475\n",
      "Iteration 11, loss = 0.15164208\n",
      "Iteration 12, loss = 0.15032494\n",
      "Iteration 13, loss = 0.14799732\n",
      "Iteration 14, loss = 0.14628069\n",
      "Iteration 15, loss = 0.14541998\n",
      "Iteration 16, loss = 0.14446528\n",
      "Iteration 17, loss = 0.14365845\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.27446063\n",
      "Iteration 2, loss = 0.20682626\n",
      "Iteration 3, loss = 0.19031355\n",
      "Iteration 4, loss = 0.18005268\n",
      "Iteration 5, loss = 0.17296039\n",
      "Iteration 6, loss = 0.16810493\n",
      "Iteration 7, loss = 0.16445819\n",
      "Iteration 8, loss = 0.15996748\n",
      "Iteration 9, loss = 0.15825103\n",
      "Iteration 10, loss = 0.15609167\n",
      "Iteration 11, loss = 0.15379970\n",
      "Iteration 12, loss = 0.15160836\n",
      "Iteration 13, loss = 0.15053613\n",
      "Iteration 14, loss = 0.14896726\n",
      "Iteration 15, loss = 0.14766689\n",
      "Iteration 16, loss = 0.14619337\n",
      "Iteration 17, loss = 0.14581580\n",
      "Iteration 18, loss = 0.14417673\n",
      "Iteration 19, loss = 0.14301579\n",
      "Iteration 20, loss = 0.14219780\n",
      "Iteration 21, loss = 0.14145358\n",
      "Iteration 22, loss = 0.13997687\n",
      "Iteration 23, loss = 0.13922212\n",
      "Iteration 24, loss = 0.13893423\n",
      "Iteration 25, loss = 0.13806506\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26812302\n",
      "Iteration 2, loss = 0.20402858\n",
      "Iteration 3, loss = 0.18781837\n",
      "Iteration 4, loss = 0.17865512\n",
      "Iteration 5, loss = 0.17270437\n",
      "Iteration 6, loss = 0.16652491\n",
      "Iteration 7, loss = 0.16410797\n",
      "Iteration 8, loss = 0.16049055\n",
      "Iteration 9, loss = 0.15830191\n",
      "Iteration 10, loss = 0.15559205\n",
      "Iteration 11, loss = 0.15382039\n",
      "Iteration 12, loss = 0.15142614\n",
      "Iteration 13, loss = 0.14980348\n",
      "Iteration 14, loss = 0.14828753\n",
      "Iteration 15, loss = 0.14747884\n",
      "Iteration 16, loss = 0.14507894\n",
      "Iteration 17, loss = 0.14418194\n",
      "Iteration 18, loss = 0.14447231\n",
      "Iteration 19, loss = 0.14291851\n",
      "Iteration 20, loss = 0.14200509\n",
      "Iteration 21, loss = 0.14064446\n",
      "Iteration 22, loss = 0.13971018\n",
      "Iteration 23, loss = 0.14011112\n",
      "Iteration 24, loss = 0.13930056\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26908778\n",
      "Iteration 2, loss = 0.20344121\n",
      "Iteration 3, loss = 0.18696732\n",
      "Iteration 4, loss = 0.17823518\n",
      "Iteration 5, loss = 0.17212686\n",
      "Iteration 6, loss = 0.16646727\n",
      "Iteration 7, loss = 0.16267600\n",
      "Iteration 8, loss = 0.15941837\n",
      "Iteration 9, loss = 0.15715409\n",
      "Iteration 10, loss = 0.15437171\n",
      "Iteration 11, loss = 0.15200342\n",
      "Iteration 12, loss = 0.15093990\n",
      "Iteration 13, loss = 0.14873115\n",
      "Iteration 14, loss = 0.14731721\n",
      "Iteration 15, loss = 0.14581420\n",
      "Iteration 16, loss = 0.14511762\n",
      "Iteration 17, loss = 0.14351560\n",
      "Iteration 18, loss = 0.14282314\n",
      "Iteration 19, loss = 0.14084456\n",
      "Iteration 20, loss = 0.14037832\n",
      "Iteration 21, loss = 0.14012727\n",
      "Iteration 22, loss = 0.13890951\n",
      "Iteration 23, loss = 0.13765345\n",
      "Iteration 24, loss = 0.13659707\n",
      "Iteration 25, loss = 0.13596404\n",
      "Iteration 26, loss = 0.13551542\n",
      "Iteration 27, loss = 0.13404938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.13458390\n",
      "Iteration 29, loss = 0.13367348\n",
      "Iteration 30, loss = 0.13305817\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22693049\n",
      "Iteration 2, loss = 0.17929734\n",
      "Iteration 3, loss = 0.16810360\n",
      "Iteration 4, loss = 0.16182591\n",
      "Iteration 5, loss = 0.15804398\n",
      "Iteration 6, loss = 0.15647174\n",
      "Iteration 7, loss = 0.15285886\n",
      "Iteration 8, loss = 0.15139221\n",
      "Iteration 9, loss = 0.14848496\n",
      "Iteration 10, loss = 0.14873737\n",
      "Iteration 11, loss = 0.14763085\n",
      "Iteration 12, loss = 0.14595765\n",
      "Iteration 13, loss = 0.14602866\n",
      "Iteration 14, loss = 0.14620365\n",
      "Iteration 15, loss = 0.14449178\n",
      "Iteration 16, loss = 0.14444609\n",
      "Iteration 17, loss = 0.14370806\n",
      "Iteration 18, loss = 0.14327479\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22353249\n",
      "Iteration 2, loss = 0.17765821\n",
      "Iteration 3, loss = 0.16819129\n",
      "Iteration 4, loss = 0.16067655\n",
      "Iteration 5, loss = 0.15836876\n",
      "Iteration 6, loss = 0.15590065\n",
      "Iteration 7, loss = 0.15149267\n",
      "Iteration 8, loss = 0.15160596\n",
      "Iteration 9, loss = 0.15088161\n",
      "Iteration 10, loss = 0.14868488\n",
      "Iteration 11, loss = 0.14705647\n",
      "Iteration 12, loss = 0.14669649\n",
      "Iteration 13, loss = 0.14659305\n",
      "Iteration 14, loss = 0.14428216\n",
      "Iteration 15, loss = 0.14544985\n",
      "Iteration 16, loss = 0.14174445\n",
      "Iteration 17, loss = 0.14323049\n",
      "Iteration 18, loss = 0.14314507\n",
      "Iteration 19, loss = 0.14331146\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22356747\n",
      "Iteration 2, loss = 0.18273682\n",
      "Iteration 3, loss = 0.17019776\n",
      "Iteration 4, loss = 0.16503719\n",
      "Iteration 5, loss = 0.16134940\n",
      "Iteration 6, loss = 0.15762477\n",
      "Iteration 7, loss = 0.15613740\n",
      "Iteration 8, loss = 0.15347800\n",
      "Iteration 9, loss = 0.15335334\n",
      "Iteration 10, loss = 0.14981217\n",
      "Iteration 11, loss = 0.14970134\n",
      "Iteration 12, loss = 0.14858111\n",
      "Iteration 13, loss = 0.14803876\n",
      "Iteration 14, loss = 0.14701884\n",
      "Iteration 15, loss = 0.14893225\n",
      "Iteration 16, loss = 0.14728250\n",
      "Iteration 17, loss = 0.14835732\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22433141\n",
      "Iteration 2, loss = 0.18157538\n",
      "Iteration 3, loss = 0.17006241\n",
      "Iteration 4, loss = 0.16429549\n",
      "Iteration 5, loss = 0.15961148\n",
      "Iteration 6, loss = 0.15792350\n",
      "Iteration 7, loss = 0.15497079\n",
      "Iteration 8, loss = 0.15247522\n",
      "Iteration 9, loss = 0.14964305\n",
      "Iteration 10, loss = 0.15016649\n",
      "Iteration 11, loss = 0.14878102\n",
      "Iteration 12, loss = 0.14884629\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22192899\n",
      "Iteration 2, loss = 0.18084918\n",
      "Iteration 3, loss = 0.16853295\n",
      "Iteration 4, loss = 0.16172571\n",
      "Iteration 5, loss = 0.15753966\n",
      "Iteration 6, loss = 0.15532982\n",
      "Iteration 7, loss = 0.15196031\n",
      "Iteration 8, loss = 0.15052924\n",
      "Iteration 9, loss = 0.15018194\n",
      "Iteration 10, loss = 0.14903951\n",
      "Iteration 11, loss = 0.14687668\n",
      "Iteration 12, loss = 0.14605997\n",
      "Iteration 13, loss = 0.14710538\n",
      "Iteration 14, loss = 0.14481723\n",
      "Iteration 15, loss = 0.14278166\n",
      "Iteration 16, loss = 0.14352178\n",
      "Iteration 17, loss = 0.14424861\n",
      "Iteration 18, loss = 0.14219354\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21762534\n",
      "Iteration 2, loss = 0.18271875\n",
      "Iteration 3, loss = 0.17242259\n",
      "Iteration 4, loss = 0.16945986\n",
      "Iteration 5, loss = 0.16767048\n",
      "Iteration 6, loss = 0.16487415\n",
      "Iteration 7, loss = 0.16502804\n",
      "Iteration 8, loss = 0.16279199\n",
      "Iteration 9, loss = 0.16283315\n",
      "Iteration 10, loss = 0.16014176\n",
      "Iteration 11, loss = 0.16030982\n",
      "Iteration 12, loss = 0.15873367\n",
      "Iteration 13, loss = 0.16127583\n",
      "Iteration 14, loss = 0.15740904\n",
      "Iteration 15, loss = 0.15960973\n",
      "Iteration 16, loss = 0.15631592\n",
      "Iteration 17, loss = 0.15732651\n",
      "Iteration 18, loss = 0.15789106\n",
      "Iteration 19, loss = 0.15609284\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22177913\n",
      "Iteration 2, loss = 0.18151549\n",
      "Iteration 3, loss = 0.17509018\n",
      "Iteration 4, loss = 0.17192663\n",
      "Iteration 5, loss = 0.16572038\n",
      "Iteration 6, loss = 0.16480673\n",
      "Iteration 7, loss = 0.16426751\n",
      "Iteration 8, loss = 0.16383247\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22122351\n",
      "Iteration 2, loss = 0.18320583\n",
      "Iteration 3, loss = 0.17575439\n",
      "Iteration 4, loss = 0.17214295\n",
      "Iteration 5, loss = 0.17120044\n",
      "Iteration 6, loss = 0.16686580\n",
      "Iteration 7, loss = 0.16710766\n",
      "Iteration 8, loss = 0.16501388\n",
      "Iteration 9, loss = 0.16542777\n",
      "Iteration 10, loss = 0.16463752\n",
      "Iteration 11, loss = 0.16104150\n",
      "Iteration 12, loss = 0.16383340\n",
      "Iteration 13, loss = 0.16149002\n",
      "Iteration 14, loss = 0.16087150\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22127763\n",
      "Iteration 2, loss = 0.18307567\n",
      "Iteration 3, loss = 0.17590471\n",
      "Iteration 4, loss = 0.17206966\n",
      "Iteration 5, loss = 0.17029562\n",
      "Iteration 6, loss = 0.16607380\n",
      "Iteration 7, loss = 0.16566249\n",
      "Iteration 8, loss = 0.16481580\n",
      "Iteration 9, loss = 0.16274578\n",
      "Iteration 10, loss = 0.16039278\n",
      "Iteration 11, loss = 0.16248718\n",
      "Iteration 12, loss = 0.16156601\n",
      "Iteration 13, loss = 0.16327103\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21892652\n",
      "Iteration 2, loss = 0.18083264\n",
      "Iteration 3, loss = 0.17379891\n",
      "Iteration 4, loss = 0.16929144\n",
      "Iteration 5, loss = 0.16820622\n",
      "Iteration 6, loss = 0.16578202\n",
      "Iteration 7, loss = 0.16504041\n",
      "Iteration 8, loss = 0.16414695\n",
      "Iteration 9, loss = 0.16151305\n",
      "Iteration 10, loss = 0.16109747\n",
      "Iteration 11, loss = 0.16158047\n",
      "Iteration 12, loss = 0.15854301\n",
      "Iteration 13, loss = 0.16034614\n",
      "Iteration 14, loss = 0.15976330\n",
      "Iteration 15, loss = 0.15760810\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36461952\n",
      "Iteration 2, loss = 0.24066567\n",
      "Iteration 3, loss = 0.21738581\n",
      "Iteration 4, loss = 0.20394883\n",
      "Iteration 5, loss = 0.19468616\n",
      "Iteration 6, loss = 0.18806203\n",
      "Iteration 7, loss = 0.18237358\n",
      "Iteration 8, loss = 0.17752393\n",
      "Iteration 9, loss = 0.17385865\n",
      "Iteration 10, loss = 0.17000999\n",
      "Iteration 11, loss = 0.16717190\n",
      "Iteration 12, loss = 0.16427242\n",
      "Iteration 13, loss = 0.16159249\n",
      "Iteration 14, loss = 0.15935910\n",
      "Iteration 15, loss = 0.15729323\n",
      "Iteration 16, loss = 0.15504026\n",
      "Iteration 17, loss = 0.15316290\n",
      "Iteration 18, loss = 0.15150022\n",
      "Iteration 19, loss = 0.14956877\n",
      "Iteration 20, loss = 0.14826765\n",
      "Iteration 21, loss = 0.14713066\n",
      "Iteration 22, loss = 0.14553565\n",
      "Iteration 23, loss = 0.14481695\n",
      "Iteration 24, loss = 0.14366406\n",
      "Iteration 25, loss = 0.14212028\n",
      "Iteration 26, loss = 0.14128615\n",
      "Iteration 27, loss = 0.14017896\n",
      "Iteration 28, loss = 0.13940903\n",
      "Iteration 29, loss = 0.13839249\n",
      "Iteration 30, loss = 0.13746447\n",
      "Iteration 31, loss = 0.13657279\n",
      "Iteration 32, loss = 0.13597636\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37624554\n",
      "Iteration 2, loss = 0.25341696\n",
      "Iteration 3, loss = 0.22738882\n",
      "Iteration 4, loss = 0.21273010\n",
      "Iteration 5, loss = 0.20218998\n",
      "Iteration 6, loss = 0.19439396\n",
      "Iteration 7, loss = 0.18822605\n",
      "Iteration 8, loss = 0.18312645\n",
      "Iteration 9, loss = 0.17861838\n",
      "Iteration 10, loss = 0.17420968\n",
      "Iteration 11, loss = 0.17094920\n",
      "Iteration 12, loss = 0.16805341\n",
      "Iteration 13, loss = 0.16502445\n",
      "Iteration 14, loss = 0.16291391\n",
      "Iteration 15, loss = 0.16018425\n",
      "Iteration 16, loss = 0.15782290\n",
      "Iteration 17, loss = 0.15611411\n",
      "Iteration 18, loss = 0.15428838\n",
      "Iteration 19, loss = 0.15229773\n",
      "Iteration 20, loss = 0.15147003\n",
      "Iteration 21, loss = 0.14936377\n",
      "Iteration 22, loss = 0.14830941\n",
      "Iteration 23, loss = 0.14717205\n",
      "Iteration 24, loss = 0.14586409\n",
      "Iteration 25, loss = 0.14472251\n",
      "Iteration 26, loss = 0.14365992\n",
      "Iteration 27, loss = 0.14293612\n",
      "Iteration 28, loss = 0.14148578\n",
      "Iteration 29, loss = 0.14034195\n",
      "Iteration 30, loss = 0.13978779\n",
      "Iteration 31, loss = 0.13871753\n",
      "Iteration 32, loss = 0.13785291\n",
      "Iteration 33, loss = 0.13718712\n",
      "Iteration 34, loss = 0.13609218\n",
      "Iteration 35, loss = 0.13561026\n",
      "Iteration 36, loss = 0.13491950\n",
      "Iteration 37, loss = 0.13431227\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37231317\n",
      "Iteration 2, loss = 0.25317907\n",
      "Iteration 3, loss = 0.22910181\n",
      "Iteration 4, loss = 0.21496240\n",
      "Iteration 5, loss = 0.20509178\n",
      "Iteration 6, loss = 0.19767724\n",
      "Iteration 7, loss = 0.19142216\n",
      "Iteration 8, loss = 0.18652587\n",
      "Iteration 9, loss = 0.18157344\n",
      "Iteration 10, loss = 0.17792641\n",
      "Iteration 11, loss = 0.17464561\n",
      "Iteration 12, loss = 0.17159344\n",
      "Iteration 13, loss = 0.16877275\n",
      "Iteration 14, loss = 0.16619350\n",
      "Iteration 15, loss = 0.16396493\n",
      "Iteration 16, loss = 0.16202895\n",
      "Iteration 17, loss = 0.15982989\n",
      "Iteration 18, loss = 0.15808094\n",
      "Iteration 19, loss = 0.15664960\n",
      "Iteration 20, loss = 0.15495211\n",
      "Iteration 21, loss = 0.15307927\n",
      "Iteration 22, loss = 0.15213131\n",
      "Iteration 23, loss = 0.15092394\n",
      "Iteration 24, loss = 0.14963798\n",
      "Iteration 25, loss = 0.14870476\n",
      "Iteration 26, loss = 0.14718546\n",
      "Iteration 27, loss = 0.14636497\n",
      "Iteration 28, loss = 0.14517217\n",
      "Iteration 29, loss = 0.14429749\n",
      "Iteration 30, loss = 0.14319540\n",
      "Iteration 31, loss = 0.14244798\n",
      "Iteration 32, loss = 0.14165942\n",
      "Iteration 33, loss = 0.14087897\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38837739\n",
      "Iteration 2, loss = 0.25143889\n",
      "Iteration 3, loss = 0.22702430\n",
      "Iteration 4, loss = 0.21266825\n",
      "Iteration 5, loss = 0.20259816\n",
      "Iteration 6, loss = 0.19491863\n",
      "Iteration 7, loss = 0.18885544\n",
      "Iteration 8, loss = 0.18378330\n",
      "Iteration 9, loss = 0.17952969\n",
      "Iteration 10, loss = 0.17580145\n",
      "Iteration 11, loss = 0.17275584\n",
      "Iteration 12, loss = 0.16949850\n",
      "Iteration 13, loss = 0.16717307\n",
      "Iteration 14, loss = 0.16479700\n",
      "Iteration 15, loss = 0.16237977\n",
      "Iteration 16, loss = 0.16022798\n",
      "Iteration 17, loss = 0.15862006\n",
      "Iteration 18, loss = 0.15667835\n",
      "Iteration 19, loss = 0.15505518\n",
      "Iteration 20, loss = 0.15364507\n",
      "Iteration 21, loss = 0.15211978\n",
      "Iteration 22, loss = 0.15106025\n",
      "Iteration 23, loss = 0.14946972\n",
      "Iteration 24, loss = 0.14857225\n",
      "Iteration 25, loss = 0.14726311\n",
      "Iteration 26, loss = 0.14618476\n",
      "Iteration 27, loss = 0.14527519\n",
      "Iteration 28, loss = 0.14425458\n",
      "Iteration 29, loss = 0.14334736\n",
      "Iteration 30, loss = 0.14270375\n",
      "Iteration 31, loss = 0.14174093\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38852886\n",
      "Iteration 2, loss = 0.24627117\n",
      "Iteration 3, loss = 0.22204602\n",
      "Iteration 4, loss = 0.20819354\n",
      "Iteration 5, loss = 0.19818068\n",
      "Iteration 6, loss = 0.19102562\n",
      "Iteration 7, loss = 0.18508377\n",
      "Iteration 8, loss = 0.17998875\n",
      "Iteration 9, loss = 0.17601720\n",
      "Iteration 10, loss = 0.17267097\n",
      "Iteration 11, loss = 0.16921789\n",
      "Iteration 12, loss = 0.16587097\n",
      "Iteration 13, loss = 0.16332411\n",
      "Iteration 14, loss = 0.16100187\n",
      "Iteration 15, loss = 0.15887722\n",
      "Iteration 16, loss = 0.15683995\n",
      "Iteration 17, loss = 0.15484108\n",
      "Iteration 18, loss = 0.15323284\n",
      "Iteration 19, loss = 0.15182869\n",
      "Iteration 20, loss = 0.15001340\n",
      "Iteration 21, loss = 0.14869106\n",
      "Iteration 22, loss = 0.14753578\n",
      "Iteration 23, loss = 0.14603234\n",
      "Iteration 24, loss = 0.14516266\n",
      "Iteration 25, loss = 0.14392647\n",
      "Iteration 26, loss = 0.14295059\n",
      "Iteration 27, loss = 0.14181316\n",
      "Iteration 28, loss = 0.14078359\n",
      "Iteration 29, loss = 0.14006036\n",
      "Iteration 30, loss = 0.13887002\n",
      "Iteration 31, loss = 0.13834183\n",
      "Iteration 32, loss = 0.13754055\n",
      "Iteration 33, loss = 0.13688629\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26088562\n",
      "Iteration 2, loss = 0.18647677\n",
      "Iteration 3, loss = 0.16947865\n",
      "Iteration 4, loss = 0.15834384\n",
      "Iteration 5, loss = 0.15100489\n",
      "Iteration 6, loss = 0.14563154\n",
      "Iteration 7, loss = 0.14216015\n",
      "Iteration 8, loss = 0.13859412\n",
      "Iteration 9, loss = 0.13548853\n",
      "Iteration 10, loss = 0.13256479\n",
      "Iteration 11, loss = 0.13110666\n",
      "Iteration 12, loss = 0.12860676\n",
      "Iteration 13, loss = 0.12602789\n",
      "Iteration 14, loss = 0.12463475\n",
      "Iteration 15, loss = 0.12264857\n",
      "Iteration 16, loss = 0.12139641\n",
      "Iteration 17, loss = 0.12014826\n",
      "Iteration 18, loss = 0.11939856\n",
      "Iteration 19, loss = 0.11697495\n",
      "Iteration 20, loss = 0.11597512\n",
      "Iteration 21, loss = 0.11525614\n",
      "Iteration 22, loss = 0.11283835\n",
      "Iteration 23, loss = 0.11203119\n",
      "Iteration 24, loss = 0.11125749\n",
      "Iteration 25, loss = 0.11075499\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26259749\n",
      "Iteration 2, loss = 0.18896015\n",
      "Iteration 3, loss = 0.17137531\n",
      "Iteration 4, loss = 0.16061440\n",
      "Iteration 5, loss = 0.15412806\n",
      "Iteration 6, loss = 0.14767276\n",
      "Iteration 7, loss = 0.14300059\n",
      "Iteration 8, loss = 0.13972443\n",
      "Iteration 9, loss = 0.13657418\n",
      "Iteration 10, loss = 0.13357999\n",
      "Iteration 11, loss = 0.13203993\n",
      "Iteration 12, loss = 0.13044793\n",
      "Iteration 13, loss = 0.12794645\n",
      "Iteration 14, loss = 0.12545428\n",
      "Iteration 15, loss = 0.12350625\n",
      "Iteration 16, loss = 0.12296428\n",
      "Iteration 17, loss = 0.12115490\n",
      "Iteration 18, loss = 0.11985463\n",
      "Iteration 19, loss = 0.11823621\n",
      "Iteration 20, loss = 0.11701323\n",
      "Iteration 21, loss = 0.11610544\n",
      "Iteration 22, loss = 0.11579722\n",
      "Iteration 23, loss = 0.11373035\n",
      "Iteration 24, loss = 0.11258391\n",
      "Iteration 25, loss = 0.11178494\n",
      "Iteration 26, loss = 0.11090017\n",
      "Iteration 27, loss = 0.10973166\n",
      "Iteration 28, loss = 0.10876489\n",
      "Iteration 29, loss = 0.10830473\n",
      "Iteration 30, loss = 0.10724795\n",
      "Iteration 31, loss = 0.10649413\n",
      "Iteration 32, loss = 0.10529633\n",
      "Iteration 33, loss = 0.10481783\n",
      "Iteration 34, loss = 0.10336995\n",
      "Iteration 35, loss = 0.10349335\n",
      "Iteration 36, loss = 0.10229113\n",
      "Iteration 37, loss = 0.10269805\n",
      "Iteration 38, loss = 0.10070340\n",
      "Iteration 39, loss = 0.10027386\n",
      "Iteration 40, loss = 0.09866223\n",
      "Iteration 41, loss = 0.09949635\n",
      "Iteration 42, loss = 0.09937048\n",
      "Iteration 43, loss = 0.09746492\n",
      "Iteration 44, loss = 0.09782681\n",
      "Iteration 45, loss = 0.09656340\n",
      "Iteration 46, loss = 0.09586074\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25709886\n",
      "Iteration 2, loss = 0.18860176\n",
      "Iteration 3, loss = 0.17081799\n",
      "Iteration 4, loss = 0.16126527\n",
      "Iteration 5, loss = 0.15361696\n",
      "Iteration 6, loss = 0.14871063\n",
      "Iteration 7, loss = 0.14466424\n",
      "Iteration 8, loss = 0.14112216\n",
      "Iteration 9, loss = 0.13930279\n",
      "Iteration 10, loss = 0.13547809\n",
      "Iteration 11, loss = 0.13400256\n",
      "Iteration 12, loss = 0.13169244\n",
      "Iteration 13, loss = 0.12988822\n",
      "Iteration 14, loss = 0.12755328\n",
      "Iteration 15, loss = 0.12701867\n",
      "Iteration 16, loss = 0.12557929\n",
      "Iteration 17, loss = 0.12407959\n",
      "Iteration 18, loss = 0.12206343\n",
      "Iteration 19, loss = 0.12132376\n",
      "Iteration 20, loss = 0.11984336\n",
      "Iteration 21, loss = 0.11931354\n",
      "Iteration 22, loss = 0.11803423\n",
      "Iteration 23, loss = 0.11604650\n",
      "Iteration 24, loss = 0.11488878\n",
      "Iteration 25, loss = 0.11536339\n",
      "Iteration 26, loss = 0.11255630\n",
      "Iteration 27, loss = 0.11272954\n",
      "Iteration 28, loss = 0.11047974\n",
      "Iteration 29, loss = 0.11089474\n",
      "Iteration 30, loss = 0.10983875\n",
      "Iteration 31, loss = 0.10817190\n",
      "Iteration 32, loss = 0.10739704\n",
      "Iteration 33, loss = 0.10681677\n",
      "Iteration 34, loss = 0.10497880\n",
      "Iteration 35, loss = 0.10511323\n",
      "Iteration 36, loss = 0.10469722\n",
      "Iteration 37, loss = 0.10270590\n",
      "Iteration 38, loss = 0.10290928\n",
      "Iteration 39, loss = 0.10334671\n",
      "Iteration 40, loss = 0.10135531\n",
      "Iteration 41, loss = 0.09972972\n",
      "Iteration 42, loss = 0.10029196\n",
      "Iteration 43, loss = 0.10004248\n",
      "Iteration 44, loss = 0.09867150\n",
      "Iteration 45, loss = 0.09862249\n",
      "Iteration 46, loss = 0.09743204\n",
      "Iteration 47, loss = 0.09721875\n",
      "Iteration 48, loss = 0.09715150\n",
      "Iteration 49, loss = 0.09590144\n",
      "Iteration 50, loss = 0.09478424\n",
      "Iteration 51, loss = 0.09506561\n",
      "Iteration 52, loss = 0.09541616\n",
      "Iteration 53, loss = 0.09252209\n",
      "Iteration 54, loss = 0.09280943\n",
      "Iteration 55, loss = 0.09333833\n",
      "Iteration 56, loss = 0.09165824\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25793132\n",
      "Iteration 2, loss = 0.18836964\n",
      "Iteration 3, loss = 0.17164779\n",
      "Iteration 4, loss = 0.16379294\n",
      "Iteration 5, loss = 0.15443292\n",
      "Iteration 6, loss = 0.14976651\n",
      "Iteration 7, loss = 0.14579428\n",
      "Iteration 8, loss = 0.14176141\n",
      "Iteration 9, loss = 0.13804438\n",
      "Iteration 10, loss = 0.13586299\n",
      "Iteration 11, loss = 0.13517704\n",
      "Iteration 12, loss = 0.13279963\n",
      "Iteration 13, loss = 0.13094723\n",
      "Iteration 14, loss = 0.12756042\n",
      "Iteration 15, loss = 0.12644896\n",
      "Iteration 16, loss = 0.12414621\n",
      "Iteration 17, loss = 0.12288124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.12121916\n",
      "Iteration 19, loss = 0.12041675\n",
      "Iteration 20, loss = 0.11915007\n",
      "Iteration 21, loss = 0.11718574\n",
      "Iteration 22, loss = 0.11656881\n",
      "Iteration 23, loss = 0.11562321\n",
      "Iteration 24, loss = 0.11349819\n",
      "Iteration 25, loss = 0.11267465\n",
      "Iteration 26, loss = 0.11267015\n",
      "Iteration 27, loss = 0.11158524\n",
      "Iteration 28, loss = 0.11117361\n",
      "Iteration 29, loss = 0.10939748\n",
      "Iteration 30, loss = 0.10859324\n",
      "Iteration 31, loss = 0.10766611\n",
      "Iteration 32, loss = 0.10609113\n",
      "Iteration 33, loss = 0.10647753\n",
      "Iteration 34, loss = 0.10511816\n",
      "Iteration 35, loss = 0.10390790\n",
      "Iteration 36, loss = 0.10301501\n",
      "Iteration 37, loss = 0.10272422\n",
      "Iteration 38, loss = 0.10336335\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25330140\n",
      "Iteration 2, loss = 0.18492862\n",
      "Iteration 3, loss = 0.16903591\n",
      "Iteration 4, loss = 0.15880676\n",
      "Iteration 5, loss = 0.15235888\n",
      "Iteration 6, loss = 0.14725544\n",
      "Iteration 7, loss = 0.14193001\n",
      "Iteration 8, loss = 0.13919863\n",
      "Iteration 9, loss = 0.13613303\n",
      "Iteration 10, loss = 0.13500480\n",
      "Iteration 11, loss = 0.13180243\n",
      "Iteration 12, loss = 0.13064309\n",
      "Iteration 13, loss = 0.12803643\n",
      "Iteration 14, loss = 0.12604080\n",
      "Iteration 15, loss = 0.12484200\n",
      "Iteration 16, loss = 0.12331830\n",
      "Iteration 17, loss = 0.12178643\n",
      "Iteration 18, loss = 0.12084778\n",
      "Iteration 19, loss = 0.11928307\n",
      "Iteration 20, loss = 0.11844005\n",
      "Iteration 21, loss = 0.11850056\n",
      "Iteration 22, loss = 0.11566817\n",
      "Iteration 23, loss = 0.11520362\n",
      "Iteration 24, loss = 0.11428842\n",
      "Iteration 25, loss = 0.11226555\n",
      "Iteration 26, loss = 0.11210707\n",
      "Iteration 27, loss = 0.11136291\n",
      "Iteration 28, loss = 0.11208969\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23028772\n",
      "Iteration 2, loss = 0.16892762\n",
      "Iteration 3, loss = 0.15694126\n",
      "Iteration 4, loss = 0.14715435\n",
      "Iteration 5, loss = 0.14247921\n",
      "Iteration 6, loss = 0.13756520\n",
      "Iteration 7, loss = 0.13361286\n",
      "Iteration 8, loss = 0.13049050\n",
      "Iteration 9, loss = 0.12726689\n",
      "Iteration 10, loss = 0.12637330\n",
      "Iteration 11, loss = 0.12312558\n",
      "Iteration 12, loss = 0.12253658\n",
      "Iteration 13, loss = 0.12093570\n",
      "Iteration 14, loss = 0.11850605\n",
      "Iteration 15, loss = 0.11651060\n",
      "Iteration 16, loss = 0.11615057\n",
      "Iteration 17, loss = 0.11354052\n",
      "Iteration 18, loss = 0.11321836\n",
      "Iteration 19, loss = 0.11234498\n",
      "Iteration 20, loss = 0.11026785\n",
      "Iteration 21, loss = 0.11017581\n",
      "Iteration 22, loss = 0.11028573\n",
      "Iteration 23, loss = 0.10689538\n",
      "Iteration 24, loss = 0.10605569\n",
      "Iteration 25, loss = 0.10492151\n",
      "Iteration 26, loss = 0.10338568\n",
      "Iteration 27, loss = 0.10330163\n",
      "Iteration 28, loss = 0.10128458\n",
      "Iteration 29, loss = 0.10083561\n",
      "Iteration 30, loss = 0.10053079\n",
      "Iteration 31, loss = 0.09916414\n",
      "Iteration 32, loss = 0.09758030\n",
      "Iteration 33, loss = 0.09650583\n",
      "Iteration 34, loss = 0.09837518\n",
      "Iteration 35, loss = 0.09588531\n",
      "Iteration 36, loss = 0.09573036\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23407679\n",
      "Iteration 2, loss = 0.17015687\n",
      "Iteration 3, loss = 0.15644601\n",
      "Iteration 4, loss = 0.14723361\n",
      "Iteration 5, loss = 0.14025729\n",
      "Iteration 6, loss = 0.13737205\n",
      "Iteration 7, loss = 0.13476990\n",
      "Iteration 8, loss = 0.13006236\n",
      "Iteration 9, loss = 0.12543910\n",
      "Iteration 10, loss = 0.12543171\n",
      "Iteration 11, loss = 0.12266982\n",
      "Iteration 12, loss = 0.11968600\n",
      "Iteration 13, loss = 0.11905827\n",
      "Iteration 14, loss = 0.11621062\n",
      "Iteration 15, loss = 0.11516225\n",
      "Iteration 16, loss = 0.11345720\n",
      "Iteration 17, loss = 0.11229607\n",
      "Iteration 18, loss = 0.11278198\n",
      "Iteration 19, loss = 0.10654048\n",
      "Iteration 20, loss = 0.10768825\n",
      "Iteration 21, loss = 0.10549267\n",
      "Iteration 22, loss = 0.10526629\n",
      "Iteration 23, loss = 0.10223690\n",
      "Iteration 24, loss = 0.10169960\n",
      "Iteration 25, loss = 0.10287340\n",
      "Iteration 26, loss = 0.10170932\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23404188\n",
      "Iteration 2, loss = 0.17063822\n",
      "Iteration 3, loss = 0.15794943\n",
      "Iteration 4, loss = 0.15066455\n",
      "Iteration 5, loss = 0.14385160\n",
      "Iteration 6, loss = 0.13956051\n",
      "Iteration 7, loss = 0.13635889\n",
      "Iteration 8, loss = 0.13321464\n",
      "Iteration 9, loss = 0.13145461\n",
      "Iteration 10, loss = 0.12959379\n",
      "Iteration 11, loss = 0.12913779\n",
      "Iteration 12, loss = 0.12536151\n",
      "Iteration 13, loss = 0.12354196\n",
      "Iteration 14, loss = 0.12102031\n",
      "Iteration 15, loss = 0.11812366\n",
      "Iteration 16, loss = 0.11700355\n",
      "Iteration 17, loss = 0.12263351\n",
      "Iteration 18, loss = 0.11486975\n",
      "Iteration 19, loss = 0.11470986\n",
      "Iteration 20, loss = 0.11278257\n",
      "Iteration 21, loss = 0.11176110\n",
      "Iteration 22, loss = 0.11082293\n",
      "Iteration 23, loss = 0.10669056\n",
      "Iteration 24, loss = 0.10995678\n",
      "Iteration 25, loss = 0.10771194\n",
      "Iteration 26, loss = 0.10557818\n",
      "Iteration 27, loss = 0.10438375\n",
      "Iteration 28, loss = 0.10352598\n",
      "Iteration 29, loss = 0.10397466\n",
      "Iteration 30, loss = 0.10319248\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23519254\n",
      "Iteration 2, loss = 0.17150468\n",
      "Iteration 3, loss = 0.15741374\n",
      "Iteration 4, loss = 0.15042882\n",
      "Iteration 5, loss = 0.14452583\n",
      "Iteration 6, loss = 0.13962893\n",
      "Iteration 7, loss = 0.13566593\n",
      "Iteration 8, loss = 0.13242241\n",
      "Iteration 9, loss = 0.13111503\n",
      "Iteration 10, loss = 0.12849435\n",
      "Iteration 11, loss = 0.12477451\n",
      "Iteration 12, loss = 0.12242152\n",
      "Iteration 13, loss = 0.12139659\n",
      "Iteration 14, loss = 0.11823737\n",
      "Iteration 15, loss = 0.11812091\n",
      "Iteration 16, loss = 0.11468534\n",
      "Iteration 17, loss = 0.11219127\n",
      "Iteration 18, loss = 0.11254237\n",
      "Iteration 19, loss = 0.11116757\n",
      "Iteration 20, loss = 0.10998509\n",
      "Iteration 21, loss = 0.10793885\n",
      "Iteration 22, loss = 0.10618640\n",
      "Iteration 23, loss = 0.10623090\n",
      "Iteration 24, loss = 0.10476488\n",
      "Iteration 25, loss = 0.10210846\n",
      "Iteration 26, loss = 0.10322601\n",
      "Iteration 27, loss = 0.10069515\n",
      "Iteration 28, loss = 0.10181544\n",
      "Iteration 29, loss = 0.09852913\n",
      "Iteration 30, loss = 0.09861635\n",
      "Iteration 31, loss = 0.10409036\n",
      "Iteration 32, loss = 0.09740649\n",
      "Iteration 33, loss = 0.09590752\n",
      "Iteration 34, loss = 0.09528444\n",
      "Iteration 35, loss = 0.09394597\n",
      "Iteration 36, loss = 0.09336624\n",
      "Iteration 37, loss = 0.09609528\n",
      "Iteration 38, loss = 0.09077499\n",
      "Iteration 39, loss = 0.09169431\n",
      "Iteration 40, loss = 0.09075530\n",
      "Iteration 41, loss = 0.09001528\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23439338\n",
      "Iteration 2, loss = 0.16887769\n",
      "Iteration 3, loss = 0.15438801\n",
      "Iteration 4, loss = 0.14796631\n",
      "Iteration 5, loss = 0.14184790\n",
      "Iteration 6, loss = 0.13736085\n",
      "Iteration 7, loss = 0.13647467\n",
      "Iteration 8, loss = 0.13229663\n",
      "Iteration 9, loss = 0.12846382\n",
      "Iteration 10, loss = 0.12762250\n",
      "Iteration 11, loss = 0.12333935\n",
      "Iteration 12, loss = 0.12448533\n",
      "Iteration 13, loss = 0.12014996\n",
      "Iteration 14, loss = 0.11950412\n",
      "Iteration 15, loss = 0.11759764\n",
      "Iteration 16, loss = 0.11591850\n",
      "Iteration 17, loss = 0.11521637\n",
      "Iteration 18, loss = 0.11328168\n",
      "Iteration 19, loss = 0.11065522\n",
      "Iteration 20, loss = 0.11033438\n",
      "Iteration 21, loss = 0.10677989\n",
      "Iteration 22, loss = 0.10617024\n",
      "Iteration 23, loss = 0.10539733\n",
      "Iteration 24, loss = 0.10467055\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43159442\n",
      "Iteration 2, loss = 0.27845951\n",
      "Iteration 3, loss = 0.25065348\n",
      "Iteration 4, loss = 0.23449673\n",
      "Iteration 5, loss = 0.22336649\n",
      "Iteration 6, loss = 0.21499969\n",
      "Iteration 7, loss = 0.20840213\n",
      "Iteration 8, loss = 0.20281010\n",
      "Iteration 9, loss = 0.19830339\n",
      "Iteration 10, loss = 0.19424497\n",
      "Iteration 11, loss = 0.19061047\n",
      "Iteration 12, loss = 0.18753129\n",
      "Iteration 13, loss = 0.18478383\n",
      "Iteration 14, loss = 0.18224133\n",
      "Iteration 15, loss = 0.17982658\n",
      "Iteration 16, loss = 0.17741719\n",
      "Iteration 17, loss = 0.17527925\n",
      "Iteration 18, loss = 0.17316100\n",
      "Iteration 19, loss = 0.17170690\n",
      "Iteration 20, loss = 0.16996546\n",
      "Iteration 21, loss = 0.16833208\n",
      "Iteration 22, loss = 0.16666738\n",
      "Iteration 23, loss = 0.16585724\n",
      "Iteration 24, loss = 0.16404715\n",
      "Iteration 25, loss = 0.16282354\n",
      "Iteration 26, loss = 0.16179476\n",
      "Iteration 27, loss = 0.16019324\n",
      "Iteration 28, loss = 0.15895788\n",
      "Iteration 29, loss = 0.15805473\n",
      "Iteration 30, loss = 0.15668020\n",
      "Iteration 31, loss = 0.15586680\n",
      "Iteration 32, loss = 0.15468194\n",
      "Iteration 33, loss = 0.15364217\n",
      "Iteration 34, loss = 0.15277524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.15185034\n",
      "Iteration 36, loss = 0.15097153\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44433403\n",
      "Iteration 2, loss = 0.28515106\n",
      "Iteration 3, loss = 0.25661649\n",
      "Iteration 4, loss = 0.23963529\n",
      "Iteration 5, loss = 0.22803764\n",
      "Iteration 6, loss = 0.21939657\n",
      "Iteration 7, loss = 0.21199811\n",
      "Iteration 8, loss = 0.20593060\n",
      "Iteration 9, loss = 0.20089581\n",
      "Iteration 10, loss = 0.19620825\n",
      "Iteration 11, loss = 0.19246782\n",
      "Iteration 12, loss = 0.18915638\n",
      "Iteration 13, loss = 0.18628965\n",
      "Iteration 14, loss = 0.18300687\n",
      "Iteration 15, loss = 0.18040164\n",
      "Iteration 16, loss = 0.17784659\n",
      "Iteration 17, loss = 0.17574340\n",
      "Iteration 18, loss = 0.17358020\n",
      "Iteration 19, loss = 0.17141328\n",
      "Iteration 20, loss = 0.16985273\n",
      "Iteration 21, loss = 0.16792531\n",
      "Iteration 22, loss = 0.16630489\n",
      "Iteration 23, loss = 0.16469658\n",
      "Iteration 24, loss = 0.16342018\n",
      "Iteration 25, loss = 0.16187613\n",
      "Iteration 26, loss = 0.16055607\n",
      "Iteration 27, loss = 0.15941443\n",
      "Iteration 28, loss = 0.15841448\n",
      "Iteration 29, loss = 0.15800169\n",
      "Iteration 30, loss = 0.15617943\n",
      "Iteration 31, loss = 0.15520514\n",
      "Iteration 32, loss = 0.15386032\n",
      "Iteration 33, loss = 0.15299291\n",
      "Iteration 34, loss = 0.15194702\n",
      "Iteration 35, loss = 0.15077299\n",
      "Iteration 36, loss = 0.15047880\n",
      "Iteration 37, loss = 0.14978845\n",
      "Iteration 38, loss = 0.14867650\n",
      "Iteration 39, loss = 0.14784236\n",
      "Iteration 40, loss = 0.14713166\n",
      "Iteration 41, loss = 0.14658470\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.44674874\n",
      "Iteration 2, loss = 0.28179879\n",
      "Iteration 3, loss = 0.25232301\n",
      "Iteration 4, loss = 0.23617338\n",
      "Iteration 5, loss = 0.22472793\n",
      "Iteration 6, loss = 0.21630247\n",
      "Iteration 7, loss = 0.20955635\n",
      "Iteration 8, loss = 0.20416469\n",
      "Iteration 9, loss = 0.19943956\n",
      "Iteration 10, loss = 0.19544700\n",
      "Iteration 11, loss = 0.19180468\n",
      "Iteration 12, loss = 0.18857065\n",
      "Iteration 13, loss = 0.18575960\n",
      "Iteration 14, loss = 0.18301452\n",
      "Iteration 15, loss = 0.18068271\n",
      "Iteration 16, loss = 0.17852027\n",
      "Iteration 17, loss = 0.17598949\n",
      "Iteration 18, loss = 0.17433570\n",
      "Iteration 19, loss = 0.17243236\n",
      "Iteration 20, loss = 0.17079277\n",
      "Iteration 21, loss = 0.16903337\n",
      "Iteration 22, loss = 0.16729960\n",
      "Iteration 23, loss = 0.16609118\n",
      "Iteration 24, loss = 0.16457059\n",
      "Iteration 25, loss = 0.16387678\n",
      "Iteration 26, loss = 0.16223409\n",
      "Iteration 27, loss = 0.16078365\n",
      "Iteration 28, loss = 0.15958083\n",
      "Iteration 29, loss = 0.15851367\n",
      "Iteration 30, loss = 0.15768583\n",
      "Iteration 31, loss = 0.15644178\n",
      "Iteration 32, loss = 0.15548503\n",
      "Iteration 33, loss = 0.15482517\n",
      "Iteration 34, loss = 0.15383053\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45617430\n",
      "Iteration 2, loss = 0.28318502\n",
      "Iteration 3, loss = 0.25500521\n",
      "Iteration 4, loss = 0.23841136\n",
      "Iteration 5, loss = 0.22716329\n",
      "Iteration 6, loss = 0.21846599\n",
      "Iteration 7, loss = 0.21150950\n",
      "Iteration 8, loss = 0.20590937\n",
      "Iteration 9, loss = 0.20075638\n",
      "Iteration 10, loss = 0.19653899\n",
      "Iteration 11, loss = 0.19269097\n",
      "Iteration 12, loss = 0.18910740\n",
      "Iteration 13, loss = 0.18601014\n",
      "Iteration 14, loss = 0.18324893\n",
      "Iteration 15, loss = 0.18070971\n",
      "Iteration 16, loss = 0.17814408\n",
      "Iteration 17, loss = 0.17598656\n",
      "Iteration 18, loss = 0.17424534\n",
      "Iteration 19, loss = 0.17190591\n",
      "Iteration 20, loss = 0.17024158\n",
      "Iteration 21, loss = 0.16841785\n",
      "Iteration 22, loss = 0.16695063\n",
      "Iteration 23, loss = 0.16546276\n",
      "Iteration 24, loss = 0.16387399\n",
      "Iteration 25, loss = 0.16303895\n",
      "Iteration 26, loss = 0.16137170\n",
      "Iteration 27, loss = 0.16035139\n",
      "Iteration 28, loss = 0.15920901\n",
      "Iteration 29, loss = 0.15839539\n",
      "Iteration 30, loss = 0.15692665\n",
      "Iteration 31, loss = 0.15605321\n",
      "Iteration 32, loss = 0.15514750\n",
      "Iteration 33, loss = 0.15410744\n",
      "Iteration 34, loss = 0.15337570\n",
      "Iteration 35, loss = 0.15236540\n",
      "Iteration 36, loss = 0.15148011\n",
      "Iteration 37, loss = 0.15076803\n",
      "Iteration 38, loss = 0.14989588\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43345106\n",
      "Iteration 2, loss = 0.28141002\n",
      "Iteration 3, loss = 0.25286842\n",
      "Iteration 4, loss = 0.23621697\n",
      "Iteration 5, loss = 0.22473636\n",
      "Iteration 6, loss = 0.21586385\n",
      "Iteration 7, loss = 0.20887335\n",
      "Iteration 8, loss = 0.20298440\n",
      "Iteration 9, loss = 0.19808865\n",
      "Iteration 10, loss = 0.19389512\n",
      "Iteration 11, loss = 0.18992301\n",
      "Iteration 12, loss = 0.18681606\n",
      "Iteration 13, loss = 0.18382474\n",
      "Iteration 14, loss = 0.18108221\n",
      "Iteration 15, loss = 0.17847224\n",
      "Iteration 16, loss = 0.17613691\n",
      "Iteration 17, loss = 0.17410311\n",
      "Iteration 18, loss = 0.17211249\n",
      "Iteration 19, loss = 0.17030090\n",
      "Iteration 20, loss = 0.16860135\n",
      "Iteration 21, loss = 0.16703671\n",
      "Iteration 22, loss = 0.16554033\n",
      "Iteration 23, loss = 0.16378346\n",
      "Iteration 24, loss = 0.16277940\n",
      "Iteration 25, loss = 0.16144152\n",
      "Iteration 26, loss = 0.16001521\n",
      "Iteration 27, loss = 0.15892434\n",
      "Iteration 28, loss = 0.15806926\n",
      "Iteration 29, loss = 0.15682880\n",
      "Iteration 30, loss = 0.15583315\n",
      "Iteration 31, loss = 0.15488054\n",
      "Iteration 32, loss = 0.15392297\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30040674\n",
      "Iteration 2, loss = 0.20901307\n",
      "Iteration 3, loss = 0.18934755\n",
      "Iteration 4, loss = 0.18002416\n",
      "Iteration 5, loss = 0.16995780\n",
      "Iteration 6, loss = 0.16281225\n",
      "Iteration 7, loss = 0.15755083\n",
      "Iteration 8, loss = 0.15341656\n",
      "Iteration 9, loss = 0.14991461\n",
      "Iteration 10, loss = 0.14589487\n",
      "Iteration 11, loss = 0.14329129\n",
      "Iteration 12, loss = 0.14117730\n",
      "Iteration 13, loss = 0.13994455\n",
      "Iteration 14, loss = 0.13916951\n",
      "Iteration 15, loss = 0.13517430\n",
      "Iteration 16, loss = 0.13375846\n",
      "Iteration 17, loss = 0.13275788\n",
      "Iteration 18, loss = 0.13049951\n",
      "Iteration 19, loss = 0.12928249\n",
      "Iteration 20, loss = 0.12744530\n",
      "Iteration 21, loss = 0.12730430\n",
      "Iteration 22, loss = 0.12542537\n",
      "Iteration 23, loss = 0.12562617\n",
      "Iteration 24, loss = 0.12324754\n",
      "Iteration 25, loss = 0.12230780\n",
      "Iteration 26, loss = 0.12125321\n",
      "Iteration 27, loss = 0.12080094\n",
      "Iteration 28, loss = 0.12261795\n",
      "Iteration 29, loss = 0.11914018\n",
      "Iteration 30, loss = 0.11833195\n",
      "Iteration 31, loss = 0.11645416\n",
      "Iteration 32, loss = 0.11621940\n",
      "Iteration 33, loss = 0.11622400\n",
      "Iteration 34, loss = 0.11472975\n",
      "Iteration 35, loss = 0.11395232\n",
      "Iteration 36, loss = 0.11338082\n",
      "Iteration 37, loss = 0.11236487\n",
      "Iteration 38, loss = 0.11155409\n",
      "Iteration 39, loss = 0.11173815\n",
      "Iteration 40, loss = 0.11069447\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32539144\n",
      "Iteration 2, loss = 0.21262800\n",
      "Iteration 3, loss = 0.18947282\n",
      "Iteration 4, loss = 0.17701270\n",
      "Iteration 5, loss = 0.16775948\n",
      "Iteration 6, loss = 0.16091790\n",
      "Iteration 7, loss = 0.15592011\n",
      "Iteration 8, loss = 0.15198376\n",
      "Iteration 9, loss = 0.14979425\n",
      "Iteration 10, loss = 0.14498982\n",
      "Iteration 11, loss = 0.14255844\n",
      "Iteration 12, loss = 0.14010757\n",
      "Iteration 13, loss = 0.13710439\n",
      "Iteration 14, loss = 0.13546371\n",
      "Iteration 15, loss = 0.13406111\n",
      "Iteration 16, loss = 0.13557310\n",
      "Iteration 17, loss = 0.13205836\n",
      "Iteration 18, loss = 0.12919279\n",
      "Iteration 19, loss = 0.12749245\n",
      "Iteration 20, loss = 0.12572752\n",
      "Iteration 21, loss = 0.12413813\n",
      "Iteration 22, loss = 0.12357324\n",
      "Iteration 23, loss = 0.12240364\n",
      "Iteration 24, loss = 0.12104388\n",
      "Iteration 25, loss = 0.11974312\n",
      "Iteration 26, loss = 0.11917038\n",
      "Iteration 27, loss = 0.11855968\n",
      "Iteration 28, loss = 0.11748630\n",
      "Iteration 29, loss = 0.11599046\n",
      "Iteration 30, loss = 0.11528802\n",
      "Iteration 31, loss = 0.11388693\n",
      "Iteration 32, loss = 0.11346515\n",
      "Iteration 33, loss = 0.11340614\n",
      "Iteration 34, loss = 0.11179069\n",
      "Iteration 35, loss = 0.11190280\n",
      "Iteration 36, loss = 0.10983753\n",
      "Iteration 37, loss = 0.10976946\n",
      "Iteration 38, loss = 0.10842484\n",
      "Iteration 39, loss = 0.10770143\n",
      "Iteration 40, loss = 0.10713099\n",
      "Iteration 41, loss = 0.10688435\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30809382\n",
      "Iteration 2, loss = 0.21129687\n",
      "Iteration 3, loss = 0.19131819\n",
      "Iteration 4, loss = 0.17961575\n",
      "Iteration 5, loss = 0.17169515\n",
      "Iteration 6, loss = 0.16360312\n",
      "Iteration 7, loss = 0.15928757\n",
      "Iteration 8, loss = 0.15457389\n",
      "Iteration 9, loss = 0.15177861\n",
      "Iteration 10, loss = 0.14789752\n",
      "Iteration 11, loss = 0.14496780\n",
      "Iteration 12, loss = 0.14273565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.13990333\n",
      "Iteration 14, loss = 0.13788506\n",
      "Iteration 15, loss = 0.13555525\n",
      "Iteration 16, loss = 0.13360486\n",
      "Iteration 17, loss = 0.13242785\n",
      "Iteration 18, loss = 0.13040718\n",
      "Iteration 19, loss = 0.12913389\n",
      "Iteration 20, loss = 0.12687191\n",
      "Iteration 21, loss = 0.12616126\n",
      "Iteration 22, loss = 0.12481865\n",
      "Iteration 23, loss = 0.12331812\n",
      "Iteration 24, loss = 0.12209317\n",
      "Iteration 25, loss = 0.12201255\n",
      "Iteration 26, loss = 0.12096635\n",
      "Iteration 27, loss = 0.11940747\n",
      "Iteration 28, loss = 0.11839792\n",
      "Iteration 29, loss = 0.11806325\n",
      "Iteration 30, loss = 0.11693449\n",
      "Iteration 31, loss = 0.11645723\n",
      "Iteration 32, loss = 0.11501664\n",
      "Iteration 33, loss = 0.11585335\n",
      "Iteration 34, loss = 0.11383590\n",
      "Iteration 35, loss = 0.11306254\n",
      "Iteration 36, loss = 0.11381413\n",
      "Iteration 37, loss = 0.11122154\n",
      "Iteration 38, loss = 0.11115869\n",
      "Iteration 39, loss = 0.11004712\n",
      "Iteration 40, loss = 0.11045368\n",
      "Iteration 41, loss = 0.10893226\n",
      "Iteration 42, loss = 0.10837371\n",
      "Iteration 43, loss = 0.10797479\n",
      "Iteration 44, loss = 0.10728807\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30634354\n",
      "Iteration 2, loss = 0.20870527\n",
      "Iteration 3, loss = 0.18813747\n",
      "Iteration 4, loss = 0.17572884\n",
      "Iteration 5, loss = 0.16706195\n",
      "Iteration 6, loss = 0.16085216\n",
      "Iteration 7, loss = 0.15589731\n",
      "Iteration 8, loss = 0.15154005\n",
      "Iteration 9, loss = 0.14899181\n",
      "Iteration 10, loss = 0.14548686\n",
      "Iteration 11, loss = 0.14327625\n",
      "Iteration 12, loss = 0.14119888\n",
      "Iteration 13, loss = 0.13870409\n",
      "Iteration 14, loss = 0.13684483\n",
      "Iteration 15, loss = 0.13534656\n",
      "Iteration 16, loss = 0.13343484\n",
      "Iteration 17, loss = 0.13195124\n",
      "Iteration 18, loss = 0.13067292\n",
      "Iteration 19, loss = 0.12992582\n",
      "Iteration 20, loss = 0.12793990\n",
      "Iteration 21, loss = 0.12666179\n",
      "Iteration 22, loss = 0.12560614\n",
      "Iteration 23, loss = 0.12446805\n",
      "Iteration 24, loss = 0.12420919\n",
      "Iteration 25, loss = 0.12299786\n",
      "Iteration 26, loss = 0.12180413\n",
      "Iteration 27, loss = 0.12048716\n",
      "Iteration 28, loss = 0.12002637\n",
      "Iteration 29, loss = 0.11850672\n",
      "Iteration 30, loss = 0.11801142\n",
      "Iteration 31, loss = 0.11665529\n",
      "Iteration 32, loss = 0.11634681\n",
      "Iteration 33, loss = 0.11759823\n",
      "Iteration 34, loss = 0.11499542\n",
      "Iteration 35, loss = 0.11450834\n",
      "Iteration 36, loss = 0.11353959\n",
      "Iteration 37, loss = 0.11231221\n",
      "Iteration 38, loss = 0.11234010\n",
      "Iteration 39, loss = 0.11702106\n",
      "Iteration 40, loss = 0.11056224\n",
      "Iteration 41, loss = 0.11045654\n",
      "Iteration 42, loss = 0.10897176\n",
      "Iteration 43, loss = 0.10883922\n",
      "Iteration 44, loss = 0.10790456\n",
      "Iteration 45, loss = 0.10741293\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30344466\n",
      "Iteration 2, loss = 0.20932905\n",
      "Iteration 3, loss = 0.18868751\n",
      "Iteration 4, loss = 0.17657886\n",
      "Iteration 5, loss = 0.16793483\n",
      "Iteration 6, loss = 0.16091784\n",
      "Iteration 7, loss = 0.15688000\n",
      "Iteration 8, loss = 0.15133131\n",
      "Iteration 9, loss = 0.14849492\n",
      "Iteration 10, loss = 0.14480430\n",
      "Iteration 11, loss = 0.14239961\n",
      "Iteration 12, loss = 0.14038687\n",
      "Iteration 13, loss = 0.13813334\n",
      "Iteration 14, loss = 0.13582178\n",
      "Iteration 15, loss = 0.13451692\n",
      "Iteration 16, loss = 0.13253940\n",
      "Iteration 17, loss = 0.13083816\n",
      "Iteration 18, loss = 0.12985032\n",
      "Iteration 19, loss = 0.12764389\n",
      "Iteration 20, loss = 0.12690646\n",
      "Iteration 21, loss = 0.12598809\n",
      "Iteration 22, loss = 0.12444238\n",
      "Iteration 23, loss = 0.12334296\n",
      "Iteration 24, loss = 0.12242950\n",
      "Iteration 25, loss = 0.12082782\n",
      "Iteration 26, loss = 0.12121353\n",
      "Iteration 27, loss = 0.11955502\n",
      "Iteration 28, loss = 0.11869933\n",
      "Iteration 29, loss = 0.11950019\n",
      "Iteration 30, loss = 0.11750972\n",
      "Iteration 31, loss = 0.11543038\n",
      "Iteration 32, loss = 0.11579538\n",
      "Iteration 33, loss = 0.11542030\n",
      "Iteration 34, loss = 0.11337248\n",
      "Iteration 35, loss = 0.11345134\n",
      "Iteration 36, loss = 0.11257220\n",
      "Iteration 37, loss = 0.11124384\n",
      "Iteration 38, loss = 0.11041644\n",
      "Iteration 39, loss = 0.10930522\n",
      "Iteration 40, loss = 0.10978552\n",
      "Iteration 41, loss = 0.10822247\n",
      "Iteration 42, loss = 0.10789656\n",
      "Iteration 43, loss = 0.10733096\n",
      "Iteration 44, loss = 0.10694392\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25544547\n",
      "Iteration 2, loss = 0.18294222\n",
      "Iteration 3, loss = 0.16604484\n",
      "Iteration 4, loss = 0.15603909\n",
      "Iteration 5, loss = 0.14890174\n",
      "Iteration 6, loss = 0.14323539\n",
      "Iteration 7, loss = 0.13934796\n",
      "Iteration 8, loss = 0.13379110\n",
      "Iteration 9, loss = 0.13077462\n",
      "Iteration 10, loss = 0.12873265\n",
      "Iteration 11, loss = 0.12652762\n",
      "Iteration 12, loss = 0.12440135\n",
      "Iteration 13, loss = 0.12295291\n",
      "Iteration 14, loss = 0.12217941\n",
      "Iteration 15, loss = 0.11950899\n",
      "Iteration 16, loss = 0.11817698\n",
      "Iteration 17, loss = 0.11623434\n",
      "Iteration 18, loss = 0.11509423\n",
      "Iteration 19, loss = 0.11412826\n",
      "Iteration 20, loss = 0.11264308\n",
      "Iteration 21, loss = 0.11206447\n",
      "Iteration 22, loss = 0.10985692\n",
      "Iteration 23, loss = 0.10945977\n",
      "Iteration 24, loss = 0.10942167\n",
      "Iteration 25, loss = 0.10719015\n",
      "Iteration 26, loss = 0.10971246\n",
      "Iteration 27, loss = 0.10422595\n",
      "Iteration 28, loss = 0.10417133\n",
      "Iteration 29, loss = 0.10234021\n",
      "Iteration 30, loss = 0.10217573\n",
      "Iteration 31, loss = 0.10208696\n",
      "Iteration 32, loss = 0.09847312\n",
      "Iteration 33, loss = 0.09809804\n",
      "Iteration 34, loss = 0.09917264\n",
      "Iteration 35, loss = 0.09729827\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26973635\n",
      "Iteration 2, loss = 0.18538467\n",
      "Iteration 3, loss = 0.16833615\n",
      "Iteration 4, loss = 0.15655209\n",
      "Iteration 5, loss = 0.15224675\n",
      "Iteration 6, loss = 0.14566018\n",
      "Iteration 7, loss = 0.14101341\n",
      "Iteration 8, loss = 0.13741856\n",
      "Iteration 9, loss = 0.13464753\n",
      "Iteration 10, loss = 0.13161646\n",
      "Iteration 11, loss = 0.12911546\n",
      "Iteration 12, loss = 0.12834791\n",
      "Iteration 13, loss = 0.12615921\n",
      "Iteration 14, loss = 0.12389248\n",
      "Iteration 15, loss = 0.12182889\n",
      "Iteration 16, loss = 0.12028740\n",
      "Iteration 17, loss = 0.12062370\n",
      "Iteration 18, loss = 0.11725145\n",
      "Iteration 19, loss = 0.11639748\n",
      "Iteration 20, loss = 0.11506466\n",
      "Iteration 21, loss = 0.11324597\n",
      "Iteration 22, loss = 0.11131372\n",
      "Iteration 23, loss = 0.11141888\n",
      "Iteration 24, loss = 0.10991082\n",
      "Iteration 25, loss = 0.10983802\n",
      "Iteration 26, loss = 0.10709589\n",
      "Iteration 27, loss = 0.10639110\n",
      "Iteration 28, loss = 0.10574321\n",
      "Iteration 29, loss = 0.10455828\n",
      "Iteration 30, loss = 0.10496415\n",
      "Iteration 31, loss = 0.10104982\n",
      "Iteration 32, loss = 0.10356996\n",
      "Iteration 33, loss = 0.10032026\n",
      "Iteration 34, loss = 0.09893957\n",
      "Iteration 35, loss = 0.09925105\n",
      "Iteration 36, loss = 0.09747470\n",
      "Iteration 37, loss = 0.09573262\n",
      "Iteration 38, loss = 0.09651038\n",
      "Iteration 39, loss = 0.09395078\n",
      "Iteration 40, loss = 0.09411308\n",
      "Iteration 41, loss = 0.09243271\n",
      "Iteration 42, loss = 0.09237243\n",
      "Iteration 43, loss = 0.09182087\n",
      "Iteration 44, loss = 0.08945236\n",
      "Iteration 45, loss = 0.08989564\n",
      "Iteration 46, loss = 0.08815473\n",
      "Iteration 47, loss = 0.08776041\n",
      "Iteration 48, loss = 0.08753659\n",
      "Iteration 49, loss = 0.08640055\n",
      "Iteration 50, loss = 0.09464381\n",
      "Iteration 51, loss = 0.08692473\n",
      "Iteration 52, loss = 0.08747866\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26745088\n",
      "Iteration 2, loss = 0.18736915\n",
      "Iteration 3, loss = 0.16890947\n",
      "Iteration 4, loss = 0.15791461\n",
      "Iteration 5, loss = 0.15177175\n",
      "Iteration 6, loss = 0.14574733\n",
      "Iteration 7, loss = 0.14194929\n",
      "Iteration 8, loss = 0.13908224\n",
      "Iteration 9, loss = 0.13658396\n",
      "Iteration 10, loss = 0.13332280\n",
      "Iteration 11, loss = 0.13037992\n",
      "Iteration 12, loss = 0.14894855\n",
      "Iteration 13, loss = 0.12727483\n",
      "Iteration 14, loss = 0.12612494\n",
      "Iteration 15, loss = 0.12391996\n",
      "Iteration 16, loss = 0.12100911\n",
      "Iteration 17, loss = 0.11972797\n",
      "Iteration 18, loss = 0.11735998\n",
      "Iteration 19, loss = 0.11661568\n",
      "Iteration 20, loss = 0.11519768\n",
      "Iteration 21, loss = 0.11359458\n",
      "Iteration 22, loss = 0.11253022\n",
      "Iteration 23, loss = 0.10988671\n",
      "Iteration 24, loss = 0.11082627\n",
      "Iteration 25, loss = 0.10807631\n",
      "Iteration 26, loss = 0.10684352\n",
      "Iteration 27, loss = 0.10671926\n",
      "Iteration 28, loss = 0.10538780\n",
      "Iteration 29, loss = 0.10604500\n",
      "Iteration 30, loss = 0.10372436\n",
      "Iteration 31, loss = 0.10227878\n",
      "Iteration 32, loss = 0.10162821\n",
      "Iteration 33, loss = 0.10019255\n",
      "Iteration 34, loss = 0.10108820\n",
      "Iteration 35, loss = 0.09836380\n",
      "Iteration 36, loss = 0.09748641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.09621500\n",
      "Iteration 38, loss = 0.09556181\n",
      "Iteration 39, loss = 0.09544397\n",
      "Iteration 40, loss = 0.09418080\n",
      "Iteration 41, loss = 0.09210575\n",
      "Iteration 42, loss = 0.09127613\n",
      "Iteration 43, loss = 0.09026492\n",
      "Iteration 44, loss = 0.09046900\n",
      "Iteration 45, loss = 0.08880646\n",
      "Iteration 46, loss = 0.09012749\n",
      "Iteration 47, loss = 0.08934286\n",
      "Iteration 48, loss = 0.08813433\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26279169\n",
      "Iteration 2, loss = 0.18813434\n",
      "Iteration 3, loss = 0.17069284\n",
      "Iteration 4, loss = 0.16076561\n",
      "Iteration 5, loss = 0.15481746\n",
      "Iteration 6, loss = 0.14757907\n",
      "Iteration 7, loss = 0.14376336\n",
      "Iteration 8, loss = 0.14084672\n",
      "Iteration 9, loss = 0.13776847\n",
      "Iteration 10, loss = 0.13453567\n",
      "Iteration 11, loss = 0.13239333\n",
      "Iteration 12, loss = 0.13000401\n",
      "Iteration 13, loss = 0.12710681\n",
      "Iteration 14, loss = 0.12702474\n",
      "Iteration 15, loss = 0.12476408\n",
      "Iteration 16, loss = 0.12348508\n",
      "Iteration 17, loss = 0.12031175\n",
      "Iteration 18, loss = 0.11906094\n",
      "Iteration 19, loss = 0.11729773\n",
      "Iteration 20, loss = 0.11696393\n",
      "Iteration 21, loss = 0.11659954\n",
      "Iteration 22, loss = 0.11410968\n",
      "Iteration 23, loss = 0.11391202\n",
      "Iteration 24, loss = 0.11346579\n",
      "Iteration 25, loss = 0.11143970\n",
      "Iteration 26, loss = 0.10839367\n",
      "Iteration 27, loss = 0.10895500\n",
      "Iteration 28, loss = 0.10802061\n",
      "Iteration 29, loss = 0.10567872\n",
      "Iteration 30, loss = 0.10476612\n",
      "Iteration 31, loss = 0.10631357\n",
      "Iteration 32, loss = 0.10491434\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.26188061\n",
      "Iteration 2, loss = 0.18707485\n",
      "Iteration 3, loss = 0.17048530\n",
      "Iteration 4, loss = 0.15916504\n",
      "Iteration 5, loss = 0.15179674\n",
      "Iteration 6, loss = 0.14590646\n",
      "Iteration 7, loss = 0.14124118\n",
      "Iteration 8, loss = 0.13749826\n",
      "Iteration 9, loss = 0.13562983\n",
      "Iteration 10, loss = 0.13203971\n",
      "Iteration 11, loss = 0.12944654\n",
      "Iteration 12, loss = 0.12802134\n",
      "Iteration 13, loss = 0.12711205\n",
      "Iteration 14, loss = 0.12323008\n",
      "Iteration 15, loss = 0.12238022\n",
      "Iteration 16, loss = 0.12089299\n",
      "Iteration 17, loss = 0.11877407\n",
      "Iteration 18, loss = 0.11799637\n",
      "Iteration 19, loss = 0.11595176\n",
      "Iteration 20, loss = 0.11570500\n",
      "Iteration 21, loss = 0.11452379\n",
      "Iteration 22, loss = 0.11211296\n",
      "Iteration 23, loss = 0.11124684\n",
      "Iteration 24, loss = 0.11204607\n",
      "Iteration 25, loss = 0.10883145\n",
      "Iteration 26, loss = 0.10746868\n",
      "Iteration 27, loss = 0.10567443\n",
      "Iteration 28, loss = 0.10631220\n",
      "Iteration 29, loss = 0.10520598\n",
      "Iteration 30, loss = 0.10439706\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56264523\n",
      "Iteration 2, loss = 0.33402775\n",
      "Iteration 3, loss = 0.29651663\n",
      "Iteration 4, loss = 0.27560160\n",
      "Iteration 5, loss = 0.26157757\n",
      "Iteration 6, loss = 0.25098668\n",
      "Iteration 7, loss = 0.24247729\n",
      "Iteration 8, loss = 0.23547733\n",
      "Iteration 9, loss = 0.22987183\n",
      "Iteration 10, loss = 0.22415086\n",
      "Iteration 11, loss = 0.21971395\n",
      "Iteration 12, loss = 0.21556548\n",
      "Iteration 13, loss = 0.21169779\n",
      "Iteration 14, loss = 0.20866140\n",
      "Iteration 15, loss = 0.20546865\n",
      "Iteration 16, loss = 0.20280946\n",
      "Iteration 17, loss = 0.20031539\n",
      "Iteration 18, loss = 0.19846604\n",
      "Iteration 19, loss = 0.19601234\n",
      "Iteration 20, loss = 0.19385558\n",
      "Iteration 21, loss = 0.19200888\n",
      "Iteration 22, loss = 0.19009338\n",
      "Iteration 23, loss = 0.18840420\n",
      "Iteration 24, loss = 0.18729530\n",
      "Iteration 25, loss = 0.18551519\n",
      "Iteration 26, loss = 0.18366964\n",
      "Iteration 27, loss = 0.18229969\n",
      "Iteration 28, loss = 0.18097829\n",
      "Iteration 29, loss = 0.17970378\n",
      "Iteration 30, loss = 0.17843407\n",
      "Iteration 31, loss = 0.17739276\n",
      "Iteration 32, loss = 0.17628687\n",
      "Iteration 33, loss = 0.17530494\n",
      "Iteration 34, loss = 0.17418232\n",
      "Iteration 35, loss = 0.17301294\n",
      "Iteration 36, loss = 0.17245747\n",
      "Iteration 37, loss = 0.17123649\n",
      "Iteration 38, loss = 0.17046005\n",
      "Iteration 39, loss = 0.16966233\n",
      "Iteration 40, loss = 0.16878577\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52240825\n",
      "Iteration 2, loss = 0.32261513\n",
      "Iteration 3, loss = 0.28720428\n",
      "Iteration 4, loss = 0.26781474\n",
      "Iteration 5, loss = 0.25479126\n",
      "Iteration 6, loss = 0.24498889\n",
      "Iteration 7, loss = 0.23700307\n",
      "Iteration 8, loss = 0.23077963\n",
      "Iteration 9, loss = 0.22537652\n",
      "Iteration 10, loss = 0.22067429\n",
      "Iteration 11, loss = 0.21644419\n",
      "Iteration 12, loss = 0.21279544\n",
      "Iteration 13, loss = 0.20967473\n",
      "Iteration 14, loss = 0.20654859\n",
      "Iteration 15, loss = 0.20371100\n",
      "Iteration 16, loss = 0.20104684\n",
      "Iteration 17, loss = 0.19868422\n",
      "Iteration 18, loss = 0.19657400\n",
      "Iteration 19, loss = 0.19425485\n",
      "Iteration 20, loss = 0.19230235\n",
      "Iteration 21, loss = 0.19049604\n",
      "Iteration 22, loss = 0.18872367\n",
      "Iteration 23, loss = 0.18696137\n",
      "Iteration 24, loss = 0.18544312\n",
      "Iteration 25, loss = 0.18433832\n",
      "Iteration 26, loss = 0.18266208\n",
      "Iteration 27, loss = 0.18142482\n",
      "Iteration 28, loss = 0.17974372\n",
      "Iteration 29, loss = 0.17845049\n",
      "Iteration 30, loss = 0.17724608\n",
      "Iteration 31, loss = 0.17614532\n",
      "Iteration 32, loss = 0.17497558\n",
      "Iteration 33, loss = 0.17382517\n",
      "Iteration 34, loss = 0.17283919\n",
      "Iteration 35, loss = 0.17167573\n",
      "Iteration 36, loss = 0.17100445\n",
      "Iteration 37, loss = 0.16981059\n",
      "Iteration 38, loss = 0.16890428\n",
      "Iteration 39, loss = 0.16796052\n",
      "Iteration 40, loss = 0.16727088\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57666888\n",
      "Iteration 2, loss = 0.33579253\n",
      "Iteration 3, loss = 0.29659946\n",
      "Iteration 4, loss = 0.27605641\n",
      "Iteration 5, loss = 0.26229510\n",
      "Iteration 6, loss = 0.25185784\n",
      "Iteration 7, loss = 0.24314118\n",
      "Iteration 8, loss = 0.23642380\n",
      "Iteration 9, loss = 0.23019445\n",
      "Iteration 10, loss = 0.22498670\n",
      "Iteration 11, loss = 0.22039467\n",
      "Iteration 12, loss = 0.21627512\n",
      "Iteration 13, loss = 0.21255523\n",
      "Iteration 14, loss = 0.20939685\n",
      "Iteration 15, loss = 0.20636912\n",
      "Iteration 16, loss = 0.20375711\n",
      "Iteration 17, loss = 0.20118892\n",
      "Iteration 18, loss = 0.19864957\n",
      "Iteration 19, loss = 0.19641958\n",
      "Iteration 20, loss = 0.19432719\n",
      "Iteration 21, loss = 0.19245683\n",
      "Iteration 22, loss = 0.19072642\n",
      "Iteration 23, loss = 0.18901613\n",
      "Iteration 24, loss = 0.18745743\n",
      "Iteration 25, loss = 0.18572874\n",
      "Iteration 26, loss = 0.18450501\n",
      "Iteration 27, loss = 0.18297040\n",
      "Iteration 28, loss = 0.18167180\n",
      "Iteration 29, loss = 0.18038057\n",
      "Iteration 30, loss = 0.17910541\n",
      "Iteration 31, loss = 0.17793249\n",
      "Iteration 32, loss = 0.17674863\n",
      "Iteration 33, loss = 0.17574922\n",
      "Iteration 34, loss = 0.17456625\n",
      "Iteration 35, loss = 0.17358439\n",
      "Iteration 36, loss = 0.17249256\n",
      "Iteration 37, loss = 0.17160091\n",
      "Iteration 38, loss = 0.17071206\n",
      "Iteration 39, loss = 0.16996171\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55871731\n",
      "Iteration 2, loss = 0.33320889\n",
      "Iteration 3, loss = 0.29373244\n",
      "Iteration 4, loss = 0.27277298\n",
      "Iteration 5, loss = 0.25838882\n",
      "Iteration 6, loss = 0.24776781\n",
      "Iteration 7, loss = 0.23952128\n",
      "Iteration 8, loss = 0.23277605\n",
      "Iteration 9, loss = 0.22740943\n",
      "Iteration 10, loss = 0.22232239\n",
      "Iteration 11, loss = 0.21783646\n",
      "Iteration 12, loss = 0.21416609\n",
      "Iteration 13, loss = 0.21084585\n",
      "Iteration 14, loss = 0.20776671\n",
      "Iteration 15, loss = 0.20479642\n",
      "Iteration 16, loss = 0.20221061\n",
      "Iteration 17, loss = 0.19988010\n",
      "Iteration 18, loss = 0.19739770\n",
      "Iteration 19, loss = 0.19515213\n",
      "Iteration 20, loss = 0.19318304\n",
      "Iteration 21, loss = 0.19197938\n",
      "Iteration 22, loss = 0.18942957\n",
      "Iteration 23, loss = 0.18787161\n",
      "Iteration 24, loss = 0.18633126\n",
      "Iteration 25, loss = 0.18483157\n",
      "Iteration 26, loss = 0.18319266\n",
      "Iteration 27, loss = 0.18176171\n",
      "Iteration 28, loss = 0.18038968\n",
      "Iteration 29, loss = 0.17916074\n",
      "Iteration 30, loss = 0.17828576\n",
      "Iteration 31, loss = 0.17669511\n",
      "Iteration 32, loss = 0.17536760\n",
      "Iteration 33, loss = 0.17452038\n",
      "Iteration 34, loss = 0.17360460\n",
      "Iteration 35, loss = 0.17220183\n",
      "Iteration 36, loss = 0.17113394\n",
      "Iteration 37, loss = 0.17014808\n",
      "Iteration 38, loss = 0.16930703\n",
      "Iteration 39, loss = 0.16828455\n",
      "Iteration 40, loss = 0.16750564\n",
      "Iteration 41, loss = 0.16668988\n",
      "Iteration 42, loss = 0.16572185\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52409688\n",
      "Iteration 2, loss = 0.31901508\n",
      "Iteration 3, loss = 0.28474997\n",
      "Iteration 4, loss = 0.26620653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.25369446\n",
      "Iteration 6, loss = 0.24407781\n",
      "Iteration 7, loss = 0.23666311\n",
      "Iteration 8, loss = 0.23034311\n",
      "Iteration 9, loss = 0.22507941\n",
      "Iteration 10, loss = 0.22048350\n",
      "Iteration 11, loss = 0.21616409\n",
      "Iteration 12, loss = 0.21248756\n",
      "Iteration 13, loss = 0.20919057\n",
      "Iteration 14, loss = 0.20595391\n",
      "Iteration 15, loss = 0.20323841\n",
      "Iteration 16, loss = 0.20069837\n",
      "Iteration 17, loss = 0.19849473\n",
      "Iteration 18, loss = 0.19606469\n",
      "Iteration 19, loss = 0.19410981\n",
      "Iteration 20, loss = 0.19201230\n",
      "Iteration 21, loss = 0.19028332\n",
      "Iteration 22, loss = 0.18854630\n",
      "Iteration 23, loss = 0.18683430\n",
      "Iteration 24, loss = 0.18542133\n",
      "Iteration 25, loss = 0.18376532\n",
      "Iteration 26, loss = 0.18219516\n",
      "Iteration 27, loss = 0.18099897\n",
      "Iteration 28, loss = 0.17966754\n",
      "Iteration 29, loss = 0.17842001\n",
      "Iteration 30, loss = 0.17721974\n",
      "Iteration 31, loss = 0.17612748\n",
      "Iteration 32, loss = 0.17496986\n",
      "Iteration 33, loss = 0.17383261\n",
      "Iteration 34, loss = 0.17274371\n",
      "Iteration 35, loss = 0.17175678\n",
      "Iteration 36, loss = 0.17074269\n",
      "Iteration 37, loss = 0.16999479\n",
      "Iteration 38, loss = 0.16891675\n",
      "Iteration 39, loss = 0.16808144\n",
      "Iteration 40, loss = 0.16710982\n",
      "Iteration 41, loss = 0.16633728\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36375723\n",
      "Iteration 2, loss = 0.23225027\n",
      "Iteration 3, loss = 0.21000598\n",
      "Iteration 4, loss = 0.19578629\n",
      "Iteration 5, loss = 0.18656103\n",
      "Iteration 6, loss = 0.18128802\n",
      "Iteration 7, loss = 0.17353407\n",
      "Iteration 8, loss = 0.16859054\n",
      "Iteration 9, loss = 0.16473951\n",
      "Iteration 10, loss = 0.16090031\n",
      "Iteration 11, loss = 0.15753093\n",
      "Iteration 12, loss = 0.15475293\n",
      "Iteration 13, loss = 0.15866031\n",
      "Iteration 14, loss = 0.15009814\n",
      "Iteration 15, loss = 0.14784809\n",
      "Iteration 16, loss = 0.14636807\n",
      "Iteration 17, loss = 0.14417863\n",
      "Iteration 18, loss = 0.14263526\n",
      "Iteration 19, loss = 0.14131836\n",
      "Iteration 20, loss = 0.13933586\n",
      "Iteration 21, loss = 0.14257677\n",
      "Iteration 22, loss = 0.13739094\n",
      "Iteration 23, loss = 0.13719389\n",
      "Iteration 24, loss = 0.13578779\n",
      "Iteration 25, loss = 0.13387454\n",
      "Iteration 26, loss = 0.13316365\n",
      "Iteration 27, loss = 0.13403153\n",
      "Iteration 28, loss = 0.13681879\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36772632\n",
      "Iteration 2, loss = 0.23807279\n",
      "Iteration 3, loss = 0.21493583\n",
      "Iteration 4, loss = 0.20087215\n",
      "Iteration 5, loss = 0.19057232\n",
      "Iteration 6, loss = 0.18263286\n",
      "Iteration 7, loss = 0.17632737\n",
      "Iteration 8, loss = 0.17206073\n",
      "Iteration 9, loss = 0.16770565\n",
      "Iteration 10, loss = 0.16394565\n",
      "Iteration 11, loss = 0.16003116\n",
      "Iteration 12, loss = 0.15650725\n",
      "Iteration 13, loss = 0.15435618\n",
      "Iteration 14, loss = 0.15189565\n",
      "Iteration 15, loss = 0.14938760\n",
      "Iteration 16, loss = 0.14837018\n",
      "Iteration 17, loss = 0.14574474\n",
      "Iteration 18, loss = 0.14378627\n",
      "Iteration 19, loss = 0.14258204\n",
      "Iteration 20, loss = 0.14105806\n",
      "Iteration 21, loss = 0.14034264\n",
      "Iteration 22, loss = 0.13789625\n",
      "Iteration 23, loss = 0.13800746\n",
      "Iteration 24, loss = 0.13715363\n",
      "Iteration 25, loss = 0.13456173\n",
      "Iteration 26, loss = 0.13333917\n",
      "Iteration 27, loss = 0.13313675\n",
      "Iteration 28, loss = 0.13435702\n",
      "Iteration 29, loss = 0.13058174\n",
      "Iteration 30, loss = 0.13058638\n",
      "Iteration 31, loss = 0.12912430\n",
      "Iteration 32, loss = 0.12740969\n",
      "Iteration 33, loss = 0.12984337\n",
      "Iteration 34, loss = 0.12610337\n",
      "Iteration 35, loss = 0.12660326\n",
      "Iteration 36, loss = 0.12464001\n",
      "Iteration 37, loss = 0.12638373\n",
      "Iteration 38, loss = 0.12312225\n",
      "Iteration 39, loss = 0.12176053\n",
      "Iteration 40, loss = 0.12203780\n",
      "Iteration 41, loss = 0.12132924\n",
      "Iteration 42, loss = 0.12028865\n",
      "Iteration 43, loss = 0.12130366\n",
      "Iteration 44, loss = 0.12033636\n",
      "Iteration 45, loss = 0.11913720\n",
      "Iteration 46, loss = 0.11822010\n",
      "Iteration 47, loss = 0.11696204\n",
      "Iteration 48, loss = 0.11678578\n",
      "Iteration 49, loss = 0.11832991\n",
      "Iteration 50, loss = 0.11603892\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37050487\n",
      "Iteration 2, loss = 0.23447449\n",
      "Iteration 3, loss = 0.21254570\n",
      "Iteration 4, loss = 0.19912621\n",
      "Iteration 5, loss = 0.19037110\n",
      "Iteration 6, loss = 0.18301627\n",
      "Iteration 7, loss = 0.17767473\n",
      "Iteration 8, loss = 0.17338868\n",
      "Iteration 9, loss = 0.16823215\n",
      "Iteration 10, loss = 0.16435379\n",
      "Iteration 11, loss = 0.16099196\n",
      "Iteration 12, loss = 0.15815263\n",
      "Iteration 13, loss = 0.15618172\n",
      "Iteration 14, loss = 0.15434405\n",
      "Iteration 15, loss = 0.15148586\n",
      "Iteration 16, loss = 0.15014454\n",
      "Iteration 17, loss = 0.14708224\n",
      "Iteration 18, loss = 0.14644475\n",
      "Iteration 19, loss = 0.14442148\n",
      "Iteration 20, loss = 0.14269512\n",
      "Iteration 21, loss = 0.14120866\n",
      "Iteration 22, loss = 0.14080230\n",
      "Iteration 23, loss = 0.13905604\n",
      "Iteration 24, loss = 0.13764537\n",
      "Iteration 25, loss = 0.13707781\n",
      "Iteration 26, loss = 0.13565747\n",
      "Iteration 27, loss = 0.13531007\n",
      "Iteration 28, loss = 0.13584253\n",
      "Iteration 29, loss = 0.13289854\n",
      "Iteration 30, loss = 0.13423069\n",
      "Iteration 31, loss = 0.13105110\n",
      "Iteration 32, loss = 0.13120757\n",
      "Iteration 33, loss = 0.12980626\n",
      "Iteration 34, loss = 0.12881625\n",
      "Iteration 35, loss = 0.12769176\n",
      "Iteration 36, loss = 0.12741601\n",
      "Iteration 37, loss = 0.12628081\n",
      "Iteration 38, loss = 0.12538723\n",
      "Iteration 39, loss = 0.12489429\n",
      "Iteration 40, loss = 0.12605073\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35743796\n",
      "Iteration 2, loss = 0.22822338\n",
      "Iteration 3, loss = 0.20710399\n",
      "Iteration 4, loss = 0.19410131\n",
      "Iteration 5, loss = 0.18563990\n",
      "Iteration 6, loss = 0.17884542\n",
      "Iteration 7, loss = 0.17360340\n",
      "Iteration 8, loss = 0.16949261\n",
      "Iteration 9, loss = 0.16754598\n",
      "Iteration 10, loss = 0.16170826\n",
      "Iteration 11, loss = 0.15984391\n",
      "Iteration 12, loss = 0.15712318\n",
      "Iteration 13, loss = 0.15375127\n",
      "Iteration 14, loss = 0.15154070\n",
      "Iteration 15, loss = 0.14986237\n",
      "Iteration 16, loss = 0.14838041\n",
      "Iteration 17, loss = 0.14651980\n",
      "Iteration 18, loss = 0.14552312\n",
      "Iteration 19, loss = 0.14394281\n",
      "Iteration 20, loss = 0.14239082\n",
      "Iteration 21, loss = 0.14107638\n",
      "Iteration 22, loss = 0.13962472\n",
      "Iteration 23, loss = 0.13907178\n",
      "Iteration 24, loss = 0.13716173\n",
      "Iteration 25, loss = 0.13641744\n",
      "Iteration 26, loss = 0.13582385\n",
      "Iteration 27, loss = 0.13493453\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36600414\n",
      "Iteration 2, loss = 0.23748827\n",
      "Iteration 3, loss = 0.21528707\n",
      "Iteration 4, loss = 0.20124999\n",
      "Iteration 5, loss = 0.19154263\n",
      "Iteration 6, loss = 0.18425904\n",
      "Iteration 7, loss = 0.17823672\n",
      "Iteration 8, loss = 0.17412798\n",
      "Iteration 9, loss = 0.16938413\n",
      "Iteration 10, loss = 0.16534775\n",
      "Iteration 11, loss = 0.16177533\n",
      "Iteration 12, loss = 0.15854303\n",
      "Iteration 13, loss = 0.15614952\n",
      "Iteration 14, loss = 0.15464345\n",
      "Iteration 15, loss = 0.15109561\n",
      "Iteration 16, loss = 0.14922739\n",
      "Iteration 17, loss = 0.15085819\n",
      "Iteration 18, loss = 0.14603730\n",
      "Iteration 19, loss = 0.14475288\n",
      "Iteration 20, loss = 0.14275819\n",
      "Iteration 21, loss = 0.14261929\n",
      "Iteration 22, loss = 0.14009367\n",
      "Iteration 23, loss = 0.13865600\n",
      "Iteration 24, loss = 0.13790805\n",
      "Iteration 25, loss = 0.13672839\n",
      "Iteration 26, loss = 0.13527767\n",
      "Iteration 27, loss = 0.13577789\n",
      "Iteration 28, loss = 0.13301409\n",
      "Iteration 29, loss = 0.13268430\n",
      "Iteration 30, loss = 0.13170524\n",
      "Iteration 31, loss = 0.13108600\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31656019\n",
      "Iteration 2, loss = 0.21048203\n",
      "Iteration 3, loss = 0.19070397\n",
      "Iteration 4, loss = 0.17807500\n",
      "Iteration 5, loss = 0.16866053\n",
      "Iteration 6, loss = 0.16291980\n",
      "Iteration 7, loss = 0.15671195\n",
      "Iteration 8, loss = 0.15191819\n",
      "Iteration 9, loss = 0.14724243\n",
      "Iteration 10, loss = 0.14415503\n",
      "Iteration 11, loss = 0.14104780\n",
      "Iteration 12, loss = 0.13877568\n",
      "Iteration 13, loss = 0.13970442\n",
      "Iteration 14, loss = 0.14811333\n",
      "Iteration 15, loss = 0.13807727\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31933202\n",
      "Iteration 2, loss = 0.20840989\n",
      "Iteration 3, loss = 0.19106587\n",
      "Iteration 4, loss = 0.17709680\n",
      "Iteration 5, loss = 0.16724178\n",
      "Iteration 6, loss = 0.16136375\n",
      "Iteration 7, loss = 0.15540055\n",
      "Iteration 8, loss = 0.15212972\n",
      "Iteration 9, loss = 0.14944008\n",
      "Iteration 10, loss = 0.14467368\n",
      "Iteration 11, loss = 0.14363174\n",
      "Iteration 12, loss = 0.13974156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.13666872\n",
      "Iteration 14, loss = 0.13425398\n",
      "Iteration 15, loss = 0.13553363\n",
      "Iteration 16, loss = 0.13111073\n",
      "Iteration 17, loss = 0.12928680\n",
      "Iteration 18, loss = 0.12969208\n",
      "Iteration 19, loss = 0.12624933\n",
      "Iteration 20, loss = 0.12454474\n",
      "Iteration 21, loss = 0.12360616\n",
      "Iteration 22, loss = 0.12299662\n",
      "Iteration 23, loss = 0.12299189\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31123380\n",
      "Iteration 2, loss = 0.20903046\n",
      "Iteration 3, loss = 0.19066966\n",
      "Iteration 4, loss = 0.17657365\n",
      "Iteration 5, loss = 0.16960425\n",
      "Iteration 6, loss = 0.16255155\n",
      "Iteration 7, loss = 0.15712437\n",
      "Iteration 8, loss = 0.15283263\n",
      "Iteration 9, loss = 0.15009111\n",
      "Iteration 10, loss = 0.14657236\n",
      "Iteration 11, loss = 0.14423928\n",
      "Iteration 12, loss = 0.14264512\n",
      "Iteration 13, loss = 0.14128506\n",
      "Iteration 14, loss = 0.13716068\n",
      "Iteration 15, loss = 0.13531336\n",
      "Iteration 16, loss = 0.13408778\n",
      "Iteration 17, loss = 0.13318216\n",
      "Iteration 18, loss = 0.13082990\n",
      "Iteration 19, loss = 0.12985282\n",
      "Iteration 20, loss = 0.12878487\n",
      "Iteration 21, loss = 0.12638753\n",
      "Iteration 22, loss = 0.12946843\n",
      "Iteration 23, loss = 0.12943467\n",
      "Iteration 24, loss = 0.12301513\n",
      "Iteration 25, loss = 0.12197756\n",
      "Iteration 26, loss = 0.12389123\n",
      "Iteration 27, loss = 0.12189586\n",
      "Iteration 28, loss = 0.11903551\n",
      "Iteration 29, loss = 0.11744737\n",
      "Iteration 30, loss = 0.11851539\n",
      "Iteration 31, loss = 0.11639084\n",
      "Iteration 32, loss = 0.11961439\n",
      "Iteration 33, loss = 0.13203701\n",
      "Iteration 34, loss = 0.11472037\n",
      "Iteration 35, loss = 0.11376495\n",
      "Iteration 36, loss = 0.11300669\n",
      "Iteration 37, loss = 0.11115887\n",
      "Iteration 38, loss = 0.11176019\n",
      "Iteration 39, loss = 0.10930603\n",
      "Iteration 40, loss = 0.10911278\n",
      "Iteration 41, loss = 0.10765709\n",
      "Iteration 42, loss = 0.10656745\n",
      "Iteration 43, loss = 0.10544034\n",
      "Iteration 44, loss = 0.10502139\n",
      "Iteration 45, loss = 0.10390668\n",
      "Iteration 46, loss = 0.10388361\n",
      "Iteration 47, loss = 0.10368966\n",
      "Iteration 48, loss = 0.10206272\n",
      "Iteration 49, loss = 0.10331758\n",
      "Iteration 50, loss = 0.10179257\n",
      "Iteration 51, loss = 0.10085106\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31245784\n",
      "Iteration 2, loss = 0.20929413\n",
      "Iteration 3, loss = 0.18864213\n",
      "Iteration 4, loss = 0.17618219\n",
      "Iteration 5, loss = 0.16759320\n",
      "Iteration 6, loss = 0.17526070\n",
      "Iteration 7, loss = 0.15967038\n",
      "Iteration 8, loss = 0.15197151\n",
      "Iteration 9, loss = 0.14858386\n",
      "Iteration 10, loss = 0.14836381\n",
      "Iteration 11, loss = 0.14318045\n",
      "Iteration 12, loss = 0.14094550\n",
      "Iteration 13, loss = 0.13744517\n",
      "Iteration 14, loss = 0.14675060\n",
      "Iteration 15, loss = 0.13418949\n",
      "Iteration 16, loss = 0.13224455\n",
      "Iteration 17, loss = 0.13048660\n",
      "Iteration 18, loss = 0.13294240\n",
      "Iteration 19, loss = 0.12744885\n",
      "Iteration 20, loss = 0.12597265\n",
      "Iteration 21, loss = 0.12536689\n",
      "Iteration 22, loss = 0.12511982\n",
      "Iteration 23, loss = 0.12265488\n",
      "Iteration 24, loss = 0.12158561\n",
      "Iteration 25, loss = 0.12044149\n",
      "Iteration 26, loss = 0.11908799\n",
      "Iteration 27, loss = 0.11843490\n",
      "Iteration 28, loss = 0.11721004\n",
      "Iteration 29, loss = 0.12230529\n",
      "Iteration 30, loss = 0.11660727\n",
      "Iteration 31, loss = 0.11474380\n",
      "Iteration 32, loss = 0.11377551\n",
      "Iteration 33, loss = 0.11270753\n",
      "Iteration 34, loss = 0.11217966\n",
      "Iteration 35, loss = 0.11156254\n",
      "Iteration 36, loss = 0.11032747\n",
      "Iteration 37, loss = 0.10970520\n",
      "Iteration 38, loss = 0.10883132\n",
      "Iteration 39, loss = 0.10834563\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.31391984\n",
      "Iteration 2, loss = 0.20790504\n",
      "Iteration 3, loss = 0.18930730\n",
      "Iteration 4, loss = 0.17731498\n",
      "Iteration 5, loss = 0.16871290\n",
      "Iteration 6, loss = 0.16166447\n",
      "Iteration 7, loss = 0.15564533\n",
      "Iteration 8, loss = 0.15075603\n",
      "Iteration 9, loss = 0.14682026\n",
      "Iteration 10, loss = 0.14463447\n",
      "Iteration 11, loss = 0.14145855\n",
      "Iteration 12, loss = 0.13765798\n",
      "Iteration 13, loss = 0.13669824\n",
      "Iteration 14, loss = 0.13492806\n",
      "Iteration 15, loss = 0.13344121\n",
      "Iteration 16, loss = 0.13045566\n",
      "Iteration 17, loss = 0.12897819\n",
      "Iteration 18, loss = 0.12661243\n",
      "Iteration 19, loss = 0.12718672\n",
      "Iteration 20, loss = 0.12455767\n",
      "Iteration 21, loss = 0.12408315\n",
      "Iteration 22, loss = 0.12236489\n",
      "Iteration 23, loss = 0.12122894\n",
      "Iteration 24, loss = 0.12025573\n",
      "Iteration 25, loss = 0.12191674\n",
      "Iteration 26, loss = 0.11803531\n",
      "Iteration 27, loss = 0.11711115\n",
      "Iteration 28, loss = 0.11592937\n",
      "Iteration 29, loss = 0.11542448\n",
      "Iteration 30, loss = 0.11394996\n",
      "Iteration 31, loss = 0.11289772\n",
      "Iteration 32, loss = 0.11193345\n",
      "Iteration 33, loss = 0.11094013\n",
      "Iteration 34, loss = 0.11208180\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77437233\n",
      "Iteration 2, loss = 0.43548539\n",
      "Iteration 3, loss = 0.36989242\n",
      "Iteration 4, loss = 0.35198358\n",
      "Iteration 5, loss = 0.34499949\n",
      "Iteration 6, loss = 0.34087219\n",
      "Iteration 7, loss = 0.33879690\n",
      "Iteration 8, loss = 0.33680839\n",
      "Iteration 9, loss = 0.33584983\n",
      "Iteration 10, loss = 0.33421795\n",
      "Iteration 11, loss = 0.33341857\n",
      "Iteration 12, loss = 0.33266667\n",
      "Iteration 13, loss = 0.33233204\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78224528\n",
      "Iteration 2, loss = 0.43574865\n",
      "Iteration 3, loss = 0.36999141\n",
      "Iteration 4, loss = 0.35184682\n",
      "Iteration 5, loss = 0.34530156\n",
      "Iteration 6, loss = 0.34134990\n",
      "Iteration 7, loss = 0.33906362\n",
      "Iteration 8, loss = 0.33698206\n",
      "Iteration 9, loss = 0.33600897\n",
      "Iteration 10, loss = 0.33450091\n",
      "Iteration 11, loss = 0.33388468\n",
      "Iteration 12, loss = 0.33312653\n",
      "Iteration 13, loss = 0.33257368\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77486814\n",
      "Iteration 2, loss = 0.43624936\n",
      "Iteration 3, loss = 0.37057964\n",
      "Iteration 4, loss = 0.35322339\n",
      "Iteration 5, loss = 0.34651172\n",
      "Iteration 6, loss = 0.34310818\n",
      "Iteration 7, loss = 0.34042602\n",
      "Iteration 8, loss = 0.33890872\n",
      "Iteration 9, loss = 0.33737219\n",
      "Iteration 10, loss = 0.33652622\n",
      "Iteration 11, loss = 0.33559877\n",
      "Iteration 12, loss = 0.33481446\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77295429\n",
      "Iteration 2, loss = 0.43586565\n",
      "Iteration 3, loss = 0.37042781\n",
      "Iteration 4, loss = 0.35236580\n",
      "Iteration 5, loss = 0.34511241\n",
      "Iteration 6, loss = 0.34125230\n",
      "Iteration 7, loss = 0.33907348\n",
      "Iteration 8, loss = 0.33696127\n",
      "Iteration 9, loss = 0.33585655\n",
      "Iteration 10, loss = 0.33526684\n",
      "Iteration 11, loss = 0.33412334\n",
      "Iteration 12, loss = 0.33316681\n",
      "Iteration 13, loss = 0.33260351\n",
      "Iteration 14, loss = 0.33239936\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77041169\n",
      "Iteration 2, loss = 0.43447986\n",
      "Iteration 3, loss = 0.36853750\n",
      "Iteration 4, loss = 0.35130557\n",
      "Iteration 5, loss = 0.34455502\n",
      "Iteration 6, loss = 0.34073841\n",
      "Iteration 7, loss = 0.33811583\n",
      "Iteration 8, loss = 0.33663299\n",
      "Iteration 9, loss = 0.33472704\n",
      "Iteration 10, loss = 0.33427081\n",
      "Iteration 11, loss = 0.33331320\n",
      "Iteration 12, loss = 0.33257692\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46347872\n",
      "Iteration 2, loss = 0.34480240\n",
      "Iteration 3, loss = 0.34064319\n",
      "Iteration 4, loss = 0.33791199\n",
      "Iteration 5, loss = 0.33718358\n",
      "Iteration 6, loss = 0.33752793\n",
      "Iteration 7, loss = 0.33716244\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46794260\n",
      "Iteration 2, loss = 0.34497679\n",
      "Iteration 3, loss = 0.34188071\n",
      "Iteration 4, loss = 0.33838784\n",
      "Iteration 5, loss = 0.33869197\n",
      "Iteration 6, loss = 0.33803444\n",
      "Iteration 7, loss = 0.33677655\n",
      "Iteration 8, loss = 0.33729315\n",
      "Iteration 9, loss = 0.33688417\n",
      "Iteration 10, loss = 0.33714816\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46827267\n",
      "Iteration 2, loss = 0.34627405\n",
      "Iteration 3, loss = 0.34375099\n",
      "Iteration 4, loss = 0.34105590\n",
      "Iteration 5, loss = 0.34117384\n",
      "Iteration 6, loss = 0.33919359\n",
      "Iteration 7, loss = 0.33839745\n",
      "Iteration 8, loss = 0.33887041\n",
      "Iteration 9, loss = 0.33896138\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46433049\n",
      "Iteration 2, loss = 0.34577215\n",
      "Iteration 3, loss = 0.33972972\n",
      "Iteration 4, loss = 0.33920366\n",
      "Iteration 5, loss = 0.33832262\n",
      "Iteration 6, loss = 0.33786291\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46468582\n",
      "Iteration 2, loss = 0.34318800\n",
      "Iteration 3, loss = 0.33958341\n",
      "Iteration 4, loss = 0.33810716\n",
      "Iteration 5, loss = 0.33693199\n",
      "Iteration 6, loss = 0.33767976\n",
      "Iteration 7, loss = 0.33628165\n",
      "Iteration 8, loss = 0.33801928\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41658377\n",
      "Iteration 2, loss = 0.35010455\n",
      "Iteration 3, loss = 0.34887806\n",
      "Iteration 4, loss = 0.34596799\n",
      "Iteration 5, loss = 0.34543308\n",
      "Iteration 6, loss = 0.34601776\n",
      "Iteration 7, loss = 0.34610926\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41241856\n",
      "Iteration 2, loss = 0.34925184\n",
      "Iteration 3, loss = 0.34651582\n",
      "Iteration 4, loss = 0.34649950\n",
      "Iteration 5, loss = 0.34639381\n",
      "Iteration 6, loss = 0.34680326\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41881396\n",
      "Iteration 2, loss = 0.35163841\n",
      "Iteration 3, loss = 0.34934525\n",
      "Iteration 4, loss = 0.34691241\n",
      "Iteration 5, loss = 0.34824211\n",
      "Iteration 6, loss = 0.34733731\n",
      "Iteration 7, loss = 0.34784865\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41502700\n",
      "Iteration 2, loss = 0.34722350\n",
      "Iteration 3, loss = 0.34667510\n",
      "Iteration 4, loss = 0.34561787\n",
      "Iteration 5, loss = 0.34441082\n",
      "Iteration 6, loss = 0.34538962\n",
      "Iteration 7, loss = 0.34410354\n",
      "Iteration 8, loss = 0.34448872\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41607429\n",
      "Iteration 2, loss = 0.34839106\n",
      "Iteration 3, loss = 0.34758355\n",
      "Iteration 4, loss = 0.34640002\n",
      "Iteration 5, loss = 0.34526677\n",
      "Iteration 6, loss = 0.34477335\n",
      "Iteration 7, loss = 0.34532782\n",
      "Iteration 8, loss = 0.34468237\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52952396\n",
      "Iteration 2, loss = 0.41786317\n",
      "Iteration 3, loss = 0.38934129\n",
      "Iteration 4, loss = 0.36996894\n",
      "Iteration 5, loss = 0.35547883\n",
      "Iteration 6, loss = 0.34301003\n",
      "Iteration 7, loss = 0.33225138\n",
      "Iteration 8, loss = 0.32264797\n",
      "Iteration 9, loss = 0.31414099\n",
      "Iteration 10, loss = 0.30664562\n",
      "Iteration 11, loss = 0.29961903\n",
      "Iteration 12, loss = 0.29332758\n",
      "Iteration 13, loss = 0.28751911\n",
      "Iteration 14, loss = 0.28237260\n",
      "Iteration 15, loss = 0.27714590\n",
      "Iteration 16, loss = 0.27274406\n",
      "Iteration 17, loss = 0.26825281\n",
      "Iteration 18, loss = 0.26448836\n",
      "Iteration 19, loss = 0.26101070\n",
      "Iteration 20, loss = 0.25751068\n",
      "Iteration 21, loss = 0.25489901\n",
      "Iteration 22, loss = 0.25187215\n",
      "Iteration 23, loss = 0.24893458\n",
      "Iteration 24, loss = 0.24661582\n",
      "Iteration 25, loss = 0.24407536\n",
      "Iteration 26, loss = 0.24169045\n",
      "Iteration 27, loss = 0.24018648\n",
      "Iteration 28, loss = 0.23803476\n",
      "Iteration 29, loss = 0.23626350\n",
      "Iteration 30, loss = 0.23464237\n",
      "Iteration 31, loss = 0.23320986\n",
      "Iteration 32, loss = 0.23157577\n",
      "Iteration 33, loss = 0.23019223\n",
      "Iteration 34, loss = 0.22934443\n",
      "Iteration 35, loss = 0.22773344\n",
      "Iteration 36, loss = 0.22650639\n",
      "Iteration 37, loss = 0.22564806\n",
      "Iteration 38, loss = 0.22481591\n",
      "Iteration 39, loss = 0.22361468\n",
      "Iteration 40, loss = 0.22314822\n",
      "Iteration 41, loss = 0.22201042\n",
      "Iteration 42, loss = 0.22124918\n",
      "Iteration 43, loss = 0.22043492\n",
      "Iteration 44, loss = 0.21973263\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53631578\n",
      "Iteration 2, loss = 0.41539312\n",
      "Iteration 3, loss = 0.38854156\n",
      "Iteration 4, loss = 0.37018473\n",
      "Iteration 5, loss = 0.35601309\n",
      "Iteration 6, loss = 0.34381009\n",
      "Iteration 7, loss = 0.33361960\n",
      "Iteration 8, loss = 0.32454731\n",
      "Iteration 9, loss = 0.31609706\n",
      "Iteration 10, loss = 0.30840125\n",
      "Iteration 11, loss = 0.30180123\n",
      "Iteration 12, loss = 0.29528017\n",
      "Iteration 13, loss = 0.28946037\n",
      "Iteration 14, loss = 0.28435571\n",
      "Iteration 15, loss = 0.27933423\n",
      "Iteration 16, loss = 0.27499708\n",
      "Iteration 17, loss = 0.27067789\n",
      "Iteration 18, loss = 0.26697447\n",
      "Iteration 19, loss = 0.26322755\n",
      "Iteration 20, loss = 0.25987531\n",
      "Iteration 21, loss = 0.25661730\n",
      "Iteration 22, loss = 0.25370826\n",
      "Iteration 23, loss = 0.25100475\n",
      "Iteration 24, loss = 0.24825641\n",
      "Iteration 25, loss = 0.24592707\n",
      "Iteration 26, loss = 0.24408051\n",
      "Iteration 27, loss = 0.24152252\n",
      "Iteration 28, loss = 0.23978468\n",
      "Iteration 29, loss = 0.23799379\n",
      "Iteration 30, loss = 0.23623903\n",
      "Iteration 31, loss = 0.23478092\n",
      "Iteration 32, loss = 0.23323533\n",
      "Iteration 33, loss = 0.23181817\n",
      "Iteration 34, loss = 0.23050416\n",
      "Iteration 35, loss = 0.22948828\n",
      "Iteration 36, loss = 0.22813368\n",
      "Iteration 37, loss = 0.22725552\n",
      "Iteration 38, loss = 0.22625025\n",
      "Iteration 39, loss = 0.22505221\n",
      "Iteration 40, loss = 0.22425082\n",
      "Iteration 41, loss = 0.22328053\n",
      "Iteration 42, loss = 0.22256728\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56241314\n",
      "Iteration 2, loss = 0.42393162\n",
      "Iteration 3, loss = 0.39586735\n",
      "Iteration 4, loss = 0.37686987\n",
      "Iteration 5, loss = 0.36202054\n",
      "Iteration 6, loss = 0.34938948\n",
      "Iteration 7, loss = 0.33888449\n",
      "Iteration 8, loss = 0.32923869\n",
      "Iteration 9, loss = 0.32070904\n",
      "Iteration 10, loss = 0.31296740\n",
      "Iteration 11, loss = 0.30584472\n",
      "Iteration 12, loss = 0.29938002\n",
      "Iteration 13, loss = 0.29313852\n",
      "Iteration 14, loss = 0.28784548\n",
      "Iteration 15, loss = 0.28250392\n",
      "Iteration 16, loss = 0.27786785\n",
      "Iteration 17, loss = 0.27371394\n",
      "Iteration 18, loss = 0.26956507\n",
      "Iteration 19, loss = 0.26586933\n",
      "Iteration 20, loss = 0.26230013\n",
      "Iteration 21, loss = 0.25905834\n",
      "Iteration 22, loss = 0.25588481\n",
      "Iteration 23, loss = 0.25326518\n",
      "Iteration 24, loss = 0.25048236\n",
      "Iteration 25, loss = 0.24823466\n",
      "Iteration 26, loss = 0.24597483\n",
      "Iteration 27, loss = 0.24390441\n",
      "Iteration 28, loss = 0.24185094\n",
      "Iteration 29, loss = 0.24010862\n",
      "Iteration 30, loss = 0.23818226\n",
      "Iteration 31, loss = 0.23654692\n",
      "Iteration 32, loss = 0.23525223\n",
      "Iteration 33, loss = 0.23387387\n",
      "Iteration 34, loss = 0.23239339\n",
      "Iteration 35, loss = 0.23117003\n",
      "Iteration 36, loss = 0.23014124\n",
      "Iteration 37, loss = 0.22907026\n",
      "Iteration 38, loss = 0.22799105\n",
      "Iteration 39, loss = 0.22736216\n",
      "Iteration 40, loss = 0.22621197\n",
      "Iteration 41, loss = 0.22541738\n",
      "Iteration 42, loss = 0.22474796\n",
      "Iteration 43, loss = 0.22377909\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54246277\n",
      "Iteration 2, loss = 0.41669949\n",
      "Iteration 3, loss = 0.38803032\n",
      "Iteration 4, loss = 0.36942089\n",
      "Iteration 5, loss = 0.35506075\n",
      "Iteration 6, loss = 0.34289744\n",
      "Iteration 7, loss = 0.33278799\n",
      "Iteration 8, loss = 0.32353851\n",
      "Iteration 9, loss = 0.31516103\n",
      "Iteration 10, loss = 0.30751989\n",
      "Iteration 11, loss = 0.30096506\n",
      "Iteration 12, loss = 0.29491763\n",
      "Iteration 13, loss = 0.28908225\n",
      "Iteration 14, loss = 0.28389930\n",
      "Iteration 15, loss = 0.27907233\n",
      "Iteration 16, loss = 0.27463764\n",
      "Iteration 17, loss = 0.27044189\n",
      "Iteration 18, loss = 0.26650016\n",
      "Iteration 19, loss = 0.26282748\n",
      "Iteration 20, loss = 0.25965562\n",
      "Iteration 21, loss = 0.25655222\n",
      "Iteration 22, loss = 0.25380438\n",
      "Iteration 23, loss = 0.25092939\n",
      "Iteration 24, loss = 0.24850362\n",
      "Iteration 25, loss = 0.24604599\n",
      "Iteration 26, loss = 0.24414015\n",
      "Iteration 27, loss = 0.24208064\n",
      "Iteration 28, loss = 0.24025348\n",
      "Iteration 29, loss = 0.23828145\n",
      "Iteration 30, loss = 0.23680799\n",
      "Iteration 31, loss = 0.23519749\n",
      "Iteration 32, loss = 0.23384135\n",
      "Iteration 33, loss = 0.23236637\n",
      "Iteration 34, loss = 0.23076286\n",
      "Iteration 35, loss = 0.23004967\n",
      "Iteration 36, loss = 0.22891905\n",
      "Iteration 37, loss = 0.22757195\n",
      "Iteration 38, loss = 0.22664319\n",
      "Iteration 39, loss = 0.22551564\n",
      "Iteration 40, loss = 0.22479696\n",
      "Iteration 41, loss = 0.22400944\n",
      "Iteration 42, loss = 0.22333353\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53571807\n",
      "Iteration 2, loss = 0.41934071\n",
      "Iteration 3, loss = 0.39051607\n",
      "Iteration 4, loss = 0.37154851\n",
      "Iteration 5, loss = 0.35724295\n",
      "Iteration 6, loss = 0.34504595\n",
      "Iteration 7, loss = 0.33493277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.32556959\n",
      "Iteration 9, loss = 0.31689472\n",
      "Iteration 10, loss = 0.30930223\n",
      "Iteration 11, loss = 0.30250394\n",
      "Iteration 12, loss = 0.29623518\n",
      "Iteration 13, loss = 0.29043791\n",
      "Iteration 14, loss = 0.28502914\n",
      "Iteration 15, loss = 0.27996500\n",
      "Iteration 16, loss = 0.27575850\n",
      "Iteration 17, loss = 0.27138628\n",
      "Iteration 18, loss = 0.26742514\n",
      "Iteration 19, loss = 0.26385286\n",
      "Iteration 20, loss = 0.26027673\n",
      "Iteration 21, loss = 0.25698677\n",
      "Iteration 22, loss = 0.25410209\n",
      "Iteration 23, loss = 0.25153557\n",
      "Iteration 24, loss = 0.24884302\n",
      "Iteration 25, loss = 0.24643649\n",
      "Iteration 26, loss = 0.24428658\n",
      "Iteration 27, loss = 0.24201926\n",
      "Iteration 28, loss = 0.24042507\n",
      "Iteration 29, loss = 0.23840550\n",
      "Iteration 30, loss = 0.23667397\n",
      "Iteration 31, loss = 0.23503519\n",
      "Iteration 32, loss = 0.23370217\n",
      "Iteration 33, loss = 0.23211336\n",
      "Iteration 34, loss = 0.23057368\n",
      "Iteration 35, loss = 0.22956492\n",
      "Iteration 36, loss = 0.22835868\n",
      "Iteration 37, loss = 0.22729095\n",
      "Iteration 38, loss = 0.22629153\n",
      "Iteration 39, loss = 0.22542880\n",
      "Iteration 40, loss = 0.22453263\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42666587\n",
      "Iteration 2, loss = 0.33039815\n",
      "Iteration 3, loss = 0.29457692\n",
      "Iteration 4, loss = 0.26975796\n",
      "Iteration 5, loss = 0.25363624\n",
      "Iteration 6, loss = 0.24097595\n",
      "Iteration 7, loss = 0.23458464\n",
      "Iteration 8, loss = 0.22831717\n",
      "Iteration 9, loss = 0.22411406\n",
      "Iteration 10, loss = 0.22078671\n",
      "Iteration 11, loss = 0.21914997\n",
      "Iteration 12, loss = 0.21654231\n",
      "Iteration 13, loss = 0.21497210\n",
      "Iteration 14, loss = 0.21396941\n",
      "Iteration 15, loss = 0.21241459\n",
      "Iteration 16, loss = 0.21155763\n",
      "Iteration 17, loss = 0.21147112\n",
      "Iteration 18, loss = 0.21069741\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42578364\n",
      "Iteration 2, loss = 0.33090547\n",
      "Iteration 3, loss = 0.29519277\n",
      "Iteration 4, loss = 0.27015340\n",
      "Iteration 5, loss = 0.25415811\n",
      "Iteration 6, loss = 0.24340653\n",
      "Iteration 7, loss = 0.23497182\n",
      "Iteration 8, loss = 0.22897686\n",
      "Iteration 9, loss = 0.22458741\n",
      "Iteration 10, loss = 0.22184516\n",
      "Iteration 11, loss = 0.21911807\n",
      "Iteration 12, loss = 0.21720046\n",
      "Iteration 13, loss = 0.21642719\n",
      "Iteration 14, loss = 0.21437159\n",
      "Iteration 15, loss = 0.21422227\n",
      "Iteration 16, loss = 0.21231121\n",
      "Iteration 17, loss = 0.21269454\n",
      "Iteration 18, loss = 0.21152552\n",
      "Iteration 19, loss = 0.21101263\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42784447\n",
      "Iteration 2, loss = 0.33196636\n",
      "Iteration 3, loss = 0.29592683\n",
      "Iteration 4, loss = 0.27261417\n",
      "Iteration 5, loss = 0.25666384\n",
      "Iteration 6, loss = 0.24596456\n",
      "Iteration 7, loss = 0.23754528\n",
      "Iteration 8, loss = 0.23131756\n",
      "Iteration 9, loss = 0.22630938\n",
      "Iteration 10, loss = 0.22389543\n",
      "Iteration 11, loss = 0.22134126\n",
      "Iteration 12, loss = 0.22008245\n",
      "Iteration 13, loss = 0.21839525\n",
      "Iteration 14, loss = 0.21680483\n",
      "Iteration 15, loss = 0.21581430\n",
      "Iteration 16, loss = 0.21482453\n",
      "Iteration 17, loss = 0.21399239\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42147953\n",
      "Iteration 2, loss = 0.33146705\n",
      "Iteration 3, loss = 0.29464619\n",
      "Iteration 4, loss = 0.27128849\n",
      "Iteration 5, loss = 0.25477656\n",
      "Iteration 6, loss = 0.24422260\n",
      "Iteration 7, loss = 0.23576182\n",
      "Iteration 8, loss = 0.22960193\n",
      "Iteration 9, loss = 0.22577330\n",
      "Iteration 10, loss = 0.22234266\n",
      "Iteration 11, loss = 0.21948414\n",
      "Iteration 12, loss = 0.21770256\n",
      "Iteration 13, loss = 0.21660621\n",
      "Iteration 14, loss = 0.21565391\n",
      "Iteration 15, loss = 0.21420633\n",
      "Iteration 16, loss = 0.21313692\n",
      "Iteration 17, loss = 0.21303603\n",
      "Iteration 18, loss = 0.21164755\n",
      "Iteration 19, loss = 0.21108004\n",
      "Iteration 20, loss = 0.21070947\n",
      "Iteration 21, loss = 0.21035316\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.42728992\n",
      "Iteration 2, loss = 0.32979073\n",
      "Iteration 3, loss = 0.29475586\n",
      "Iteration 4, loss = 0.27144561\n",
      "Iteration 5, loss = 0.25565769\n",
      "Iteration 6, loss = 0.24442669\n",
      "Iteration 7, loss = 0.23545191\n",
      "Iteration 8, loss = 0.22937279\n",
      "Iteration 9, loss = 0.22471817\n",
      "Iteration 10, loss = 0.22224627\n",
      "Iteration 11, loss = 0.21897793\n",
      "Iteration 12, loss = 0.21681307\n",
      "Iteration 13, loss = 0.21528853\n",
      "Iteration 14, loss = 0.21436407\n",
      "Iteration 15, loss = 0.21367345\n",
      "Iteration 16, loss = 0.21278095\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38730809\n",
      "Iteration 2, loss = 0.28720432\n",
      "Iteration 3, loss = 0.25157739\n",
      "Iteration 4, loss = 0.23550041\n",
      "Iteration 5, loss = 0.22576113\n",
      "Iteration 6, loss = 0.22016711\n",
      "Iteration 7, loss = 0.21726160\n",
      "Iteration 8, loss = 0.21481133\n",
      "Iteration 9, loss = 0.21357307\n",
      "Iteration 10, loss = 0.21257117\n",
      "Iteration 11, loss = 0.21252859\n",
      "Iteration 12, loss = 0.21034343\n",
      "Iteration 13, loss = 0.21028212\n",
      "Iteration 14, loss = 0.21004167\n",
      "Iteration 15, loss = 0.20961187\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38551112\n",
      "Iteration 2, loss = 0.28684851\n",
      "Iteration 3, loss = 0.25325115\n",
      "Iteration 4, loss = 0.23578544\n",
      "Iteration 5, loss = 0.22550656\n",
      "Iteration 6, loss = 0.22185404\n",
      "Iteration 7, loss = 0.21829110\n",
      "Iteration 8, loss = 0.21711589\n",
      "Iteration 9, loss = 0.21511477\n",
      "Iteration 10, loss = 0.21424812\n",
      "Iteration 11, loss = 0.21215871\n",
      "Iteration 12, loss = 0.21303085\n",
      "Iteration 13, loss = 0.21195630\n",
      "Iteration 14, loss = 0.21258022\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38676640\n",
      "Iteration 2, loss = 0.28831437\n",
      "Iteration 3, loss = 0.25351826\n",
      "Iteration 4, loss = 0.23644032\n",
      "Iteration 5, loss = 0.22828326\n",
      "Iteration 6, loss = 0.22335045\n",
      "Iteration 7, loss = 0.21975635\n",
      "Iteration 8, loss = 0.21863890\n",
      "Iteration 9, loss = 0.21649262\n",
      "Iteration 10, loss = 0.21623171\n",
      "Iteration 11, loss = 0.21444185\n",
      "Iteration 12, loss = 0.21397951\n",
      "Iteration 13, loss = 0.21308848\n",
      "Iteration 14, loss = 0.21307939\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38426402\n",
      "Iteration 2, loss = 0.28622274\n",
      "Iteration 3, loss = 0.25216698\n",
      "Iteration 4, loss = 0.23532517\n",
      "Iteration 5, loss = 0.22700460\n",
      "Iteration 6, loss = 0.22182458\n",
      "Iteration 7, loss = 0.21821613\n",
      "Iteration 8, loss = 0.21634630\n",
      "Iteration 9, loss = 0.21532391\n",
      "Iteration 10, loss = 0.21383801\n",
      "Iteration 11, loss = 0.21310279\n",
      "Iteration 12, loss = 0.21227691\n",
      "Iteration 13, loss = 0.21115366\n",
      "Iteration 14, loss = 0.21203967\n",
      "Iteration 15, loss = 0.21095160\n",
      "Iteration 16, loss = 0.21164218\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38383601\n",
      "Iteration 2, loss = 0.28543078\n",
      "Iteration 3, loss = 0.25133236\n",
      "Iteration 4, loss = 0.23480851\n",
      "Iteration 5, loss = 0.22536625\n",
      "Iteration 6, loss = 0.22032854\n",
      "Iteration 7, loss = 0.21763759\n",
      "Iteration 8, loss = 0.21578864\n",
      "Iteration 9, loss = 0.21425345\n",
      "Iteration 10, loss = 0.21335817\n",
      "Iteration 11, loss = 0.21201460\n",
      "Iteration 12, loss = 0.21066302\n",
      "Iteration 13, loss = 0.21124749\n",
      "Iteration 14, loss = 0.21063809\n",
      "Iteration 15, loss = 0.21155089\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56306587\n",
      "Iteration 2, loss = 0.37163277\n",
      "Iteration 3, loss = 0.34253464\n",
      "Iteration 4, loss = 0.32545641\n",
      "Iteration 5, loss = 0.31379718\n",
      "Iteration 6, loss = 0.30476292\n",
      "Iteration 7, loss = 0.29736198\n",
      "Iteration 8, loss = 0.29142603\n",
      "Iteration 9, loss = 0.28626445\n",
      "Iteration 10, loss = 0.28180814\n",
      "Iteration 11, loss = 0.27775966\n",
      "Iteration 12, loss = 0.27410518\n",
      "Iteration 13, loss = 0.27075400\n",
      "Iteration 14, loss = 0.26786452\n",
      "Iteration 15, loss = 0.26484506\n",
      "Iteration 16, loss = 0.26192075\n",
      "Iteration 17, loss = 0.25981999\n",
      "Iteration 18, loss = 0.25715475\n",
      "Iteration 19, loss = 0.25517469\n",
      "Iteration 20, loss = 0.25300985\n",
      "Iteration 21, loss = 0.25125798\n",
      "Iteration 22, loss = 0.24928982\n",
      "Iteration 23, loss = 0.24724930\n",
      "Iteration 24, loss = 0.24549252\n",
      "Iteration 25, loss = 0.24383768\n",
      "Iteration 26, loss = 0.24217362\n",
      "Iteration 27, loss = 0.24048643\n",
      "Iteration 28, loss = 0.23901821\n",
      "Iteration 29, loss = 0.23744264\n",
      "Iteration 30, loss = 0.23754193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.23517497\n",
      "Iteration 32, loss = 0.23372447\n",
      "Iteration 33, loss = 0.23266757\n",
      "Iteration 34, loss = 0.23116984\n",
      "Iteration 35, loss = 0.22998497\n",
      "Iteration 36, loss = 0.22873161\n",
      "Iteration 37, loss = 0.22767725\n",
      "Iteration 38, loss = 0.22667867\n",
      "Iteration 39, loss = 0.22554050\n",
      "Iteration 40, loss = 0.22445976\n",
      "Iteration 41, loss = 0.22355277\n",
      "Iteration 42, loss = 0.22234124\n",
      "Iteration 43, loss = 0.22137414\n",
      "Iteration 44, loss = 0.22059586\n",
      "Iteration 45, loss = 0.21968268\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55098642\n",
      "Iteration 2, loss = 0.37187134\n",
      "Iteration 3, loss = 0.34247556\n",
      "Iteration 4, loss = 0.32574085\n",
      "Iteration 5, loss = 0.31420420\n",
      "Iteration 6, loss = 0.30542999\n",
      "Iteration 7, loss = 0.29837688\n",
      "Iteration 8, loss = 0.29248909\n",
      "Iteration 9, loss = 0.28744726\n",
      "Iteration 10, loss = 0.28284207\n",
      "Iteration 11, loss = 0.27885952\n",
      "Iteration 12, loss = 0.27522884\n",
      "Iteration 13, loss = 0.27190321\n",
      "Iteration 14, loss = 0.26882033\n",
      "Iteration 15, loss = 0.26583261\n",
      "Iteration 16, loss = 0.26321473\n",
      "Iteration 17, loss = 0.26059061\n",
      "Iteration 18, loss = 0.25818886\n",
      "Iteration 19, loss = 0.25594263\n",
      "Iteration 20, loss = 0.25374076\n",
      "Iteration 21, loss = 0.25159007\n",
      "Iteration 22, loss = 0.24957867\n",
      "Iteration 23, loss = 0.24764134\n",
      "Iteration 24, loss = 0.24601435\n",
      "Iteration 25, loss = 0.24416876\n",
      "Iteration 26, loss = 0.24255989\n",
      "Iteration 27, loss = 0.24082246\n",
      "Iteration 28, loss = 0.23944257\n",
      "Iteration 29, loss = 0.23788439\n",
      "Iteration 30, loss = 0.23633662\n",
      "Iteration 31, loss = 0.23505341\n",
      "Iteration 32, loss = 0.23356377\n",
      "Iteration 33, loss = 0.23246242\n",
      "Iteration 34, loss = 0.23125426\n",
      "Iteration 35, loss = 0.22978960\n",
      "Iteration 36, loss = 0.22877845\n",
      "Iteration 37, loss = 0.22741569\n",
      "Iteration 38, loss = 0.22639431\n",
      "Iteration 39, loss = 0.22525816\n",
      "Iteration 40, loss = 0.22425799\n",
      "Iteration 41, loss = 0.22315471\n",
      "Iteration 42, loss = 0.22211243\n",
      "Iteration 43, loss = 0.22110169\n",
      "Iteration 44, loss = 0.22032244\n",
      "Iteration 45, loss = 0.21940732\n",
      "Iteration 46, loss = 0.21829745\n",
      "Iteration 47, loss = 0.21755513\n",
      "Iteration 48, loss = 0.21671886\n",
      "Iteration 49, loss = 0.21581474\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53649796\n",
      "Iteration 2, loss = 0.37481210\n",
      "Iteration 3, loss = 0.34609835\n",
      "Iteration 4, loss = 0.32946952\n",
      "Iteration 5, loss = 0.31797590\n",
      "Iteration 6, loss = 0.30911516\n",
      "Iteration 7, loss = 0.30195407\n",
      "Iteration 8, loss = 0.29606156\n",
      "Iteration 9, loss = 0.29077430\n",
      "Iteration 10, loss = 0.28615976\n",
      "Iteration 11, loss = 0.28219430\n",
      "Iteration 12, loss = 0.27829457\n",
      "Iteration 13, loss = 0.27486293\n",
      "Iteration 14, loss = 0.27157031\n",
      "Iteration 15, loss = 0.26848670\n",
      "Iteration 16, loss = 0.26555776\n",
      "Iteration 17, loss = 0.26303030\n",
      "Iteration 18, loss = 0.26032619\n",
      "Iteration 19, loss = 0.25815402\n",
      "Iteration 20, loss = 0.25590272\n",
      "Iteration 21, loss = 0.25356145\n",
      "Iteration 22, loss = 0.25154454\n",
      "Iteration 23, loss = 0.24961223\n",
      "Iteration 24, loss = 0.24763931\n",
      "Iteration 25, loss = 0.24600780\n",
      "Iteration 26, loss = 0.24399929\n",
      "Iteration 27, loss = 0.24237211\n",
      "Iteration 28, loss = 0.24078443\n",
      "Iteration 29, loss = 0.23917888\n",
      "Iteration 30, loss = 0.23772038\n",
      "Iteration 31, loss = 0.23620514\n",
      "Iteration 32, loss = 0.23490125\n",
      "Iteration 33, loss = 0.23366428\n",
      "Iteration 34, loss = 0.23226044\n",
      "Iteration 35, loss = 0.23093047\n",
      "Iteration 36, loss = 0.22980863\n",
      "Iteration 37, loss = 0.22853420\n",
      "Iteration 38, loss = 0.22750805\n",
      "Iteration 39, loss = 0.22635579\n",
      "Iteration 40, loss = 0.22524813\n",
      "Iteration 41, loss = 0.22405633\n",
      "Iteration 42, loss = 0.22312946\n",
      "Iteration 43, loss = 0.22225263\n",
      "Iteration 44, loss = 0.22126791\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54088077\n",
      "Iteration 2, loss = 0.37157830\n",
      "Iteration 3, loss = 0.34299274\n",
      "Iteration 4, loss = 0.32661420\n",
      "Iteration 5, loss = 0.31528156\n",
      "Iteration 6, loss = 0.30674584\n",
      "Iteration 7, loss = 0.29990108\n",
      "Iteration 8, loss = 0.29384313\n",
      "Iteration 9, loss = 0.28895395\n",
      "Iteration 10, loss = 0.28437815\n",
      "Iteration 11, loss = 0.28036866\n",
      "Iteration 12, loss = 0.27691202\n",
      "Iteration 13, loss = 0.27346378\n",
      "Iteration 14, loss = 0.27051901\n",
      "Iteration 15, loss = 0.26756664\n",
      "Iteration 16, loss = 0.26494748\n",
      "Iteration 17, loss = 0.26259007\n",
      "Iteration 18, loss = 0.26002743\n",
      "Iteration 19, loss = 0.25762740\n",
      "Iteration 20, loss = 0.25533941\n",
      "Iteration 21, loss = 0.25332145\n",
      "Iteration 22, loss = 0.25119164\n",
      "Iteration 23, loss = 0.24950422\n",
      "Iteration 24, loss = 0.24758803\n",
      "Iteration 25, loss = 0.24574770\n",
      "Iteration 26, loss = 0.24395236\n",
      "Iteration 27, loss = 0.24265790\n",
      "Iteration 28, loss = 0.24094340\n",
      "Iteration 29, loss = 0.23943802\n",
      "Iteration 30, loss = 0.23796869\n",
      "Iteration 31, loss = 0.23646091\n",
      "Iteration 32, loss = 0.23512901\n",
      "Iteration 33, loss = 0.23372036\n",
      "Iteration 34, loss = 0.23266278\n",
      "Iteration 35, loss = 0.23114154\n",
      "Iteration 36, loss = 0.23007242\n",
      "Iteration 37, loss = 0.22900256\n",
      "Iteration 38, loss = 0.22772867\n",
      "Iteration 39, loss = 0.22659055\n",
      "Iteration 40, loss = 0.22562808\n",
      "Iteration 41, loss = 0.22463231\n",
      "Iteration 42, loss = 0.22349379\n",
      "Iteration 43, loss = 0.22265097\n",
      "Iteration 44, loss = 0.22167537\n",
      "Iteration 45, loss = 0.22059237\n",
      "Iteration 46, loss = 0.21974657\n",
      "Iteration 47, loss = 0.21902154\n",
      "Iteration 48, loss = 0.21819809\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50996507\n",
      "Iteration 2, loss = 0.37075336\n",
      "Iteration 3, loss = 0.34351428\n",
      "Iteration 4, loss = 0.32760609\n",
      "Iteration 5, loss = 0.31665626\n",
      "Iteration 6, loss = 0.30815971\n",
      "Iteration 7, loss = 0.30140097\n",
      "Iteration 8, loss = 0.29539175\n",
      "Iteration 9, loss = 0.29042237\n",
      "Iteration 10, loss = 0.28595251\n",
      "Iteration 11, loss = 0.28202473\n",
      "Iteration 12, loss = 0.27820161\n",
      "Iteration 13, loss = 0.27491130\n",
      "Iteration 14, loss = 0.27169943\n",
      "Iteration 15, loss = 0.26874094\n",
      "Iteration 16, loss = 0.26590035\n",
      "Iteration 17, loss = 0.26344907\n",
      "Iteration 18, loss = 0.26104431\n",
      "Iteration 19, loss = 0.25878460\n",
      "Iteration 20, loss = 0.25640787\n",
      "Iteration 21, loss = 0.25428585\n",
      "Iteration 22, loss = 0.25215455\n",
      "Iteration 23, loss = 0.25030433\n",
      "Iteration 24, loss = 0.24851895\n",
      "Iteration 25, loss = 0.24666906\n",
      "Iteration 26, loss = 0.24501632\n",
      "Iteration 27, loss = 0.24334846\n",
      "Iteration 28, loss = 0.24187473\n",
      "Iteration 29, loss = 0.24013712\n",
      "Iteration 30, loss = 0.23858645\n",
      "Iteration 31, loss = 0.23726890\n",
      "Iteration 32, loss = 0.23582539\n",
      "Iteration 33, loss = 0.23455090\n",
      "Iteration 34, loss = 0.23324275\n",
      "Iteration 35, loss = 0.23197020\n",
      "Iteration 36, loss = 0.23077984\n",
      "Iteration 37, loss = 0.22961034\n",
      "Iteration 38, loss = 0.22834738\n",
      "Iteration 39, loss = 0.22724696\n",
      "Iteration 40, loss = 0.22606272\n",
      "Iteration 41, loss = 0.22475363\n",
      "Iteration 42, loss = 0.22401873\n",
      "Iteration 43, loss = 0.22291420\n",
      "Iteration 44, loss = 0.22206314\n",
      "Iteration 45, loss = 0.22086850\n",
      "Iteration 46, loss = 0.22031207\n",
      "Iteration 47, loss = 0.21919399\n",
      "Iteration 48, loss = 0.21831861\n",
      "Iteration 49, loss = 0.21743012\n",
      "Iteration 50, loss = 0.21646240\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38660265\n",
      "Iteration 2, loss = 0.29500741\n",
      "Iteration 3, loss = 0.27446428\n",
      "Iteration 4, loss = 0.26001414\n",
      "Iteration 5, loss = 0.24942195\n",
      "Iteration 6, loss = 0.24084670\n",
      "Iteration 7, loss = 0.23443995\n",
      "Iteration 8, loss = 0.22694478\n",
      "Iteration 9, loss = 0.22170477\n",
      "Iteration 10, loss = 0.21758939\n",
      "Iteration 11, loss = 0.21289459\n",
      "Iteration 12, loss = 0.20840921\n",
      "Iteration 13, loss = 0.20614746\n",
      "Iteration 14, loss = 0.20358887\n",
      "Iteration 15, loss = 0.20104815\n",
      "Iteration 16, loss = 0.19857635\n",
      "Iteration 17, loss = 0.19800272\n",
      "Iteration 18, loss = 0.19478916\n",
      "Iteration 19, loss = 0.19324954\n",
      "Iteration 20, loss = 0.19146546\n",
      "Iteration 21, loss = 0.19036740\n",
      "Iteration 22, loss = 0.18892231\n",
      "Iteration 23, loss = 0.18768581\n",
      "Iteration 24, loss = 0.18856654\n",
      "Iteration 25, loss = 0.18606116\n",
      "Iteration 26, loss = 0.18531793\n",
      "Iteration 27, loss = 0.18346153\n",
      "Iteration 28, loss = 0.18438923\n",
      "Iteration 29, loss = 0.18331620\n",
      "Iteration 30, loss = 0.18262133\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38935319\n",
      "Iteration 2, loss = 0.29373153\n",
      "Iteration 3, loss = 0.27367549\n",
      "Iteration 4, loss = 0.25983742\n",
      "Iteration 5, loss = 0.24984133\n",
      "Iteration 6, loss = 0.24149717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.23494699\n",
      "Iteration 8, loss = 0.22834074\n",
      "Iteration 9, loss = 0.22237132\n",
      "Iteration 10, loss = 0.21870452\n",
      "Iteration 11, loss = 0.21492200\n",
      "Iteration 12, loss = 0.21155775\n",
      "Iteration 13, loss = 0.20814446\n",
      "Iteration 14, loss = 0.20537766\n",
      "Iteration 15, loss = 0.20266841\n",
      "Iteration 16, loss = 0.20058853\n",
      "Iteration 17, loss = 0.19826638\n",
      "Iteration 18, loss = 0.19644260\n",
      "Iteration 19, loss = 0.19492543\n",
      "Iteration 20, loss = 0.19329065\n",
      "Iteration 21, loss = 0.19196829\n",
      "Iteration 22, loss = 0.19092738\n",
      "Iteration 23, loss = 0.19036864\n",
      "Iteration 24, loss = 0.18853376\n",
      "Iteration 25, loss = 0.18839921\n",
      "Iteration 26, loss = 0.18649069\n",
      "Iteration 27, loss = 0.18604480\n",
      "Iteration 28, loss = 0.18531386\n",
      "Iteration 29, loss = 0.18501023\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38844680\n",
      "Iteration 2, loss = 0.29781767\n",
      "Iteration 3, loss = 0.27730919\n",
      "Iteration 4, loss = 0.26371531\n",
      "Iteration 5, loss = 0.25285006\n",
      "Iteration 6, loss = 0.24373677\n",
      "Iteration 7, loss = 0.23690561\n",
      "Iteration 8, loss = 0.23099048\n",
      "Iteration 9, loss = 0.22523403\n",
      "Iteration 10, loss = 0.22095571\n",
      "Iteration 11, loss = 0.21696228\n",
      "Iteration 12, loss = 0.21313727\n",
      "Iteration 13, loss = 0.20995666\n",
      "Iteration 14, loss = 0.20714534\n",
      "Iteration 15, loss = 0.20549650\n",
      "Iteration 16, loss = 0.20207982\n",
      "Iteration 17, loss = 0.20013793\n",
      "Iteration 18, loss = 0.19841806\n",
      "Iteration 19, loss = 0.19722281\n",
      "Iteration 20, loss = 0.19506561\n",
      "Iteration 21, loss = 0.19434886\n",
      "Iteration 22, loss = 0.19274117\n",
      "Iteration 23, loss = 0.19184341\n",
      "Iteration 24, loss = 0.19040161\n",
      "Iteration 25, loss = 0.18994192\n",
      "Iteration 26, loss = 0.19057389\n",
      "Iteration 27, loss = 0.18771471\n",
      "Iteration 28, loss = 0.18716410\n",
      "Iteration 29, loss = 0.18648429\n",
      "Iteration 30, loss = 0.18563800\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38632079\n",
      "Iteration 2, loss = 0.29375101\n",
      "Iteration 3, loss = 0.27341736\n",
      "Iteration 4, loss = 0.25962378\n",
      "Iteration 5, loss = 0.24948330\n",
      "Iteration 6, loss = 0.24161799\n",
      "Iteration 7, loss = 0.23431624\n",
      "Iteration 8, loss = 0.22842278\n",
      "Iteration 9, loss = 0.22336306\n",
      "Iteration 10, loss = 0.21807172\n",
      "Iteration 11, loss = 0.21514301\n",
      "Iteration 12, loss = 0.21090608\n",
      "Iteration 13, loss = 0.20822458\n",
      "Iteration 14, loss = 0.20521651\n",
      "Iteration 15, loss = 0.20400819\n",
      "Iteration 16, loss = 0.20045681\n",
      "Iteration 17, loss = 0.19901002\n",
      "Iteration 18, loss = 0.19740300\n",
      "Iteration 19, loss = 0.19498481\n",
      "Iteration 20, loss = 0.19353255\n",
      "Iteration 21, loss = 0.19266776\n",
      "Iteration 22, loss = 0.19086936\n",
      "Iteration 23, loss = 0.19003677\n",
      "Iteration 24, loss = 0.18914754\n",
      "Iteration 25, loss = 0.18804367\n",
      "Iteration 26, loss = 0.18822342\n",
      "Iteration 27, loss = 0.18653387\n",
      "Iteration 28, loss = 0.18588157\n",
      "Iteration 29, loss = 0.18537559\n",
      "Iteration 30, loss = 0.18464322\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.38561679\n",
      "Iteration 2, loss = 0.29955775\n",
      "Iteration 3, loss = 0.27865545\n",
      "Iteration 4, loss = 0.26315067\n",
      "Iteration 5, loss = 0.25307124\n",
      "Iteration 6, loss = 0.24414067\n",
      "Iteration 7, loss = 0.23640352\n",
      "Iteration 8, loss = 0.23015529\n",
      "Iteration 9, loss = 0.22524297\n",
      "Iteration 10, loss = 0.22032586\n",
      "Iteration 11, loss = 0.21702520\n",
      "Iteration 12, loss = 0.21300152\n",
      "Iteration 13, loss = 0.20959047\n",
      "Iteration 14, loss = 0.20646035\n",
      "Iteration 15, loss = 0.20423070\n",
      "Iteration 16, loss = 0.20157699\n",
      "Iteration 17, loss = 0.19932700\n",
      "Iteration 18, loss = 0.19780643\n",
      "Iteration 19, loss = 0.19549847\n",
      "Iteration 20, loss = 0.19404705\n",
      "Iteration 21, loss = 0.19274832\n",
      "Iteration 22, loss = 0.19176805\n",
      "Iteration 23, loss = 0.19024076\n",
      "Iteration 24, loss = 0.18910313\n",
      "Iteration 25, loss = 0.18829429\n",
      "Iteration 26, loss = 0.18743204\n",
      "Iteration 27, loss = 0.18730273\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35178897\n",
      "Iteration 2, loss = 0.27174353\n",
      "Iteration 3, loss = 0.24814871\n",
      "Iteration 4, loss = 0.23479023\n",
      "Iteration 5, loss = 0.22330182\n",
      "Iteration 6, loss = 0.21363826\n",
      "Iteration 7, loss = 0.20722552\n",
      "Iteration 8, loss = 0.20271372\n",
      "Iteration 9, loss = 0.19869012\n",
      "Iteration 10, loss = 0.19437275\n",
      "Iteration 11, loss = 0.19644569\n",
      "Iteration 12, loss = 0.18905689\n",
      "Iteration 13, loss = 0.18797074\n",
      "Iteration 14, loss = 0.18646760\n",
      "Iteration 15, loss = 0.18596289\n",
      "Iteration 16, loss = 0.18513038\n",
      "Iteration 17, loss = 0.18260689\n",
      "Iteration 18, loss = 0.18204524\n",
      "Iteration 19, loss = 0.18144121\n",
      "Iteration 20, loss = 0.18057040\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35721112\n",
      "Iteration 2, loss = 0.27164373\n",
      "Iteration 3, loss = 0.25040120\n",
      "Iteration 4, loss = 0.23535158\n",
      "Iteration 5, loss = 0.22318878\n",
      "Iteration 6, loss = 0.21533026\n",
      "Iteration 7, loss = 0.21149180\n",
      "Iteration 8, loss = 0.20410113\n",
      "Iteration 9, loss = 0.19979640\n",
      "Iteration 10, loss = 0.19991772\n",
      "Iteration 11, loss = 0.19365154\n",
      "Iteration 12, loss = 0.19221747\n",
      "Iteration 13, loss = 0.18951560\n",
      "Iteration 14, loss = 0.18821348\n",
      "Iteration 15, loss = 0.18655289\n",
      "Iteration 16, loss = 0.18483084\n",
      "Iteration 17, loss = 0.18423781\n",
      "Iteration 18, loss = 0.18281260\n",
      "Iteration 19, loss = 0.18244434\n",
      "Iteration 20, loss = 0.18148806\n",
      "Iteration 21, loss = 0.18109724\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35718042\n",
      "Iteration 2, loss = 0.27423864\n",
      "Iteration 3, loss = 0.25206280\n",
      "Iteration 4, loss = 0.23613297\n",
      "Iteration 5, loss = 0.22530603\n",
      "Iteration 6, loss = 0.21700701\n",
      "Iteration 7, loss = 0.21130188\n",
      "Iteration 8, loss = 0.20580159\n",
      "Iteration 9, loss = 0.20123266\n",
      "Iteration 10, loss = 0.19828814\n",
      "Iteration 11, loss = 0.19523746\n",
      "Iteration 12, loss = 0.19361795\n",
      "Iteration 13, loss = 0.19026415\n",
      "Iteration 14, loss = 0.18960025\n",
      "Iteration 15, loss = 0.18810286\n",
      "Iteration 16, loss = 0.18724182\n",
      "Iteration 17, loss = 0.18564701\n",
      "Iteration 18, loss = 0.18507727\n",
      "Iteration 19, loss = 0.18439042\n",
      "Iteration 20, loss = 0.18320314\n",
      "Iteration 21, loss = 0.18301230\n",
      "Iteration 22, loss = 0.18285020\n",
      "Iteration 23, loss = 0.18190203\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.34958811\n",
      "Iteration 2, loss = 0.26931005\n",
      "Iteration 3, loss = 0.24932980\n",
      "Iteration 4, loss = 0.23517224\n",
      "Iteration 5, loss = 0.22560636\n",
      "Iteration 6, loss = 0.21684863\n",
      "Iteration 7, loss = 0.21045261\n",
      "Iteration 8, loss = 0.20517627\n",
      "Iteration 9, loss = 0.20254815\n",
      "Iteration 10, loss = 0.19834465\n",
      "Iteration 11, loss = 0.19574154\n",
      "Iteration 12, loss = 0.19388396\n",
      "Iteration 13, loss = 0.19129440\n",
      "Iteration 14, loss = 0.18908912\n",
      "Iteration 15, loss = 0.18818259\n",
      "Iteration 16, loss = 0.18680084\n",
      "Iteration 17, loss = 0.18611717\n",
      "Iteration 18, loss = 0.18542427\n",
      "Iteration 19, loss = 0.18515172\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35024596\n",
      "Iteration 2, loss = 0.26918054\n",
      "Iteration 3, loss = 0.24792278\n",
      "Iteration 4, loss = 0.23334775\n",
      "Iteration 5, loss = 0.22319019\n",
      "Iteration 6, loss = 0.21487917\n",
      "Iteration 7, loss = 0.20824978\n",
      "Iteration 8, loss = 0.20416354\n",
      "Iteration 9, loss = 0.19938695\n",
      "Iteration 10, loss = 0.19531941\n",
      "Iteration 11, loss = 0.19425411\n",
      "Iteration 12, loss = 0.19069745\n",
      "Iteration 13, loss = 0.18943942\n",
      "Iteration 14, loss = 0.18814185\n",
      "Iteration 15, loss = 0.18656004\n",
      "Iteration 16, loss = 0.18511939\n",
      "Iteration 17, loss = 0.18397830\n",
      "Iteration 18, loss = 0.18484766\n",
      "Iteration 19, loss = 0.18226438\n",
      "Iteration 20, loss = 0.18224899\n",
      "Iteration 21, loss = 0.18125638\n",
      "Iteration 22, loss = 0.18024993\n",
      "Iteration 23, loss = 0.18082879\n",
      "Iteration 24, loss = 0.18140676\n",
      "Iteration 25, loss = 0.17926944\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60279894\n",
      "Iteration 2, loss = 0.37280662\n",
      "Iteration 3, loss = 0.33639290\n",
      "Iteration 4, loss = 0.31663684\n",
      "Iteration 5, loss = 0.30315853\n",
      "Iteration 6, loss = 0.29312934\n",
      "Iteration 7, loss = 0.28519476\n",
      "Iteration 8, loss = 0.27857901\n",
      "Iteration 9, loss = 0.27307154\n",
      "Iteration 10, loss = 0.26843248\n",
      "Iteration 11, loss = 0.26413815\n",
      "Iteration 12, loss = 0.26036972\n",
      "Iteration 13, loss = 0.25745066\n",
      "Iteration 14, loss = 0.25399398\n",
      "Iteration 15, loss = 0.25115965\n",
      "Iteration 16, loss = 0.24863211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.24627485\n",
      "Iteration 18, loss = 0.24411496\n",
      "Iteration 19, loss = 0.24192441\n",
      "Iteration 20, loss = 0.24029166\n",
      "Iteration 21, loss = 0.23817802\n",
      "Iteration 22, loss = 0.23641293\n",
      "Iteration 23, loss = 0.23497948\n",
      "Iteration 24, loss = 0.23322413\n",
      "Iteration 25, loss = 0.23160037\n",
      "Iteration 26, loss = 0.23028931\n",
      "Iteration 27, loss = 0.22914061\n",
      "Iteration 28, loss = 0.22788057\n",
      "Iteration 29, loss = 0.22633546\n",
      "Iteration 30, loss = 0.22505139\n",
      "Iteration 31, loss = 0.22398899\n",
      "Iteration 32, loss = 0.22285616\n",
      "Iteration 33, loss = 0.22173368\n",
      "Iteration 34, loss = 0.22062480\n",
      "Iteration 35, loss = 0.21965072\n",
      "Iteration 36, loss = 0.21888366\n",
      "Iteration 37, loss = 0.21770710\n",
      "Iteration 38, loss = 0.21684766\n",
      "Iteration 39, loss = 0.21592769\n",
      "Iteration 40, loss = 0.21496327\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64249041\n",
      "Iteration 2, loss = 0.38371154\n",
      "Iteration 3, loss = 0.34428103\n",
      "Iteration 4, loss = 0.32352589\n",
      "Iteration 5, loss = 0.30992888\n",
      "Iteration 6, loss = 0.29953762\n",
      "Iteration 7, loss = 0.29143184\n",
      "Iteration 8, loss = 0.28500507\n",
      "Iteration 9, loss = 0.27863485\n",
      "Iteration 10, loss = 0.27377753\n",
      "Iteration 11, loss = 0.26941480\n",
      "Iteration 12, loss = 0.26530010\n",
      "Iteration 13, loss = 0.26170351\n",
      "Iteration 14, loss = 0.25851705\n",
      "Iteration 15, loss = 0.25553989\n",
      "Iteration 16, loss = 0.25284845\n",
      "Iteration 17, loss = 0.25044159\n",
      "Iteration 18, loss = 0.24805191\n",
      "Iteration 19, loss = 0.24580111\n",
      "Iteration 20, loss = 0.24352715\n",
      "Iteration 21, loss = 0.24220235\n",
      "Iteration 22, loss = 0.23977012\n",
      "Iteration 23, loss = 0.23798747\n",
      "Iteration 24, loss = 0.23629619\n",
      "Iteration 25, loss = 0.23477350\n",
      "Iteration 26, loss = 0.23330194\n",
      "Iteration 27, loss = 0.23169012\n",
      "Iteration 28, loss = 0.23030506\n",
      "Iteration 29, loss = 0.22905563\n",
      "Iteration 30, loss = 0.22767245\n",
      "Iteration 31, loss = 0.22678977\n",
      "Iteration 32, loss = 0.22525625\n",
      "Iteration 33, loss = 0.22408472\n",
      "Iteration 34, loss = 0.22298659\n",
      "Iteration 35, loss = 0.22190716\n",
      "Iteration 36, loss = 0.22088349\n",
      "Iteration 37, loss = 0.21990494\n",
      "Iteration 38, loss = 0.21879025\n",
      "Iteration 39, loss = 0.21798833\n",
      "Iteration 40, loss = 0.21691961\n",
      "Iteration 41, loss = 0.21599324\n",
      "Iteration 42, loss = 0.21507942\n",
      "Iteration 43, loss = 0.21458051\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61328041\n",
      "Iteration 2, loss = 0.37636740\n",
      "Iteration 3, loss = 0.33770199\n",
      "Iteration 4, loss = 0.31675767\n",
      "Iteration 5, loss = 0.30320018\n",
      "Iteration 6, loss = 0.29303491\n",
      "Iteration 7, loss = 0.28516936\n",
      "Iteration 8, loss = 0.27875896\n",
      "Iteration 9, loss = 0.27342301\n",
      "Iteration 10, loss = 0.26887866\n",
      "Iteration 11, loss = 0.26483276\n",
      "Iteration 12, loss = 0.26156388\n",
      "Iteration 13, loss = 0.25816917\n",
      "Iteration 14, loss = 0.25512816\n",
      "Iteration 15, loss = 0.25246261\n",
      "Iteration 16, loss = 0.24999622\n",
      "Iteration 17, loss = 0.24766895\n",
      "Iteration 18, loss = 0.24569186\n",
      "Iteration 19, loss = 0.24366976\n",
      "Iteration 20, loss = 0.24198976\n",
      "Iteration 21, loss = 0.24011894\n",
      "Iteration 22, loss = 0.23836985\n",
      "Iteration 23, loss = 0.23683056\n",
      "Iteration 24, loss = 0.23535365\n",
      "Iteration 25, loss = 0.23412530\n",
      "Iteration 26, loss = 0.23268914\n",
      "Iteration 27, loss = 0.23144248\n",
      "Iteration 28, loss = 0.23005884\n",
      "Iteration 29, loss = 0.22886222\n",
      "Iteration 30, loss = 0.22767505\n",
      "Iteration 31, loss = 0.22648468\n",
      "Iteration 32, loss = 0.22539313\n",
      "Iteration 33, loss = 0.22443699\n",
      "Iteration 34, loss = 0.22330317\n",
      "Iteration 35, loss = 0.22240037\n",
      "Iteration 36, loss = 0.22145526\n",
      "Iteration 37, loss = 0.22055632\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58780061\n",
      "Iteration 2, loss = 0.36770777\n",
      "Iteration 3, loss = 0.33303657\n",
      "Iteration 4, loss = 0.31431152\n",
      "Iteration 5, loss = 0.30143307\n",
      "Iteration 6, loss = 0.29188530\n",
      "Iteration 7, loss = 0.28421863\n",
      "Iteration 8, loss = 0.27788931\n",
      "Iteration 9, loss = 0.27267879\n",
      "Iteration 10, loss = 0.26791181\n",
      "Iteration 11, loss = 0.26381525\n",
      "Iteration 12, loss = 0.26013044\n",
      "Iteration 13, loss = 0.25673673\n",
      "Iteration 14, loss = 0.25418994\n",
      "Iteration 15, loss = 0.25088740\n",
      "Iteration 16, loss = 0.24843303\n",
      "Iteration 17, loss = 0.24651895\n",
      "Iteration 18, loss = 0.24388427\n",
      "Iteration 19, loss = 0.24170791\n",
      "Iteration 20, loss = 0.23985829\n",
      "Iteration 21, loss = 0.23800087\n",
      "Iteration 22, loss = 0.23634781\n",
      "Iteration 23, loss = 0.23466185\n",
      "Iteration 24, loss = 0.23301435\n",
      "Iteration 25, loss = 0.23188853\n",
      "Iteration 26, loss = 0.23013032\n",
      "Iteration 27, loss = 0.22893576\n",
      "Iteration 28, loss = 0.22770712\n",
      "Iteration 29, loss = 0.22632486\n",
      "Iteration 30, loss = 0.22502955\n",
      "Iteration 31, loss = 0.22413124\n",
      "Iteration 32, loss = 0.22290599\n",
      "Iteration 33, loss = 0.22174916\n",
      "Iteration 34, loss = 0.22081183\n",
      "Iteration 35, loss = 0.21971873\n",
      "Iteration 36, loss = 0.21882111\n",
      "Iteration 37, loss = 0.21783373\n",
      "Iteration 38, loss = 0.21682743\n",
      "Iteration 39, loss = 0.21596265\n",
      "Iteration 40, loss = 0.21526767\n",
      "Iteration 41, loss = 0.21430346\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62291704\n",
      "Iteration 2, loss = 0.37971559\n",
      "Iteration 3, loss = 0.34052672\n",
      "Iteration 4, loss = 0.32012741\n",
      "Iteration 5, loss = 0.30593786\n",
      "Iteration 6, loss = 0.29549514\n",
      "Iteration 7, loss = 0.28722569\n",
      "Iteration 8, loss = 0.28040523\n",
      "Iteration 9, loss = 0.27452863\n",
      "Iteration 10, loss = 0.26960708\n",
      "Iteration 11, loss = 0.26529003\n",
      "Iteration 12, loss = 0.26155933\n",
      "Iteration 13, loss = 0.25791287\n",
      "Iteration 14, loss = 0.25506957\n",
      "Iteration 15, loss = 0.25203348\n",
      "Iteration 16, loss = 0.24927475\n",
      "Iteration 17, loss = 0.24681562\n",
      "Iteration 18, loss = 0.24455240\n",
      "Iteration 19, loss = 0.24237904\n",
      "Iteration 20, loss = 0.24036974\n",
      "Iteration 21, loss = 0.23852407\n",
      "Iteration 22, loss = 0.23675343\n",
      "Iteration 23, loss = 0.23508166\n",
      "Iteration 24, loss = 0.23346104\n",
      "Iteration 25, loss = 0.23194692\n",
      "Iteration 26, loss = 0.23035836\n",
      "Iteration 27, loss = 0.22923805\n",
      "Iteration 28, loss = 0.22827554\n",
      "Iteration 29, loss = 0.22675494\n",
      "Iteration 30, loss = 0.22552795\n",
      "Iteration 31, loss = 0.22417239\n",
      "Iteration 32, loss = 0.22310950\n",
      "Iteration 33, loss = 0.22189624\n",
      "Iteration 34, loss = 0.22098032\n",
      "Iteration 35, loss = 0.21992641\n",
      "Iteration 36, loss = 0.21897307\n",
      "Iteration 37, loss = 0.21798044\n",
      "Iteration 38, loss = 0.21705848\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41737856\n",
      "Iteration 2, loss = 0.27774191\n",
      "Iteration 3, loss = 0.25642428\n",
      "Iteration 4, loss = 0.24465493\n",
      "Iteration 5, loss = 0.23560114\n",
      "Iteration 6, loss = 0.22856314\n",
      "Iteration 7, loss = 0.22271725\n",
      "Iteration 8, loss = 0.21831412\n",
      "Iteration 9, loss = 0.21484091\n",
      "Iteration 10, loss = 0.21002730\n",
      "Iteration 11, loss = 0.20639734\n",
      "Iteration 12, loss = 0.20326102\n",
      "Iteration 13, loss = 0.20079140\n",
      "Iteration 14, loss = 0.19993671\n",
      "Iteration 15, loss = 0.19626112\n",
      "Iteration 16, loss = 0.19404456\n",
      "Iteration 17, loss = 0.19200850\n",
      "Iteration 18, loss = 0.19000355\n",
      "Iteration 19, loss = 0.18826102\n",
      "Iteration 20, loss = 0.18678305\n",
      "Iteration 21, loss = 0.18552126\n",
      "Iteration 22, loss = 0.18441134\n",
      "Iteration 23, loss = 0.18245859\n",
      "Iteration 24, loss = 0.18197951\n",
      "Iteration 25, loss = 0.18191577\n",
      "Iteration 26, loss = 0.17998442\n",
      "Iteration 27, loss = 0.17845076\n",
      "Iteration 28, loss = 0.17703669\n",
      "Iteration 29, loss = 0.17617702\n",
      "Iteration 30, loss = 0.17817242\n",
      "Iteration 31, loss = 0.17409235\n",
      "Iteration 32, loss = 0.17432740\n",
      "Iteration 33, loss = 0.17303646\n",
      "Iteration 34, loss = 0.17488646\n",
      "Iteration 35, loss = 0.17238346\n",
      "Iteration 36, loss = 0.17039412\n",
      "Iteration 37, loss = 0.17000887\n",
      "Iteration 38, loss = 0.17347283\n",
      "Iteration 39, loss = 0.16912736\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40715980\n",
      "Iteration 2, loss = 0.28386096\n",
      "Iteration 3, loss = 0.26190122\n",
      "Iteration 4, loss = 0.24816930\n",
      "Iteration 5, loss = 0.23830908\n",
      "Iteration 6, loss = 0.23084068\n",
      "Iteration 7, loss = 0.22762831\n",
      "Iteration 8, loss = 0.21983398\n",
      "Iteration 9, loss = 0.21494021\n",
      "Iteration 10, loss = 0.21286561\n",
      "Iteration 11, loss = 0.20835086\n",
      "Iteration 12, loss = 0.20488026\n",
      "Iteration 13, loss = 0.20184942\n",
      "Iteration 14, loss = 0.19936945\n",
      "Iteration 15, loss = 0.19715808\n",
      "Iteration 16, loss = 0.19481861\n",
      "Iteration 17, loss = 0.19303571\n",
      "Iteration 18, loss = 0.19331296\n",
      "Iteration 19, loss = 0.18917577\n",
      "Iteration 20, loss = 0.18730918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.18590020\n",
      "Iteration 22, loss = 0.18475414\n",
      "Iteration 23, loss = 0.18335939\n",
      "Iteration 24, loss = 0.18322764\n",
      "Iteration 25, loss = 0.18093880\n",
      "Iteration 26, loss = 0.17978426\n",
      "Iteration 27, loss = 0.18089820\n",
      "Iteration 28, loss = 0.17764629\n",
      "Iteration 29, loss = 0.17874730\n",
      "Iteration 30, loss = 0.17534342\n",
      "Iteration 31, loss = 0.17486427\n",
      "Iteration 32, loss = 0.17381160\n",
      "Iteration 33, loss = 0.17308170\n",
      "Iteration 34, loss = 0.17250193\n",
      "Iteration 35, loss = 0.17214100\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.41594817\n",
      "Iteration 2, loss = 0.27837018\n",
      "Iteration 3, loss = 0.25864042\n",
      "Iteration 4, loss = 0.24759764\n",
      "Iteration 5, loss = 0.23789281\n",
      "Iteration 6, loss = 0.23174023\n",
      "Iteration 7, loss = 0.22598838\n",
      "Iteration 8, loss = 0.22097377\n",
      "Iteration 9, loss = 0.21714847\n",
      "Iteration 10, loss = 0.21349360\n",
      "Iteration 11, loss = 0.21000778\n",
      "Iteration 12, loss = 0.20758922\n",
      "Iteration 13, loss = 0.20448200\n",
      "Iteration 14, loss = 0.20361479\n",
      "Iteration 15, loss = 0.20051598\n",
      "Iteration 16, loss = 0.19709861\n",
      "Iteration 17, loss = 0.19721708\n",
      "Iteration 18, loss = 0.19348712\n",
      "Iteration 19, loss = 0.19210737\n",
      "Iteration 20, loss = 0.18997286\n",
      "Iteration 21, loss = 0.18859033\n",
      "Iteration 22, loss = 0.18786467\n",
      "Iteration 23, loss = 0.18582420\n",
      "Iteration 24, loss = 0.18446042\n",
      "Iteration 25, loss = 0.18325169\n",
      "Iteration 26, loss = 0.18242915\n",
      "Iteration 27, loss = 0.18132818\n",
      "Iteration 28, loss = 0.18033057\n",
      "Iteration 29, loss = 0.17921210\n",
      "Iteration 30, loss = 0.17879631\n",
      "Iteration 31, loss = 0.17763640\n",
      "Iteration 32, loss = 0.17660625\n",
      "Iteration 33, loss = 0.17586468\n",
      "Iteration 34, loss = 0.17447573\n",
      "Iteration 35, loss = 0.17435698\n",
      "Iteration 36, loss = 0.17346677\n",
      "Iteration 37, loss = 0.17248848\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40654999\n",
      "Iteration 2, loss = 0.27407603\n",
      "Iteration 3, loss = 0.25558136\n",
      "Iteration 4, loss = 0.24432821\n",
      "Iteration 5, loss = 0.23596662\n",
      "Iteration 6, loss = 0.23028090\n",
      "Iteration 7, loss = 0.22417406\n",
      "Iteration 8, loss = 0.21912082\n",
      "Iteration 9, loss = 0.21532250\n",
      "Iteration 10, loss = 0.21187978\n",
      "Iteration 11, loss = 0.20851262\n",
      "Iteration 12, loss = 0.20575545\n",
      "Iteration 13, loss = 0.20318133\n",
      "Iteration 14, loss = 0.20185608\n",
      "Iteration 15, loss = 0.19955129\n",
      "Iteration 16, loss = 0.19732941\n",
      "Iteration 17, loss = 0.19510749\n",
      "Iteration 18, loss = 0.19322088\n",
      "Iteration 19, loss = 0.19169589\n",
      "Iteration 20, loss = 0.19030977\n",
      "Iteration 21, loss = 0.18854871\n",
      "Iteration 22, loss = 0.18718283\n",
      "Iteration 23, loss = 0.18611111\n",
      "Iteration 24, loss = 0.18469485\n",
      "Iteration 25, loss = 0.18327748\n",
      "Iteration 26, loss = 0.18312200\n",
      "Iteration 27, loss = 0.18125765\n",
      "Iteration 28, loss = 0.18028403\n",
      "Iteration 29, loss = 0.17951354\n",
      "Iteration 30, loss = 0.17826597\n",
      "Iteration 31, loss = 0.17768780\n",
      "Iteration 32, loss = 0.17672232\n",
      "Iteration 33, loss = 0.17650487\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40744966\n",
      "Iteration 2, loss = 0.28042368\n",
      "Iteration 3, loss = 0.25834556\n",
      "Iteration 4, loss = 0.24472165\n",
      "Iteration 5, loss = 0.23554037\n",
      "Iteration 6, loss = 0.22880713\n",
      "Iteration 7, loss = 0.22286765\n",
      "Iteration 8, loss = 0.21818559\n",
      "Iteration 9, loss = 0.21445601\n",
      "Iteration 10, loss = 0.21161066\n",
      "Iteration 11, loss = 0.20776337\n",
      "Iteration 12, loss = 0.20571773\n",
      "Iteration 13, loss = 0.20226789\n",
      "Iteration 14, loss = 0.20134677\n",
      "Iteration 15, loss = 0.20138373\n",
      "Iteration 16, loss = 0.19583612\n",
      "Iteration 17, loss = 0.19407995\n",
      "Iteration 18, loss = 0.19217924\n",
      "Iteration 19, loss = 0.19147108\n",
      "Iteration 20, loss = 0.18962673\n",
      "Iteration 21, loss = 0.18774281\n",
      "Iteration 22, loss = 0.18608551\n",
      "Iteration 23, loss = 0.18456986\n",
      "Iteration 24, loss = 0.18367518\n",
      "Iteration 25, loss = 0.18300265\n",
      "Iteration 26, loss = 0.18200333\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36496650\n",
      "Iteration 2, loss = 0.25321147\n",
      "Iteration 3, loss = 0.23412714\n",
      "Iteration 4, loss = 0.22322086\n",
      "Iteration 5, loss = 0.21401236\n",
      "Iteration 6, loss = 0.20625656\n",
      "Iteration 7, loss = 0.20118962\n",
      "Iteration 8, loss = 0.19716779\n",
      "Iteration 9, loss = 0.19183294\n",
      "Iteration 10, loss = 0.18975189\n",
      "Iteration 11, loss = 0.19477517\n",
      "Iteration 12, loss = 0.19236083\n",
      "Iteration 13, loss = 0.18168184\n",
      "Iteration 14, loss = 0.17976851\n",
      "Iteration 15, loss = 0.17749190\n",
      "Iteration 16, loss = 0.17520814\n",
      "Iteration 17, loss = 0.18085513\n",
      "Iteration 18, loss = 0.17304442\n",
      "Iteration 19, loss = 0.17053964\n",
      "Iteration 20, loss = 0.16924599\n",
      "Iteration 21, loss = 0.16837818\n",
      "Iteration 22, loss = 0.17265195\n",
      "Iteration 23, loss = 0.17065536\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35715245\n",
      "Iteration 2, loss = 0.25404106\n",
      "Iteration 3, loss = 0.23728164\n",
      "Iteration 4, loss = 0.22331531\n",
      "Iteration 5, loss = 0.21585643\n",
      "Iteration 6, loss = 0.20753498\n",
      "Iteration 7, loss = 0.20156496\n",
      "Iteration 8, loss = 0.19854837\n",
      "Iteration 9, loss = 0.19383515\n",
      "Iteration 10, loss = 0.18944435\n",
      "Iteration 11, loss = 0.18605115\n",
      "Iteration 12, loss = 0.18446780\n",
      "Iteration 13, loss = 0.18114365\n",
      "Iteration 14, loss = 0.17954127\n",
      "Iteration 15, loss = 0.17725208\n",
      "Iteration 16, loss = 0.17528174\n",
      "Iteration 17, loss = 0.17329419\n",
      "Iteration 18, loss = 0.17265470\n",
      "Iteration 19, loss = 0.18042439\n",
      "Iteration 20, loss = 0.17637178\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35675486\n",
      "Iteration 2, loss = 0.25790135\n",
      "Iteration 3, loss = 0.23816898\n",
      "Iteration 4, loss = 0.22988000\n",
      "Iteration 5, loss = 0.21748486\n",
      "Iteration 6, loss = 0.21066076\n",
      "Iteration 7, loss = 0.20488035\n",
      "Iteration 8, loss = 0.20062416\n",
      "Iteration 9, loss = 0.19663502\n",
      "Iteration 10, loss = 0.19273497\n",
      "Iteration 11, loss = 0.18901626\n",
      "Iteration 12, loss = 0.18644949\n",
      "Iteration 13, loss = 0.18429266\n",
      "Iteration 14, loss = 0.18143489\n",
      "Iteration 15, loss = 0.18007239\n",
      "Iteration 16, loss = 0.17822960\n",
      "Iteration 17, loss = 0.17654821\n",
      "Iteration 18, loss = 0.17785250\n",
      "Iteration 19, loss = 0.17454785\n",
      "Iteration 20, loss = 0.17272112\n",
      "Iteration 21, loss = 0.17321888\n",
      "Iteration 22, loss = 0.17031760\n",
      "Iteration 23, loss = 0.16976091\n",
      "Iteration 24, loss = 0.16832231\n",
      "Iteration 25, loss = 0.16775035\n",
      "Iteration 26, loss = 0.16885696\n",
      "Iteration 27, loss = 0.16625470\n",
      "Iteration 28, loss = 0.16506940\n",
      "Iteration 29, loss = 0.16584060\n",
      "Iteration 30, loss = 0.16435721\n",
      "Iteration 31, loss = 0.16373344\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35785274\n",
      "Iteration 2, loss = 0.25125488\n",
      "Iteration 3, loss = 0.23406994\n",
      "Iteration 4, loss = 0.22183754\n",
      "Iteration 5, loss = 0.21349489\n",
      "Iteration 6, loss = 0.21082236\n",
      "Iteration 7, loss = 0.20421236\n",
      "Iteration 8, loss = 0.19840908\n",
      "Iteration 9, loss = 0.19431847\n",
      "Iteration 10, loss = 0.20034587\n",
      "Iteration 11, loss = 0.19084517\n",
      "Iteration 12, loss = 0.18723861\n",
      "Iteration 13, loss = 0.18343871\n",
      "Iteration 14, loss = 0.18156001\n",
      "Iteration 15, loss = 0.17952217\n",
      "Iteration 16, loss = 0.18014787\n",
      "Iteration 17, loss = 0.17747138\n",
      "Iteration 18, loss = 0.17485381\n",
      "Iteration 19, loss = 0.17388057\n",
      "Iteration 20, loss = 0.17252619\n",
      "Iteration 21, loss = 0.17120186\n",
      "Iteration 22, loss = 0.17044932\n",
      "Iteration 23, loss = 0.16957047\n",
      "Iteration 24, loss = 0.16806136\n",
      "Iteration 25, loss = 0.16706323\n",
      "Iteration 26, loss = 0.16729938\n",
      "Iteration 27, loss = 0.16934180\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.36214296\n",
      "Iteration 2, loss = 0.25495528\n",
      "Iteration 3, loss = 0.23723213\n",
      "Iteration 4, loss = 0.22613546\n",
      "Iteration 5, loss = 0.21716665\n",
      "Iteration 6, loss = 0.21056472\n",
      "Iteration 7, loss = 0.20497477\n",
      "Iteration 8, loss = 0.20100778\n",
      "Iteration 9, loss = 0.19660797\n",
      "Iteration 10, loss = 0.19282039\n",
      "Iteration 11, loss = 0.18974298\n",
      "Iteration 12, loss = 0.18671229\n",
      "Iteration 13, loss = 0.18468976\n",
      "Iteration 14, loss = 0.18219393\n",
      "Iteration 15, loss = 0.18013210\n",
      "Iteration 16, loss = 0.17801564\n",
      "Iteration 17, loss = 0.17602069\n",
      "Iteration 18, loss = 0.17821878\n",
      "Iteration 19, loss = 0.17405543\n",
      "Iteration 20, loss = 0.17170516\n",
      "Iteration 21, loss = 0.17192658\n",
      "Iteration 22, loss = 0.16976451\n",
      "Iteration 23, loss = 0.16850452\n",
      "Iteration 24, loss = 0.16767129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.16753655\n",
      "Iteration 26, loss = 0.16573497\n",
      "Iteration 27, loss = 0.16468444\n",
      "Iteration 28, loss = 0.16397266\n",
      "Iteration 29, loss = 0.16333398\n",
      "Iteration 30, loss = 0.16270498\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.24326187\n",
      "Iteration 2, loss = 0.17641940\n",
      "Iteration 3, loss = 0.15983735\n",
      "Iteration 4, loss = 0.15060919\n",
      "Iteration 5, loss = 0.14375925\n",
      "Iteration 6, loss = 0.13893166\n",
      "Iteration 7, loss = 0.13463911\n",
      "Iteration 8, loss = 0.13161421\n",
      "Iteration 9, loss = 0.12847940\n",
      "Iteration 10, loss = 0.12610832\n",
      "Iteration 11, loss = 0.12452430\n",
      "Iteration 12, loss = 0.12167441\n",
      "Iteration 13, loss = 0.11911791\n",
      "Iteration 14, loss = 0.11755161\n",
      "Iteration 15, loss = 0.11532956\n",
      "Iteration 16, loss = 0.11286041\n",
      "Iteration 17, loss = 0.11191572\n",
      "Iteration 18, loss = 0.11090897\n",
      "Iteration 19, loss = 0.10955336\n",
      "Iteration 20, loss = 0.10740000\n",
      "Iteration 21, loss = 0.10678220\n",
      "Iteration 22, loss = 0.10420515\n",
      "Iteration 23, loss = 0.10370641\n",
      "Iteration 24, loss = 0.10162463\n",
      "Iteration 25, loss = 0.10074769\n",
      "Iteration 26, loss = 0.09840934\n",
      "Iteration 27, loss = 0.09797481\n",
      "Iteration 28, loss = 0.09717987\n",
      "Iteration 29, loss = 0.09612661\n",
      "Iteration 30, loss = 0.09503649\n",
      "Iteration 31, loss = 0.09287498\n",
      "Iteration 32, loss = 0.09310078\n",
      "Iteration 33, loss = 0.09225631\n",
      "Iteration 34, loss = 0.09034943\n",
      "Iteration 35, loss = 0.08984131\n",
      "Iteration 36, loss = 0.08927176\n",
      "Iteration 37, loss = 0.08830093\n",
      "Training loss did not improve more than tol=0.001000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(300,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'alpha': [0.001, 0.01, 0.5], 'batch_size': [10, 50, 100, 200], 'learning_rate_init': [0.001, 0.005, 0.01]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='precision_macro', verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncomp = 15\n",
    "featofvar = 'Particle Size'\n",
    "test = pcadataset.final.values[:, -ncomp:]\n",
    "y = pcadataset.final[featofvar].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, y, test_size=0.9)\n",
    "\n",
    "num_trials = 2\n",
    "model = MLPClassifier(hidden_layer_sizes=(300, ), solver='sgd', verbose=True, max_iter=100, tol=0.001)\n",
    "scores = np.zeros(num_trials)\n",
    "gridps = [{'alpha': [0.001, 0.01, 0.5], 'batch_size': [10, 50, 100, 200], 'learning_rate_init': [0.001, 0.005, 0.01]}]\n",
    "\n",
    "print('# Tuning hyper-parameters for precision')\n",
    "clf = GridSearchCV(estimator=model, param_grid=gridps, cv=5, scoring='precision_macro')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found in development set:\n",
      "\n",
      "{'alpha': 0.001, 'batch_size': 50, 'learning_rate_init': 0.005}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.942 (+/-0.010) for {'alpha': 0.001, 'batch_size': 10, 'learning_rate_init': 0.001}\n",
      "0.941 (+/-0.007) for {'alpha': 0.001, 'batch_size': 10, 'learning_rate_init': 0.005}\n",
      "0.938 (+/-0.016) for {'alpha': 0.001, 'batch_size': 10, 'learning_rate_init': 0.01}\n",
      "0.930 (+/-0.012) for {'alpha': 0.001, 'batch_size': 50, 'learning_rate_init': 0.001}\n",
      "0.945 (+/-0.004) for {'alpha': 0.001, 'batch_size': 50, 'learning_rate_init': 0.005}\n",
      "0.944 (+/-0.011) for {'alpha': 0.001, 'batch_size': 50, 'learning_rate_init': 0.01}\n",
      "0.923 (+/-0.009) for {'alpha': 0.001, 'batch_size': 100, 'learning_rate_init': 0.001}\n",
      "0.939 (+/-0.009) for {'alpha': 0.001, 'batch_size': 100, 'learning_rate_init': 0.005}\n",
      "0.934 (+/-0.042) for {'alpha': 0.001, 'batch_size': 100, 'learning_rate_init': 0.01}\n",
      "0.913 (+/-0.012) for {'alpha': 0.001, 'batch_size': 200, 'learning_rate_init': 0.001}\n",
      "0.930 (+/-0.008) for {'alpha': 0.001, 'batch_size': 200, 'learning_rate_init': 0.005}\n",
      "0.937 (+/-0.007) for {'alpha': 0.001, 'batch_size': 200, 'learning_rate_init': 0.01}\n",
      "0.938 (+/-0.012) for {'alpha': 0.01, 'batch_size': 10, 'learning_rate_init': 0.001}\n",
      "0.934 (+/-0.010) for {'alpha': 0.01, 'batch_size': 10, 'learning_rate_init': 0.005}\n",
      "0.929 (+/-0.012) for {'alpha': 0.01, 'batch_size': 10, 'learning_rate_init': 0.01}\n",
      "0.928 (+/-0.007) for {'alpha': 0.01, 'batch_size': 50, 'learning_rate_init': 0.001}\n",
      "0.941 (+/-0.010) for {'alpha': 0.01, 'batch_size': 50, 'learning_rate_init': 0.005}\n",
      "0.941 (+/-0.009) for {'alpha': 0.01, 'batch_size': 50, 'learning_rate_init': 0.01}\n",
      "0.921 (+/-0.011) for {'alpha': 0.01, 'batch_size': 100, 'learning_rate_init': 0.001}\n",
      "0.939 (+/-0.014) for {'alpha': 0.01, 'batch_size': 100, 'learning_rate_init': 0.005}\n",
      "0.942 (+/-0.010) for {'alpha': 0.01, 'batch_size': 100, 'learning_rate_init': 0.01}\n",
      "0.912 (+/-0.011) for {'alpha': 0.01, 'batch_size': 200, 'learning_rate_init': 0.001}\n",
      "0.931 (+/-0.014) for {'alpha': 0.01, 'batch_size': 200, 'learning_rate_init': 0.005}\n",
      "0.935 (+/-0.025) for {'alpha': 0.01, 'batch_size': 200, 'learning_rate_init': 0.01}\n",
      "0.899 (+/-0.013) for {'alpha': 0.5, 'batch_size': 10, 'learning_rate_init': 0.001}\n",
      "0.895 (+/-0.011) for {'alpha': 0.5, 'batch_size': 10, 'learning_rate_init': 0.005}\n",
      "0.891 (+/-0.016) for {'alpha': 0.5, 'batch_size': 10, 'learning_rate_init': 0.01}\n",
      "0.923 (+/-0.009) for {'alpha': 0.5, 'batch_size': 50, 'learning_rate_init': 0.001}\n",
      "0.924 (+/-0.007) for {'alpha': 0.5, 'batch_size': 50, 'learning_rate_init': 0.005}\n",
      "0.923 (+/-0.008) for {'alpha': 0.5, 'batch_size': 50, 'learning_rate_init': 0.01}\n",
      "0.922 (+/-0.010) for {'alpha': 0.5, 'batch_size': 100, 'learning_rate_init': 0.001}\n",
      "0.929 (+/-0.008) for {'alpha': 0.5, 'batch_size': 100, 'learning_rate_init': 0.005}\n",
      "0.929 (+/-0.013) for {'alpha': 0.5, 'batch_size': 100, 'learning_rate_init': 0.01}\n",
      "0.911 (+/-0.008) for {'alpha': 0.5, 'batch_size': 200, 'learning_rate_init': 0.001}\n",
      "0.929 (+/-0.010) for {'alpha': 0.5, 'batch_size': 200, 'learning_rate_init': 0.005}\n",
      "0.927 (+/-0.008) for {'alpha': 0.5, 'batch_size': 200, 'learning_rate_init': 0.01}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters set found in development set:')\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std*2, params))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report\n",
      "\n",
      "The model is trained on the full development set\n",
      "The scores are computed on the full evaluation set\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     88119\n",
      "        200       0.91      0.93      0.92     51480\n",
      "        500       0.92      0.91      0.91     49464\n",
      "\n",
      "avg / total       0.96      0.96      0.96    189063\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Detailed classification report\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set\")\n",
    "print(\"The scores are computed on the full evaluation set\")\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17733204\n",
      "Validation score: 0.939230\n",
      "Iteration 2, loss = 0.13490218\n",
      "Validation score: 0.945498\n",
      "Iteration 3, loss = 0.12501898\n",
      "Validation score: 0.952479\n",
      "Iteration 4, loss = 0.11759184\n",
      "Validation score: 0.954859\n",
      "Iteration 5, loss = 0.11268121\n",
      "Validation score: 0.953273\n",
      "Iteration 6, loss = 0.10867934\n",
      "Validation score: 0.956129\n",
      "Iteration 7, loss = 0.10451286\n",
      "Validation score: 0.959381\n",
      "Iteration 8, loss = 0.10048258\n",
      "Validation score: 0.957953\n",
      "Iteration 9, loss = 0.09694705\n",
      "Validation score: 0.963903\n",
      "Iteration 10, loss = 0.09342943\n",
      "Validation score: 0.965887\n",
      "Iteration 11, loss = 0.09001000\n",
      "Validation score: 0.966759\n",
      "Iteration 12, loss = 0.08755734\n",
      "Validation score: 0.961920\n",
      "Iteration 13, loss = 0.08472537\n",
      "Validation score: 0.969298\n",
      "Iteration 14, loss = 0.08166005\n",
      "Validation score: 0.967949\n",
      "Iteration 15, loss = 0.07909393\n",
      "Validation score: 0.966839\n",
      "Iteration 16, loss = 0.07673826\n",
      "Validation score: 0.970964\n",
      "Iteration 17, loss = 0.07456497\n",
      "Validation score: 0.969853\n",
      "Iteration 18, loss = 0.07254007\n",
      "Validation score: 0.974851\n",
      "Iteration 19, loss = 0.07076104\n",
      "Validation score: 0.974137\n",
      "Iteration 20, loss = 0.06882734\n",
      "Validation score: 0.974772\n",
      "Iteration 21, loss = 0.06674034\n",
      "Validation score: 0.969774\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.001000\n",
      "Iteration 22, loss = 0.05555321\n",
      "Validation score: 0.981198\n",
      "Iteration 23, loss = 0.05364853\n",
      "Validation score: 0.981595\n",
      "Iteration 24, loss = 0.05310254\n",
      "Validation score: 0.980801\n",
      "Iteration 25, loss = 0.05248365\n",
      "Validation score: 0.981753\n",
      "Iteration 26, loss = 0.05203749\n",
      "Validation score: 0.981039\n",
      "Iteration 27, loss = 0.05151668\n",
      "Validation score: 0.982388\n",
      "Iteration 28, loss = 0.05088685\n",
      "Validation score: 0.981119\n",
      "Iteration 29, loss = 0.05053781\n",
      "Validation score: 0.981991\n",
      "Iteration 30, loss = 0.05006227\n",
      "Validation score: 0.982467\n",
      "Iteration 31, loss = 0.04975877\n",
      "Validation score: 0.982309\n",
      "Iteration 32, loss = 0.04929199\n",
      "Validation score: 0.982467\n",
      "Iteration 33, loss = 0.04875772\n",
      "Validation score: 0.982626\n",
      "Iteration 34, loss = 0.04862719\n",
      "Validation score: 0.983181\n",
      "Iteration 35, loss = 0.04804716\n",
      "Validation score: 0.981119\n",
      "Iteration 36, loss = 0.04761198\n",
      "Validation score: 0.984530\n",
      "Iteration 37, loss = 0.04728857\n",
      "Validation score: 0.983340\n",
      "Iteration 38, loss = 0.04701920\n",
      "Validation score: 0.983657\n",
      "Iteration 39, loss = 0.04647004\n",
      "Validation score: 0.984371\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 40, loss = 0.04425415\n",
      "Validation score: 0.985799\n",
      "Iteration 41, loss = 0.04409733\n",
      "Validation score: 0.985799\n",
      "Iteration 42, loss = 0.04396769\n",
      "Validation score: 0.984609\n",
      "Iteration 43, loss = 0.04382800\n",
      "Validation score: 0.985085\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 44, loss = 0.04346550\n",
      "Validation score: 0.985561\n",
      "Iteration 45, loss = 0.04336468\n",
      "Validation score: 0.985641\n",
      "Iteration 46, loss = 0.04333317\n",
      "Validation score: 0.985482\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 47, loss = 0.04324648\n",
      "Validation score: 0.985561\n",
      "Iteration 48, loss = 0.04322815\n",
      "Validation score: 0.985641\n",
      "Iteration 49, loss = 0.04322108\n",
      "Validation score: 0.985641\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 50, loss = 0.04319386\n",
      "Validation score: 0.985641\n",
      "Iteration 51, loss = 0.04319335\n",
      "Validation score: 0.985641\n",
      "Iteration 52, loss = 0.04319258\n",
      "Validation score: 0.985641\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 53, loss = 0.04318645\n",
      "Validation score: 0.985641\n",
      "Iteration 54, loss = 0.04318638\n",
      "Validation score: 0.985561\n",
      "Iteration 55, loss = 0.04318621\n",
      "Validation score: 0.985641\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training Results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     58868\n",
      "        200       0.98      0.98      0.98     34239\n",
      "        500       0.98      0.98      0.98     32935\n",
      "\n",
      "avg / total       0.99      0.99      0.99    126042\n",
      "\n",
      "Test Results\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     39027\n",
      "        200       0.98      0.97      0.97     22895\n",
      "        500       0.97      0.97      0.97     22106\n",
      "\n",
      "avg / total       0.98      0.98      0.98     84028\n",
      "\n",
      "Iteration 1, loss = 0.17397446\n",
      "Validation score: 0.940738\n",
      "Iteration 2, loss = 0.13366654\n",
      "Validation score: 0.948195\n",
      "Iteration 3, loss = 0.12356681\n",
      "Validation score: 0.954621\n",
      "Iteration 4, loss = 0.11702211\n",
      "Validation score: 0.953035\n",
      "Iteration 5, loss = 0.11170053\n",
      "Validation score: 0.954463\n",
      "Iteration 6, loss = 0.10759418\n",
      "Validation score: 0.959461\n",
      "Iteration 7, loss = 0.10346663\n",
      "Validation score: 0.959461\n",
      "Iteration 8, loss = 0.09975957\n",
      "Validation score: 0.959223\n",
      "Iteration 9, loss = 0.09642745\n",
      "Validation score: 0.958747\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.001000\n",
      "Iteration 10, loss = 0.08731925\n",
      "Validation score: 0.964459\n",
      "Iteration 11, loss = 0.08569770\n",
      "Validation score: 0.965887\n",
      "Iteration 12, loss = 0.08472013\n",
      "Validation score: 0.963507\n",
      "Iteration 13, loss = 0.08420398\n",
      "Validation score: 0.966283\n",
      "Iteration 14, loss = 0.08323782\n",
      "Validation score: 0.966363\n",
      "Iteration 15, loss = 0.08256373\n",
      "Validation score: 0.966601\n",
      "Iteration 16, loss = 0.08165148\n",
      "Validation score: 0.966680\n",
      "Iteration 17, loss = 0.08105520\n",
      "Validation score: 0.966601\n",
      "Iteration 18, loss = 0.08045823\n",
      "Validation score: 0.969139\n",
      "Iteration 19, loss = 0.07962148\n",
      "Validation score: 0.967632\n",
      "Iteration 20, loss = 0.07888180\n",
      "Validation score: 0.969298\n",
      "Iteration 21, loss = 0.07821897\n",
      "Validation score: 0.969695\n",
      "Iteration 22, loss = 0.07754474\n",
      "Validation score: 0.971202\n",
      "Iteration 23, loss = 0.07675682\n",
      "Validation score: 0.968743\n",
      "Iteration 24, loss = 0.07605349\n",
      "Validation score: 0.969139\n",
      "Iteration 25, loss = 0.07558704\n",
      "Validation score: 0.971123\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 26, loss = 0.07318221\n",
      "Validation score: 0.971678\n",
      "Iteration 27, loss = 0.07289491\n",
      "Validation score: 0.971995\n",
      "Iteration 28, loss = 0.07284615\n",
      "Validation score: 0.972233\n",
      "Iteration 29, loss = 0.07264384\n",
      "Validation score: 0.971281\n",
      "Iteration 30, loss = 0.07249185\n",
      "Validation score: 0.971757\n",
      "Iteration 31, loss = 0.07234421\n",
      "Validation score: 0.971995\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 32, loss = 0.07183679\n",
      "Validation score: 0.972471\n",
      "Iteration 33, loss = 0.07176224\n",
      "Validation score: 0.972471\n",
      "Iteration 34, loss = 0.07175174\n",
      "Validation score: 0.972313\n",
      "Iteration 35, loss = 0.07170537\n",
      "Validation score: 0.971678\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 36, loss = 0.07161676\n",
      "Validation score: 0.971995\n",
      "Iteration 37, loss = 0.07159680\n",
      "Validation score: 0.972233\n",
      "Iteration 38, loss = 0.07158718\n",
      "Validation score: 0.972154\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 39, loss = 0.07156608\n",
      "Validation score: 0.972233\n",
      "Iteration 40, loss = 0.07156267\n",
      "Validation score: 0.972392\n",
      "Iteration 41, loss = 0.07156076\n",
      "Validation score: 0.972392\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 42, loss = 0.07155457\n",
      "Validation score: 0.972392\n",
      "Iteration 43, loss = 0.07155428\n",
      "Validation score: 0.972392\n",
      "Iteration 44, loss = 0.07155400\n",
      "Validation score: 0.972392\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training Results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     58684\n",
      "        200       0.96      0.95      0.96     34338\n",
      "        500       0.95      0.96      0.95     33020\n",
      "\n",
      "avg / total       0.98      0.98      0.98    126042\n",
      "\n",
      "Test Results\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     39211\n",
      "        200       0.95      0.95      0.95     22796\n",
      "        500       0.95      0.95      0.95     22021\n",
      "\n",
      "avg / total       0.97      0.97      0.97     84028\n",
      "\n",
      "Iteration 1, loss = 0.17663294\n",
      "Validation score: 0.942007\n",
      "Iteration 2, loss = 0.13589526\n",
      "Validation score: 0.951924\n",
      "Iteration 3, loss = 0.12519362\n",
      "Validation score: 0.953590\n",
      "Iteration 4, loss = 0.11855656\n",
      "Validation score: 0.958350\n",
      "Iteration 5, loss = 0.11291114\n",
      "Validation score: 0.961206\n",
      "Iteration 6, loss = 0.10813642\n",
      "Validation score: 0.960651\n",
      "Iteration 7, loss = 0.10440218\n",
      "Validation score: 0.958429\n",
      "Iteration 8, loss = 0.10010150\n",
      "Validation score: 0.964062\n",
      "Iteration 9, loss = 0.09735522\n",
      "Validation score: 0.967553\n",
      "Iteration 10, loss = 0.09449423\n",
      "Validation score: 0.967870\n",
      "Iteration 11, loss = 0.09095640\n",
      "Validation score: 0.966283\n",
      "Iteration 12, loss = 0.08956662\n",
      "Validation score: 0.969060\n",
      "Iteration 13, loss = 0.08605449\n",
      "Validation score: 0.967235\n",
      "Iteration 14, loss = 0.08347946\n",
      "Validation score: 0.970250\n",
      "Iteration 15, loss = 0.08184214\n",
      "Validation score: 0.972551\n",
      "Iteration 16, loss = 0.07864529\n",
      "Validation score: 0.973582\n",
      "Iteration 17, loss = 0.07694261\n",
      "Validation score: 0.975010\n",
      "Iteration 18, loss = 0.07480199\n",
      "Validation score: 0.973027\n",
      "Iteration 19, loss = 0.07279980\n",
      "Validation score: 0.976835\n",
      "Iteration 20, loss = 0.07061301\n",
      "Validation score: 0.972551\n",
      "Iteration 21, loss = 0.06893129\n",
      "Validation score: 0.971281\n",
      "Iteration 22, loss = 0.06667750\n",
      "Validation score: 0.972392\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.001000\n",
      "Iteration 23, loss = 0.05534842\n",
      "Validation score: 0.980960\n",
      "Iteration 24, loss = 0.05362785\n",
      "Validation score: 0.981674\n",
      "Iteration 25, loss = 0.05300357\n",
      "Validation score: 0.981991\n",
      "Iteration 26, loss = 0.05250061\n",
      "Validation score: 0.982467\n",
      "Iteration 27, loss = 0.05191346\n",
      "Validation score: 0.981833\n",
      "Iteration 28, loss = 0.05137128\n",
      "Validation score: 0.981753\n",
      "Iteration 29, loss = 0.05078449\n",
      "Validation score: 0.983023\n",
      "Iteration 30, loss = 0.05032692\n",
      "Validation score: 0.982388\n",
      "Iteration 31, loss = 0.04989872\n",
      "Validation score: 0.983261\n",
      "Iteration 32, loss = 0.04929522\n",
      "Validation score: 0.982467\n",
      "Iteration 33, loss = 0.04907452\n",
      "Validation score: 0.983340\n",
      "Iteration 34, loss = 0.04853168\n",
      "Validation score: 0.982785\n",
      "Iteration 35, loss = 0.04819369\n",
      "Validation score: 0.983419\n",
      "Iteration 36, loss = 0.04754013\n",
      "Validation score: 0.982705\n",
      "Iteration 37, loss = 0.04718594\n",
      "Validation score: 0.983578\n",
      "Iteration 38, loss = 0.04670553\n",
      "Validation score: 0.983816\n",
      "Iteration 39, loss = 0.04655449\n",
      "Validation score: 0.984689\n",
      "Iteration 40, loss = 0.04583146\n",
      "Validation score: 0.983895\n",
      "Iteration 41, loss = 0.04561273\n",
      "Validation score: 0.984292\n",
      "Iteration 42, loss = 0.04509170\n",
      "Validation score: 0.984292\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 43, loss = 0.04303752\n",
      "Validation score: 0.985561\n",
      "Iteration 44, loss = 0.04274813\n",
      "Validation score: 0.984847\n",
      "Iteration 45, loss = 0.04264016\n",
      "Validation score: 0.985165\n",
      "Iteration 46, loss = 0.04255906\n",
      "Validation score: 0.984609\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 47, loss = 0.04209895\n",
      "Validation score: 0.985244\n",
      "Iteration 48, loss = 0.04206048\n",
      "Validation score: 0.985323\n",
      "Iteration 49, loss = 0.04199913\n",
      "Validation score: 0.985244\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 50, loss = 0.04194086\n",
      "Validation score: 0.985323\n",
      "Iteration 51, loss = 0.04192065\n",
      "Validation score: 0.985403\n",
      "Iteration 52, loss = 0.04192105\n",
      "Validation score: 0.985323\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 53, loss = 0.04189248\n",
      "Validation score: 0.985323\n",
      "Iteration 54, loss = 0.04189125\n",
      "Validation score: 0.985323\n",
      "Iteration 55, loss = 0.04189062\n",
      "Validation score: 0.985323\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 56, loss = 0.04188508\n",
      "Validation score: 0.985323\n",
      "Iteration 57, loss = 0.04188498\n",
      "Validation score: 0.985323\n",
      "Iteration 58, loss = 0.04188478\n",
      "Validation score: 0.985323\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training Results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     58583\n",
      "        200       0.98      0.98      0.98     34253\n",
      "        500       0.98      0.98      0.98     33206\n",
      "\n",
      "avg / total       0.99      0.99      0.99    126042\n",
      "\n",
      "Test Results\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100       1.00      1.00      1.00     39312\n",
      "        200       0.97      0.97      0.97     22881\n",
      "        500       0.97      0.97      0.97     21835\n",
      "\n",
      "avg / total       0.99      0.99      0.99     84028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for run in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(test, y, test_size=0.4)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(300, ), solver='sgd', verbose=True, max_iter=500, tol=0.00001,\n",
    "                        alpha=0.001, batch_size=50, learning_rate_init=0.005, learning_rate='adaptive',\n",
    "                        early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Training Results')\n",
    "    y_true1, y_pred1 = y_train, clf.predict(X_train)\n",
    "    print(classification_report(y_true1, y_pred1))\n",
    "    \n",
    "    print('Test Results')\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Validation Scores')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(clf.loss_curve_, linewidth=4)\n",
    "#ax1.set_xlim([0, 60])\n",
    "ax1.set_ylim([0.04, 0.18])\n",
    "ax1.set_ylabel('Loss Curve')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(clf.validation_scores_, linewidth=4, c='g')\n",
    "ax2.set_ylim([0.94, 0.99])\n",
    "ax2.set_ylabel('Validation Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9998    1.0000    0.9999     58583\n",
      "        200     0.9800    0.9796    0.9798     34253\n",
      "        500     0.9793    0.9794    0.9793     33206\n",
      "\n",
      "avg / total     0.9890    0.9890    0.9890    126042\n",
      "\n",
      "Test Results\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9998    1.0000    0.9999     39312\n",
      "        200     0.9742    0.9722    0.9732     22881\n",
      "        500     0.9712    0.9731    0.9722     21835\n",
      "\n",
      "avg / total     0.9854    0.9854    0.9854     84028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training Results')\n",
    "y_true1, y_pred1 = y_train, clf.predict(X_train)\n",
    "print(classification_report(y_true1, y_pred1, digits=4))\n",
    "\n",
    "print('Test Results')\n",
    "print()\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([           'Track_ID',               'alpha',               'D_fit',\n",
       "                  'kurtosis',          'asymmetry1',          'asymmetry2',\n",
       "                'asymmetry3',                  'AR',          'elongation',\n",
       "               'boundedness',         'fractal_dim',         'trappedness',\n",
       "                'efficiency',        'straightness',           'MSD_ratio',\n",
       "                    'frames',                   'X',                   'Y',\n",
       "                   'Quality',      'Mean_Intensity',            'SN_Ratio',\n",
       "                     'Deff1',               'Deff2',          'Mean alpha',\n",
       "                 'Std alpha',          'Mean D_fit',           'Std D_fit',\n",
       "             'Mean kurtosis',        'Std kurtosis',     'Mean asymmetry1',\n",
       "            'Std asymmetry1',     'Mean asymmetry2',      'Std asymmetry2',\n",
       "           'Mean asymmetry3',      'Std asymmetry3',             'Mean AR',\n",
       "                    'Std AR',     'Mean elongation',      'Std elongation',\n",
       "          'Mean boundedness',     'Std boundedness',    'Mean fractal_dim',\n",
       "           'Std fractal_dim',    'Mean trappedness',     'Std trappedness',\n",
       "           'Mean efficiency',      'Std efficiency',   'Mean straightness',\n",
       "          'Std straightness',      'Mean MSD_ratio',       'Std MSD_ratio',\n",
       "               'Mean frames',          'Std frames',              'Mean X',\n",
       "                     'Std X',              'Mean Y',               'Std Y',\n",
       "              'Mean Quality',         'Std Quality', 'Mean Mean_Intensity',\n",
       "        'Std Mean_Intensity',       'Mean SN_Ratio',        'Std SN_Ratio',\n",
       "                'Mean Deff1',           'Std Deff1',          'Mean Deff2',\n",
       "                 'Std Deff2',       'Particle Size',        'Video Number',\n",
       "                           0,                     1,                     2,\n",
       "                           3,                     4,                     5,\n",
       "                           6,                     7,                     8,\n",
       "                           9,                    10,                    11,\n",
       "                          12,                    13,                    14],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deff1 = fstats_sub.Deff1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1.872, 3.858, 10**20]\n",
    "binned = np.histogram(Deff1, [0, 1.872, 3.858, 10**20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([98542, 25117, 41341]),\n",
       " array([0, 1.872, 3.858, 100000000000000000000], dtype=object))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = list(pd.cut(fstats_tot.Deff1.values, bins=[0, 1.872, 3.858, 1000], labels=['500', '200', '100']).astype(str))\n",
    "y_true2 = fstats_tot['Particle Size'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9777    0.7223    0.8309     97895\n",
      "        200     0.3095    0.1851    0.2316     57134\n",
      "        500     0.4886    0.9191    0.6380     55041\n",
      "        nan     0.0000    0.0000    0.0000         0\n",
      "\n",
      "avg / total     0.6678    0.6278    0.6174    210070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "kb = 1.381*10**-23\n",
    "T = 303\n",
    "nu = 0.000797\n",
    "\n",
    "\n",
    "size2 = 2*10**9*10**12*kb*T/(fstats_tot.Deff1.values*6*np.pi*nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = list(pd.cut(size2, bins=[-100, 150, 350, 1000000], labels=['100', '200', '500']).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9749    0.7433    0.8434     97895\n",
      "        200     0.3916    0.2781    0.3252     57134\n",
      "        500     0.5157    0.8882    0.6525     55041\n",
      "        nan     0.0000    0.0000    0.0000         0\n",
      "\n",
      "avg / total     0.6959    0.6547    0.6525    210070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fcab9c153c8>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fcab9bc49b0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x7fcab9b65da0>], dtype=object)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fstats_tot.hist(column='Deff1', by='Particle Size', sharex=True, bins=np.linspace(0, 10, 100),\n",
    "                figsize=(9, 7), grid=False, layout=(3,1), sharey=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(-10, 10, 200)\n",
    "axes = fstats_tot.hist(column='LogDeff1', by='Particle Size', layout=(3, 1), bins=bins, sharex=True, sharey=True,\n",
    "                        figsize=(10, 8), edgecolor='k')\n",
    "\n",
    "means = []\n",
    "types2 = ['100', '200', '500']\n",
    "for ax, typ in zip(axes, types2):\n",
    "    ax.set_ylim([0,10000])\n",
    "    #ax.set_xscale(\"log\", nonposx='clip')\n",
    "    ax.set_xlim([-7.5,3.5])\n",
    "    means.append(fstats_tot[fstats_tot['Particle Size']==typ]['LogDeff1'].median())\n",
    "    ax.axvline(fstats_tot[fstats_tot['Particle Size']==typ]['LogDeff1'].median(), color='k', linestyle='dashed', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6765970540572321, 0.11314102441714863, -0.5795098191928005]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.2331844   0.89486904]\n"
     ]
    }
   ],
   "source": [
    "meanD = np.array(means)\n",
    "meanD.sort()\n",
    "Dbins = meanD[0:-1] + np.diff(meanD)/2\n",
    "print(Dbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAIqCAYAAADrd7anAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3X20XXV97/v3R8KTRSSAxjwVsEYtUD0lKXDqOD070vJUEe+pnGJ7NXroSS8XrafHjoptPViUUzzl1uqopSc9UMDrMKUUS7RaGtHdjp4rSIKKBATiA7BJkIdENAWBwPf+seYOy2TvPOy59157r/V+jbHGmvM7f3Pu71q/bPLNj9/8zVQVkiRJkibuBb1OQJIkSZrtLKolSZKkliyqJUmSpJYsqiVJkqSWLKolSZKkliyqJUmSpJYsqiVJkqSWLKolaQZL8s4k65I8leSqnY6dkuSbSZ5I8qUkR3UdOzDJlUl+kOShJP912pOXpAFiUS1JM9sm4EPAld3BJEcC1wPvBw4H1gF/3dXkA8AS4ChgOfC7SU6fhnwlaSBZVEvSDFZV11fV3wGP7XToPwAbqupvqupHdIro1yZ5dXP8bcAHq2prVd0F/CXwdoAkQ0lGkrwnycNJNid5x+iFk1yV5M+TfD7JtiT/O8nLkvxpkq3N6PjPTvFHl6RZxaJakman44Cvj+5U1b8C3wKOSzIXWNB9vNk+rmv/ZcCLgYXAecDHm/NG/UfgD4AjgaeALwO3NfvXAX8yyZ9HkmY1i2pJmp0OAR7fKfY48KLmGDsdHz026hng4qp6pqo+B2wDXtV1/NNVtb4ZBf808KOquqaqnqUzzcSRaknqYlEtSbPTNuDQnWKHAj9sjrHT8dFjox6rqu1d+0/wfDEO8L2u7SfH2O9uK0kDz6JakmanDcBrR3eS/ATwU3TmWW8FNncfb7Y3TGuGkjRALKolaQZLMifJQcB+wH5JDkoyh86UjOOT/Epz/L8Bt1fVN5tTrwH+IMnc5ubF/wxc1YOPIEkDwaJakma2P6Az3eJC4P9stv+gqh4BfgW4BNgKnASc23XeRXRuXLwP+Cfgj6vqH6Yxb0kaKKmqXucgSZIkzWqOVEuSJEkt7bGobh5z+3CSO7pihydZm+Te5n1uE0+SjyXZmOT2JCd0nbOiaX9vkhVd8aVJvtGc87EkmewPKUmSJE2lvRmpvgrY+dG2FwI3VdUS4KZmH+AMOo/FXQKsBC6HThFOZ37fScCJwEVdDxm4vGk7ep6P0ZUkSdKssseiuqr+GdiyU/hs4Opm+2rgTV3xa6rjZuCwJPOB04C1VbWlWeppLXB6c+zQqvpydSZ3X9N1LUmSJGlWmOic6nlVtRmgeX9pE18IPNDVbqSJ7S4+MkZckiRJmjXmTPL1xpoPXROIj33xZCWdqSIcfPDBSxcvXjyRHLWPnnvuOV7wAu9p7Xf2c/+zjweD/TzzPPbYYzu2jzjiiEm5pv08fe65555Hq+ole2o30aL6e0nmV9XmZgrHw018BOiudBcBm5r40E7x4Sa+aIz2Y6qqVcAqgGXLltW6desmmL72xfDwMENDQ71OQ1PMfu5/9vFgsJ9nnu41GB599NFJuab9PH2S3Lc37Sb6T5w1wOgKHiuAG7rib2tWATkZeLyZHnIjcGrzZK+5wKnAjc2xHyY5uVn1421d15IkSZJmhT2OVCf5FJ1R5iOTjNBZxeNS4Nok5wH3A+c0zT8HnAlsBJ4A3gFQVVuSfBC4tWl3cVWN3vx4Pp0VRg4GPt+8JEmSpFljj0V1Vb1lnEOnjNG2gAvGuc6VwJVjxNcBx+8pD0mSJGmmcoa7JEmS1JJFtSRJktSSRbUkSZLU0mSvUy1JkqQua9as6XUKmgYW1ZIkSVPorLPO6nUKmgZO/5AkSZJasqiWJEmSWrKoliRJklpyTrUkSdIUWrBgwY7tTZs29TATTSWLakmSpCm0efPmXqegaeD0D0mSJKkli2pJkiSpJYtqSZIkqSWLakmSJKkli2pJkiSpJYtqSZIkqSWLakmSJKkli2pJkiSppVZFdZLfTrIhyR1JPpXkoCTHJLklyb1J/jrJAU3bA5v9jc3xo7uu874mfneS09p9JEmSJGl6TfiJikkWAr8FHFtVTya5FjgXOBP4SFWtTvIXwHnA5c371qp6RZJzgQ8Dv5rk2Oa844AFwBeSvLKqnm31ySRJkmaAdevW9ToFTYO2jymfAxyc5BnghcBm4PXArzXHrwY+QKeoPrvZBrgO+LMkaeKrq+op4DtJNgInAl9umZskSVLPLV26tNcpaBpMePpHVT0IXAbcT6eYfhxYD3y/qrY3zUaAhc32QuCB5tztTfsjuuNjnCNJkiTNeG2mf8ylM8p8DPB94G+AM8ZoWqOnjHNsvPhYP3MlsBJg3rx5DA8P71vSmpBt27b5XQ8A+7n/2ceDwX4eDPbzzNNm+scvAt+pqkcAklwP/DxwWJI5zWj0ImBT034EWAyMJJkDvBjY0hUf1X3Oj6mqVcAqgGXLltXQ0FCL9LW3hoeH8bvuf/Zz/7OPB4P9PBjs55mnzeof9wMnJ3lhMzf6FOBO4EvAm5s2K4Abmu01zT7N8S9WVTXxc5vVQY4BlgBfaZGXJEnSjJFkx0v9a8Ij1VV1S5LrgNuA7cBX6Ywi/z2wOsmHmtgVzSlXAJ9obkTcQmfFD6pqQ7NyyJ3NdS5w5Q9JkiTNJq1W/6iqi4CLdgp/m87qHTu3/RFwzjjXuQS4pE0ukiRJUq/4REVJkqRp0j0VZMHiBb1OR5Oo7TrVkiRJ2kvHX3X8ju073n5HDzPRZHOkWpIkaRIsWLzgx0aivTlxsDhSLUmSNAk2j2z+sZHoUeONSGfO2EX3/EXz2fTAmKsLawazqJYkSeqB2l77VIRrZnP6hyRJktSSRbUkSZLUkkW1JEmS1JJzqiVJkqbYqz7yqr1u6w2Ms5NFtSRJ0hTbf+7+e93WGxhnJ6d/SJIkSS1ZVEuSJO0DH/KisTj9Q5IkaR/s60NeAJ7Z+gywb9NANLtYVEuSJE2xu3/7boAxi3H1B6d/SJIkSS1ZVEuSJEktWVRLkiRJLVlUS5IkSS1ZVEuSJEkttSqqkxyW5Lok30xyV5J/m+TwJGuT3Nu8z23aJsnHkmxMcnuSE7qus6Jpf2+SFW0/lCRJkjSd2o5UfxT4h6p6NfBa4C7gQuCmqloC3NTsA5wBLGleK4HLAZIcDlwEnAScCFw0WohLkiRJs8GEi+okhwK/AFwBUFVPV9X3gbOBq5tmVwNvarbPBq6pjpuBw5LMB04D1lbVlqraCqwFTp9oXpIkSdJ0S1VN7MTk3wCrgDvpjFKvB94NPFhVh3W121pVc5N8Fri0qv6lid8EvBcYAg6qqg818fcDT1bVZWP8zJV0RrmZN2/e0tWrV08od+2bbdu2ccghh/Q6DU0x+7n/2ceDwX6eeuvXr+fgow/eJf7kd58cM37Bmy/Ysf3x6z6+x/bjxu97EpqybdGiRYyMjOw4tv8B+/Oan3nNPn0O7Z3ly5evr6ple2rX5omKc4ATgHdV1S1JPsrzUz3GkjFitZv4rsGqVXQKeZYtW1ZDQ0P7lLAmZnh4GL/r/mc/9z/7eDDYz1Nv+fLlYz+m/Hfu2OMTEy/fdvke248bf8/z8fPnnM9VR171/LG338FEB0o1OdrMqR4BRqrqlmb/OjpF9veaaR007w93tV/cdf4iYNNu4pIkST2zYPECkuzymojjrzreR5T3uQmPVFfVQ0keSPKqqrobOIXOVJA7gRXApc37Dc0pa4B3JllN56bEx6tqc5Ibgf/edXPiqcD7JpqXJEnSZNg8snnsEeO339GDbDTTtZn+AfAu4JNJDgC+DbyDzuj3tUnOA+4Hzmnafg44E9gIPNG0paq2JPkgcGvT7uKq2tIyL0mSJGnatCqqq+prwFgTt08Zo20BF4zRlqq6EriyTS6SJElSr/hERUmSpCn25Hef5MnvPtnrNDSF2k7/kCRJ0h586wPfAvBmxT7mSLUkSZLUkkW1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJFtSRJGmiT+TjyXsmcXfNPwoLFC3qd2sBwST1JkjTQ+uFx5LW9Zv1nmO0cqZYkSZJasqiWJEmSWnL6hyRJ0hSbc5glV7+zhyVJkqbYq//01b1OQVPM6R+SJElSSxbVkiRJUksW1ZIkSVJLFtWSJElT7Adf/QE/+OoPep2GppBFtSRJ0hS7/6P3c/9H7+91GppCrYvqJPsl+WqSzzb7xyS5Jcm9Sf46yQFN/MBmf2Nz/Oiua7yvid+d5LS2OUmSJEnTaTJGqt8N3NW1/2HgI1W1BNgKnNfEzwO2VtUrgI807UhyLHAucBxwOvDnSfabhLwkSZIGWuaEZNfXgsULep1a32m1TnWSRcAvA5cA/zVJgNcDv9Y0uRr4AHA5cHazDXAd8GdN+7OB1VX1FPCdJBuBE4Evt8lNkiRp0NX24virjt8lfsfb7+hBNv0tVTXxk5PrgD8CXgT8DvB24OZmNJoki4HPV9XxSe4ATq+qkebYt4CT6BTaN1fV/9vEr2jOuW6Mn7cSWAkwb968patXr55w7tp727Zt45BDDul1Gppi9nP/s48Hg/08ttu/cTvPPP3MuMcPPvrgXWJPfvfJSYlf8OYLdmx//LqPT8r1X7LfS3jk2UdaXWvp0qW7xLWr5cuXr6+qZXtqN+GR6iRvAB6uqvVJhkbDYzStPRzb3Tk/HqxaBawCWLZsWQ0NDY3VTJNseHgYv+v+Zz/3P/t4MNjPY1u+fPmYI7bQGbUdczT3dyYn3u3ybZdPyvXPP+T81tdqM7CqXbWZ/vE64I1JzgQOAg4F/hQ4LMmcqtoOLAI2Ne1HgMXASJI5wIuBLV3xUd3nSJIkSTPehG9UrKr3VdWiqjqazo2GX6yqXwe+BLy5abYCuKHZXtPs0xz/YnX+ibQGOLdZHeQYYAnwlYnmJUmSJE23VjcqjuO9wOokHwK+ClzRxK8APtHciLiFTiFOVW1Ici1wJ7AduKCqnp2CvCRJkqQpMSlFdVUNA8PN9rfprN6xc5sfAeeMc/4ldFYQkSRJkmadqRipliRJUpeDjjqo1yloillUS5IkTbFX/OErep2CpthkPFFRkiRJGmgW1ZIkSVJLFtWSJEkDJnNCsutrweIFvU5t1nJOtSRJ6hsLFi9g88jmXqexiy3DWwA4fOjwHmfSUdtr7Cctvv2OHmTTHyyqJUlS39g8snlGFoubruo8LHqmFNWafE7/kCRJklqyqJYkSZJasqiWJEmSWrKoliRJklqyqJYkSZJasqiWJEmSWrKoliRJs86CxQvGfHiJ1CuuUy1JkmadmboetQaXI9WSJEkCfHx5G45US5IkTbEXvfZFvU5hr/j48omzqJYkSZpiR/32Ub1OQVNswtM/kixO8qUkdyXZkOTdTfzwJGuT3Nu8z23iSfKxJBuT3J7khK5rrWja35tkRfuPJUmSJE2fNnOqtwPvqaqfBk4GLkhyLHAhcFNVLQFuavYBzgCWNK+VwOXQKcKBi4CTgBOBi0YLcUmSJGk2mHBRXVWbq+q2ZvuHwF3AQuBs4Oqm2dXAm5rts4FrquNm4LAk84HTgLVVtaWqtgJrgdMnmpckSZI03VJV7S+SHA38M3A8cH9VHdZ1bGtVzU3yWeDSqvqXJn4T8F5gCDioqj7UxN8PPFlVl43xc1bSGeVm3rx5S1evXt06d+3Ztm3bOOSQQ3qdhqaY/dz/7OPBMCj9vH79eg4++uBd4k9+98l9ik/knH2N/93//Dv2P2x/AH75V395Uq7/kv1ewiPPPjJtn+HJ+56EMUrG/Q/Yn9f8zGt2PdBHli9fvr6qlu2pXesbFZMcAvwt8F+q6ge7WXh9rAO1m/iuwapVwCqAZcuW1dDQ0D7nq303PDyM33X/s5/7n308GAaln5cvXz72KhW/c8c+xSdyzj7H1z6/csb9v3z/pFz//EPO5/Jtl0/fZ3jPOPG338FkDND2g1brVCfZn05B/cmqur4Jf6+Z1kHz/nATHwEWd52+CNi0m7gkSZI0K7RZ/SPAFcBdVfUnXYfWAKMreKwAbuiKv61ZBeRk4PGq2gzcCJyaZG5zg+KpTUySJEmaFdpM/3gd8FbgG0m+1sR+D7gUuDbJecD9wDnNsc8BZwIbgSeAdwBU1ZYkHwRubdpdXFVbWuQlSZIkTasJF9XNDYfjTaA+ZYz2BVwwzrWuBK6caC6SJGl2O3rRfO578KFd4kctfBnfHdncg4ykfeMTFSVJUs/d9+BD1EWH7hLPH+5aaEszUasbFSVJkjS4Micku74WLF7Q69SmnSPVkiRJmpDaXuMutTdoHKmWJEmSWrKoliRJ0+boRfPHnC4wnvGmF0gzjdM/JEnStBn/hsQfjNm+X6YXzP33c3udgqaYRbUkSdIUW/iOhb1OQVPM6R+SJGnS7es0D/WXQVwVxJFqSZI06fZ1mof6S79M29kXjlRLkiRJLVlUS5KkCXOax9558K8e5MG/erDXafRcP08LcfqHJEmaMKd57J2t/7QV8IbFfp4W4ki1JEnaI0ekpd1zpFqSJO2RI9LS7jlSLUmSpJ4ab671bJpv7Ui1JEkD5uhF87nvwYd2ib/wgBfwxNPP9SAjDbrx5lrD7JlvbVEtSdKA2d1UjrHio8ekXhgdxd7Z/EXz2fTAph5kNDaLakmS+tR4I9LSbDJbVgyZMXOqk5ye5O4kG5Nc2Ot8JEmaacZbgeMnDtxvx/b69et3bI+OSO/8kvrBTFvzekaMVCfZD/g48EvACHBrkjVVdWdvM5Mkqb35i36Shx58YJf4fgccxLNP/2iX+AsP2I8nnn52zGvtadrG8IL9dmw7ZUP9bLwR7A2/saEn00VmRFENnAhsrKpvAyRZDZwNWFRL0oDY18JztsRHHfXez+4Su+/Dbxg37vJ10sTsa7G934H78exTu/4jdl+L8FTVvmU6BZK8GTi9qn6j2X8rcFJVvXOndiuBlc3uq4C7pzXRwXUk8Givk9CUs5/7n308GOznwWA/T5+jquole2o0U0aqx3ok0y7VflWtAlZNfTrqlmRdVS3rdR6aWvZz/7OPB4P9PBjs55lnptyoOAIs7tpfBMycNVIkSZKk3ZgpRfWtwJIkxyQ5ADgXWNPjnCRJkqS9MiOmf1TV9iTvBG4E9gOurKoNPU5Lz3PKzWCwn/uffTwY7OfBYD/PMDPiRkVJkiRpNpsp0z8kSZKkWcuiWpIkSWrJolp7Lcm7mkfJb0jyP3qdj6ZGkt9JUkmO7HUumnxJ/jjJN5PcnuTTSQ7rdU6aPElOb/47vTHJhb3OR5MryeIkX0pyV/N38bt7nZOeZ1GtvZJkOZ2nXL6mqo4DLutxSpoCSRYDvwTc3+tcNGXWAsdX1WuAe4D39TgfTZIk+wEfB84AjgXekuTY3malSbYdeE9V/TRwMnCBfTxzWFRrb50PXFpVTwFU1cM9zkdT4yPA7zLGw5fUH6rqH6tqe7N7M53nAqg/nAhsrKpvV9XTwGo6gyHqE1W1uapua7Z/CNwFLOxtVhplUa299Urg3yW5Jck/Jfm5XiekyZXkjcCDVfX1XueiafOfgM/3OglNmoXAA137I1hw9a0kRwM/C9zS20w0akasU62ZIckXgJeNcej36fxZmUvnfzf9HHBtkpeXazLOKnvo498DTp3ejDQVdtfPVXVD0+b36fyv5E9OZ26aUhkj5n+j+1CSQ4C/Bf5LVf2g1/mow6JaO1TVL453LMn5wPVNEf2VJM8BRwKPTFd+am+8Pk7yM8AxwNeTQGdKwG1JTqyqh6YxRU2C3f0uAyRZAbwBOMV/GPeVEWBx1/4iYFOPctEUSbI/nYL6k1V1fa/z0fOc/qG99XfA6wGSvBI4AHi0pxlp0lTVN6rqpVV1dFUdTecv5xMsqPtPktOB9wJvrKonep2PJtWtwJIkxyQ5ADgXWNPjnDSJ0hn1uAK4q6r+pNf56MdZVGtvXQm8PMkddG5+WeEIlzQr/RnwImBtkq8l+YteJ6TJ0dyA+k7gRjo3sF1bVRt6m5Um2euAtwKvb35/v5bkzF4npQ4fUy5JkiS15Ei1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJFtSTNUEkOTHJFkvuS/DDJV5Oc0XX8lCTfTPJEki8lOWqnc69M8oMkDyX5r735FJI0GCyqJWnmmgM8APx74MXA+4Frkxyd5Ejg+iZ2OLAO+Ouucz8ALAGOApYDv5vk9OlLXZIGi0W1JM1QVfWvVfWBqvpuVT1XVZ8FvgMsBf4DsKGq/qaqfkSniH5tklc3p78N+GBVba2qu4C/BN4OkGQoyUiS9yR5OMnmJO8Y/blJrkry50k+n2Rbkv+d5GVJ/jTJ1mZ0/Gen75uQpJnPolqSZokk84BXAhuA44Cvjx6rqn8FvgUcl2QusKD7eLN9XNf+y+iMfi8EzgM+3pw36j8CfwAcCTwFfBm4rdm/DviTyfxskjTbWVRL0iyQZH/gk8DVVfVN4BDg8Z2aPQ68qDnGTsdHj416Bri4qp6pqs8B24BXdR3/dFWtb0bBPw38qKquqapn6UwzcaRakrpYVEvSDJfkBcAngKeBdzbhbcChOzU9FPhhc4ydjo8eG/VYVW3v2n+C54txgO91bT85xn53W0kaeBbVkjSDJQlwBTAP+JWqeqY5tAF4bVe7nwB+is48663A5u7jzfaGaUlakgaQRbUkzWyXAz8NnFVVT3bFPw0cn+RXkhwE/Dfg9mZqCMA1wB8kmdvcvPifgaumMW9JGigW1ZI0QzXrTv8m8G+Ah5qVOLYl+fWqegT4FeASYCtwEnBu1+kX0blx8T7gn4A/rqp/mNYPIEkDJFXV6xwkSZKkWc2RakmSJKmlPRbVzWNuH05yR1fs8CRrk9zbvM9t4knysSQbk9ye5ISuc1Y07e9NsqIrvjTJN5pzPtbclCNJkiTNGnszUn0VsPOjbS8EbqqqJcBNzT7AGXQei7sEWEnnBhuSHE5nft9JwInARV0PGbi8aTt6no/RlSRJ0qyyx6K6qv4Z2LJT+Gzg6mb7auBNXfFrquNm4LAk84HTgLVVtaVZ6mktcHpz7NCq+nJ1Jndf03UtSZIkaVaYM8Hz5lXVZoCq2pzkpU18IfBAV7uRJra7+MgY8TElWUlnVJuDDz546eLFiyeYvvbFc889xwte4PT7fmc/9z/7eDCM18+PPfbYju0jjjhiOlPSFPD3efrcc889j1bVS/bUbqJF9XjGmg9dE4iPqapWAasAli1bVuvWrZtIjtpHw8PDDA0N9ToNTTH7uf/Zx4NhvH7uvmXp0UcfncaMNBX8fZ4+Se7bm3YT/SfO95qpGzTvDzfxEaB7+HgRsGkP8UVjxCVJkqRZY6JF9RpgdAWPFcANXfG3NauAnAw83kwTuRE4tXmy11zgVODG5tgPk5zcrPrxtq5rSZIkSbPCHqd/JPkUMAQcmWSEzioelwLXJjkPuB84p2n+OeBMYCPwBPAOgKrakuSDwK1Nu4uravTmx/PprDByMPD55iVJkiTNGnssqqvqLeMcOmWMtgVcMM51rgSuHCO+Djh+T3lIkiRJM5W3jUqSJEktWVRLkiRJLVlUS5IkSS1N9jrVkiRpBlqzZk2vU5D6mkW1JEkD4Kyzzup1ClJfc/qHJEmS1JJFtSRJktSSRbUkSZLUknOqJUkaAAsWLNixvWnTph5mIvUni2pJkgbA5s2be52C1Nec/iFJkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktdSqqE7y20k2JLkjyaeSHJTkmCS3JLk3yV8nOaBpe2Czv7E5fnTXdd7XxO9Oclq7jyRJkiRNrwk/UTHJQuC3gGOr6skk1wLnAmcCH6mq1Un+AjgPuLx531pVr0hyLvBh4FeTHNucdxywAPhCkldW1bOtPpkkSdph3bp1vU5B6mttH1M+Bzg4yTPAC4HNwOuBX2uOXw18gE5RfXazDXAd8GdJ0sRXV9VTwHeSbAROBL7cMjdJktRYunRpr1OQ+tqEi+qqejDJZcD9wJPAPwLrge9X1fam2QiwsNleCDzQnLs9yePAEU385q5Ld5/zY5KsBFYCzJs3j+Hh4Ymmr32wbds2v+sBYD/3P/t4MNjPg8F+nnnaTP+YS2eU+Rjg+8DfAGeM0bRGTxnn2HjxXYNVq4BVAMuWLauhoaF9S1oTMjw8jN91/7Of+599PBjs58FgP888bW5U/EXgO1X1SFU9A1wP/DxwWJLRYn0RsKnZHgEWAzTHXwxs6Y6PcY4kSZI047Upqu8HTk7ywmZu9CnAncCXgDc3bVYANzTba5p9muNfrKpq4uc2q4McAywBvtIiL0mStJMkO16SJl+bOdW3JLkOuA3YDnyVztSMvwdWJ/lQE7uiOeUK4BPNjYhb6Kz4QVVtaFYOubO5zgWu/CFJkqTZpNXqH1V1EXDRTuFv01m9Y+e2PwLOGec6lwCXtMlFkiRJ6hWfqChJkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktdTq4S+SJGl2ePDBB3udgtTXLKolSRoACxYs6HUKUl9z+ockSZLUkkW1JEmS1JLTPyRJGgCbNm3ase1UEGnyWVRLkjQAFi5cuGO7qnqYidSfnP4hSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktdSqqE5yWJLrknwzyV1J/m2Sw5OsTXJv8z63aZskH0uyMcntSU7ous6Kpv29SVa0/VCSJEnSdGo7Uv1R4B+q6tXAa4G7gAuBm6pqCXBTsw9wBrCkea0ELgdIcjhwEXAScCJw0WghLkmSJM0GEy6qkxwK/AJwBUBVPV1V3wfOBq5uml0NvKnZPhu4pjpuBg5LMh84DVhbVVuqaiuwFjh9onlJkiRJ063NSPXLgUeAv0ry1ST/K8lPAPOqajNA8/7Spv1C4IGu80ea2HhxSZIkaVZo8/CXOcAJwLuq6pbCfvZpAAAgAElEQVQkH+X5qR5jyRix2k181wskK+lMHWHevHkMDw/vU8KamG3btvldDwD7uf/Zx4Nhb/rZPwezn7/PM0+bonoEGKmqW5r96+gU1d9LMr+qNjfTOx7uar+46/xFwKYmPrRTfHisH1hVq4BVAMuWLauhoaGxmmmSDQ8P43fd/+zn/mcfD4a96Wf/HMx+/j7PPBOe/lFVDwEPJHlVEzoFuBNYA4yu4LECuKHZXgO8rVkF5GTg8WZ6yI3AqUnmNjcontrEJEnSJKmqHS9Jk6/NSDXAu4BPJjkA+DbwDjqF+rVJzgPuB85p2n4OOBPYCDzRtKWqtiT5IHBr0+7iqtrSMi9JkiRp2rQqqqvqa8CyMQ6dMkbbAi4Y5zpXAle2yUWSJEnqFZ+oKEmSJLXUdvqHJEmaBdavX79je+nSpT3MROpPFtWSJA2AZcuen63pzYrS5HP6hyRJktSSRbUkSZLUkkW1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJFtSRJktSSRbUkSZLUkkW1JEmS1JJPVJQkaQDMnz+/1ylIfc2iWpKkAbBp06ZepyD1Nad/SJIkSS1ZVEuSJEktWVRLkiRJLTmnWpKkAfCZz3xmx/ZZZ53Vw0yk/tS6qE6yH7AOeLCq3pDkGGA1cDhwG/DWqno6yYHANcBS4DHgV6vqu8013gecBzwL/FZV3dg2L0mS9Lw3vvGNO7arqoeZSP1pMqZ/vBu4q2v/w8BHqmoJsJVOsUzzvrWqXgF8pGlHkmOBc4HjgNOBP28KdUmSJGlWaFVUJ1kE/DLwv5r9AK8HrmuaXA28qdk+u9mnOX5K0/5sYHVVPVVV3wE2Aie2yUuSJEmaTm1Hqv8U+F3guWb/COD7VbW92R8BFjbbC4EHAJrjjzftd8THOEeSJEma8SY8pzrJG4CHq2p9kqHR8BhNaw/HdnfOzj9zJbASYN68eQwPD+9Lypqgbdu2+V0PAPu5/9nHg2Fv+tk/B7Ofv88zT5sbFV8HvDHJmcBBwKF0Rq4PSzKnGY1eBIw+wmkEWAyMJJkDvBjY0hUf1X3Oj6mqVcAqgGXLltXQ0FCL9LW3hoeH8bvuf/Zz/7OPB8Pe9LN/DmY/f59nnglP/6iq91XVoqo6ms6Nhl+sql8HvgS8uWm2Arih2V7T7NMc/2J1bj9eA5yb5MBm5ZAlwFcmmpckSZI03aZiner3AquTfAj4KnBFE78C+ESSjXRGqM8FqKoNSa4F7gS2AxdU1bNTkJckSZI0JSalqK6qYWC42f42Y6zeUVU/As4Z5/xLgEsmIxdJkiRpuvmYckmSJKklH1MuSdIAOOGEE1qdf/Si+dz34EO7xF94wAt44unndokftfBlfHdkc6ufKc0mFtWSJA2A9evXtzr/vgcfoi46dJd4/vAH48R3LcClfub0D0mSBHRGo5OM+ZK0e45US5IkYPzRaOiMSEsanyPVkiRJUkuOVEuSNABWrVq1Y3vlypVT/vMO3I8xp414A6P6lUW1JEkD4Dd/8zd3bP/3iy8acyWPyfTUs3gDowaKRbUkSQNmdyt5SJoY51RLkiRJLVlUS5LUp7qXyJM0tSyqJUnqU6PTPMZbJk/S5LGoliRJklqyqJYkSdNmdKm9nV9HL5rf69SkVlz9Q5IkTRuX2lO/cqRakqRZrvuGxPXr13tzotQDjlRLkjTLda87Pbxgvx3brjstTR+LakmSBsAbXvn8X/mfvWd7DzOR+pNFtSRJA+Azb3nhjm1HsKXJN+E51UkWJ/lSkruSbEjy7iZ+eJK1Se5t3uc28ST5WJKNSW5PckLXtVY07e9NsqL9x5Ikqf90z53ufknqvTYj1duB91TVbUleBKxPshZ4O3BTVV2a5ELgQuC9wBnAkuZ1EnA5cFKSw4GLgGVANddZU1VbW+QmSVLf6Z473a0fRp5Hl9rb2VELX8Z3Rzb3ICNp30y4qK6qzcDmZvuHSe4CFgJnA0NNs6uBYTpF9dnANVVVwM1JDksyv2m7tqq2ADSF+enApyaamyRJml1cak+zXTo1bsuLJEcD/wwcD9xfVYd1HdtaVXOTfBa4tKr+pYnfRKfYHgIOqqoPNfH3A09W1WVj/JyVwEqAefPmLV29enXr3LVn27Zt45BDDul1Gppi9nP/s49nv/Xr17N0wX67xjc9uyO+7cAFHPLUpl3iV133Dzva/8zP/9Ier7O3x6YlvnTpmDkNMn+fp8/y5cvXV9WyPbVrXVQnOQT4J+CSqro+yffHKar/HvijnYrq3wVeDxy4U1H9RFX9P7v7ucuWLat169a1yl17Z3h4mKGhoV6noSlmP/c/+3j2SzLu9I8dS+q96g8ZuvuiXeI7TxHZ03X29ti0xCdhALDf+Ps8fZLsVVHd6uEvSfYH/hb4ZFVd34S/10zroHl/uImPAIu7Tl8EbNpNXJIkSZoV2qz+EeAK4K6q+pOuQ2uA0RU8VgA3dMXf1qwCcjLweDMv+0bg1CRzm5VCTm1ikiT1tfmLfnLM1TzmL/rJXqcmaR+1Wf3jdcBbgW8k+VoT+z3gUuDaJOcB9wPnNMc+B5wJbASeAN4BUFVbknwQuLVpd/HoTYuSJPWzhx58gKPe+9ld4vd9+A09yEZSG21W//gXYLzFMU8Zo30BF4xzrSuBKyeaiyRJktRLPlFRkqQZZrw1myXNXBbVkiTNMOOv2Tz7H/Ii9atWq39IkqQ9G++GREn9w5FqSZImwfxFP8lDDz4w7nFvSJT6m0W1JEmTYLyVPMDiWRoEFtWSJA2A/3zC/ju2//K2Z3qYyeRYsHgBm0c27xKfv2g+mx7wGXKafhbVkiTtgz1N85ipVp118I7tfiiqN49s5virjt8lfsfb7+hBNpJFtSRJ+8QHtkgai6t/SJI0BlfsmBkyZ9c+sB80EzlSLUkaaLubzuGIdO/V9tqnaR6jRfjOnGutqWZRLUkaaIMynWPlZ57sdQrTYl+LcGmyWFRLkgbCbL3BcLL0w82J0kxmUS1JmpXGK5L3O+Agnn36R2OeMwgj0pJ6w6JakjSjTWTOs8WzdjbeXGtwvrUmh0W1JGna7K5AdoRZU2m8udbgfGtNDotqSdKk29fRZXCEWb3jiiGaDBbVkvrKREZCX7ZwMZtH7t/ra413nX2Nj/dzp8NUfzZwdFmzhyuGaDJYVEua0SbrZjTYzUjoZf/HuHMt93XO7j7Fx/m5Eylg9+acyy67jOXLl+84NqWfzeJZfcARbO2LGVNUJzkd+CiwH/C/qurSHqckaQpM5ooNk1bMPftMbwrD3fzcfYnv7TkHvGz7jm2LXmnPxhvB3vAbGyy2tYsZUVQn2Q/4OPBLwAhwa5I1VXVnbzOT+tNYhe1ll13GL552RquR0L2Jgys2SJrdLLY1lhlRVAMnAhur6tsASVYDZwMW1QNkvBHMfZ3v2qv5sZN5ramOw66F7QEv286zT/+o1Ujo3sYlqR/ta7G934H78exTz+4StwifnVJVvc6BJG8GTq+q32j23wqcVFXv3KndSmBls/sq4O5pTXRwHQk82uskNOXs5/5nHw8G+3kw2M/T56iqesmeGs2Ukeqx7hDapdqvqlXAqqlPR92SrKuqZb3OQ1PLfu5/9vFgsJ8Hg/0887yg1wk0RoDFXfuLAP+/hyRJkmaFmVJU3wosSXJMkgOAc4E1Pc5JkiRJ2iszYvpHVW1P8k7gRjpL6l1ZVRt6nJae55SbwWA/9z/7eDDYz4PBfp5hZsSNipIkSdJsNlOmf0iSJEmzlkW1JEmS1JJFtfZakncluTvJhiT/o9f5aGok+Z0kleTIXueiyZfkj5N8M8ntST6d5LBe56TJk+T05r/TG5Nc2Ot8NLmSLE7ypSR3NX8Xv7vXOel5FtXaK0mW03nK5Wuq6jjgsh6npCmQZDHwS8Cuj6RUv1gLHF9VrwHuAd7X43w0SZLsB3wcOAM4FnhLkmN7m5Um2XbgPVX108DJwAX28cxhUa29dT5waVU9BVBVD/c4H02NjwC/yxgPX1J/qKp/rKrtze7NdJ4LoP5wIrCxqr5dVU8Dq+kMhqhPVNXmqrqt2f4hcBewsLdZaZRFtfbWK4F/l+SWJP+U5Od6nZAmV5I3Ag9W1dd7nYumzX8CPt/rJDRpFgIPdO2PYMHVt5IcDfwscEtvM9GoGbFOtWaGJF8AXjbGod+n82dlLp3/3fRzwLVJXl6uyTir7KGPfw84dXoz0lTYXT9X1Q1Nm9+n87+SPzmduWlKZYyY/43uQ0kOAf4W+C9V9YNe56MOi2rtUFW/ON6xJOcD1zdF9FeSPAccCTwyXfmpvfH6OMnPAMcAX08CnSkBtyU5saoemsYUNQl297sMkGQF8AbgFP9h3FdGgMVd+4uATT3KRVMkyf50CupPVtX1vc5Hz3P6h/bW3wGvB0jySuAA4NGeZqRJU1XfqKqXVtXRVXU0nb+cT7Cg7j9JTgfeC7yxqp7odT6aVLcCS5Ick+QA4FxgTY9z0iRKZ9TjCuCuqvqTXuejH2dRrb11JfDyJHfQufllhSNc0qz0Z8CLgLVJvpbkL3qdkCZHcwPqO4Eb6dzAdm1VbehtVppkrwPeCry++f39WpIze52UOnxMuSRJktSSI9WSJElSSxbVkiRJUksW1ZIkSVJLFtWSJElSSxbVkiRJUksW1ZIkSVJLFtWSJElSSxbVkiRJUksW1ZIkSVJLFtWSJElSSxbVkjSDJRlO8qMk25rX3V3Hfi3JfUn+NcnfJTm869jhST7dHLsvya/15hNI0mCwqJakme+dVXVI83oVQJLjgP8JvBWYBzwB/HnXOR8Hnm6O/TpweXOOJGkKWFRL0uz068Bnquqfq2ob8H7gPyR5UZKfAH4FeH9VbauqfwHW0CnASfL2JP+S5LIkW5N8J8kZoxduRsc/lOT/a0bHP5PkiCSfTPKDJLcmOXraP7EkzWAW1ZI08/1RkkeT/O8kQ03sOODrow2q6lt0RqZf2byerap7uq7x9eacUScBdwNHAv8DuCJJuo6fS6cIXwj8FPBl4K+Aw4G7gIsm7dNJUh+wqJakme29wMvpFLergM8k+SngEODxndo+DrxoD8dG3VdVf1lVzwJXA/PpTBUZ9VdV9a2qehz4PPCtqvpCVW0H/gb42Un5dJLUJ+b0OgFJ0viq6pau3auTvAU4E9gGHLpT80OBHwLP7ebYqIe6fsYTzSD1IV3Hv9e1/eQY+91tJWngOVItSbNLAQE2AK8dDSZ5OXAgcE/zmpNkSdd5r23OkSRNAYtqSZqhkhyW5LQkByWZk+TXgV8AbgQ+CZyV5N81NyZeDFxfVT+sqn8FrgcuTvITSV4HnA18olefRZL6ndM/JGnm2h/4EPBq4Fngm8CbqupugCT/F53i+gjgC8A7us79v4ErgYeBx4Dzq8qRakmaIqmqXucgSZIkzWpO/5AkSZJa2mNRneTKJA8nuaMrdniStUnubd7nNvEk+ViSjUluT3JC1zkrmvb3JlnRFV+a5BvNOR/baZ1USZIkacbbm5Hqq4DTd4pdCNxUVUuAm5p9gDOAJc1rJXA5dIpwOg8KOAk4EbhotBBv2qzsOm/nnyVJkiTNaHssqqvqn4EtO4XPpvOwAJr3N3XFr6mOm4HDkswHTgPWVtWWqtoKrAVOb44dWlVfrs7k7mu6riVJkiTNChNd/WNeVW0GqKrNSV7axBcCD3S1G2liu4uPjBEfU5KVdEa1Ofjgg5cuXrx4gulrXzz33HO84AVOv+939nP/s4+nxmOPPbZj+4gjjuhhJh3282Cwn6fPPffc82hVvWRP7SZ7Sb2x5kPXBOJjqqpVdB7Ty7Jly2rdunUTyVH7aHh4mKGhoV6noSlmP/c/+3hqdN8K9Oijj/Ywkw77eTDYz9MnyX17026i/8T5XjN1g+b94SY+AnQPHy8CNu0hvmiMuCRJkjRrTLSoXgOMruCxArihK/62ZhWQk4HHm2kiNwKnJpnb3KB4KnBjc+yHSU5uVv14W9e1JEmSpFlhj9M/knwKGAKOTDJCZxWPS4Frk5wH3A+c0zT/HHAmsBF4gubpXlW1JckHgVubdhdX1ejNj+fTWWHkYODzzUuSJEmaNfZYVFfVW8Y5dMoYbQu4YJzrXEnnkbk7x9cBx+8pD0mSJGmm8rZRSZIkqSWLakmSJKkli2pJkiSppclep1qSpIGyZs2aXqcgaQawqJYkqYWzzjqr1ylImgGc/iFJkiS1ZFEtSZIktWRRLUmSJLXknGpJklpYsGDBju1Nmzb1MBNJvWRRLUlSC5s3b+51CpJmAKd/SJIkSS1ZVEuSJEktWVRLkiRJLVlUS5IkSS1ZVEuSJEktWVRLkiRJLVlUS5IkSS1ZVEuSJEkttSqqk/x2kg1J7kjyqSQHJTkmyS1J7k3y10kOaNoe2OxvbI4f3XWd9zXxu5Oc1u4jSZIkSdNrwk9UTLIQ+C3g2Kp6Msm1wLnAmcBHqmp1kr8AzgMub963VtUrkpwLfBj41STHNucdBywAvpDklVX1bKtPJknSNFi3bl2vU5A0A7R9TPkc4OAkzwAvBDYDrwd+rTl+NfABOkX12c02wHXAnyVJE19dVU8B30myETgR+HLL3CRJmnJLly7tdQqSZoAJF9VV9WCSy4D7gSeBfwTWA9+vqu1NsxFgYbO9EHigOXd7kseBI5r4zV2X7j7nxyRZCawEmDdvHsPDwxNNX/tg27ZtftcDwH7uf/bxYLCfB4P9PPO0mf4xl84o8zHA94G/Ac4Yo2mNnjLOsfHiuwarVgGrAJYtW1ZDQ0P7lrQmZHh4GL/r/mc/9z/7eDDYz4PBfp552tyo+IvAd6rqkap6Brge+HngsCSjxfoiYFOzPQIsBmiOvxjY0h0f4xxJkiRpxmtTVN8PnJzkhc3c6FOAO4EvAW9u2qwAbmi21zT7NMe/WFXVxM9tVgc5BlgCfKVFXpIkTZskO16SBlebOdW3JLkOuA3YDnyVztSMvwdWJ/lQE7uiOeUK4BPNjYhb6Kz4QVVtaFYOubO5zgWu/CFJkqTZpNXqH1V1EXDRTuFv01m9Y+e2PwLOGec6lwCXtMlFkiRJ6hWfqChJkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktWRRLUmSJLVkUS1JkiS1ZFEtSZIktdTq4S+SJA26Bx98sNcpSJoBLKolSWphwYIFvU5B0gzg9A9JkiSpJYtqSZIkqSWnf0iS1MKmTZt2bDsVRBpcFtWSJLWwcOHCHdtV1cNMJPWS0z8kSZKkliyqJUmSpJYsqiVJkqSWLKolSZKklloV1UkOS3Jdkm8muSvJv01yeJK1Se5t3uc2bZPkY0k2Jrk9yQld11nRtL83yYq2H0qSJEmaTm1Hqj8K/ENVvRp4LXAXcCFwU1UtAW5q9gHOAJY0r5XA5QBJDgcuAk4CTgQuGi3EJUmSpNlgwkV1kkOBXwCuAKiqp6vq+8DZwNVNs6uBNzXbZwPXVMfNwGFJ5gOnAWuraktVbQXWAqdPNC9JkiRpurVZp/rlwCPAXyV5LbAeeDcwr6o2A1TV5iQvbdovBB7oOn+kiY0X30WSlXRGuZk3bx7Dw8Mt0tfe2rZtm9/1ALCf+599PPVmwvdrPw8G+3nmaVNUzwFOAN5VVbck+SjPT/UYS8aI1W7iuwarVgGrAJYtW1ZDQ0P7lLAmZnh4GL/r/mc/9z/7eOrNhO/Xfh4M9vPM02ZO9QgwUlW3NPvX0Smyv9dM66B5f7ir/eKu8xcBm3YTlyRJkmaFCRfVVfUQ8ECSVzWhU4A7gTXA6AoeK4Abmu01wNuaVUBOBh5vponcCJyaZG5zg+KpTUySpBmvqna8JA2uNtM/AN4FfDLJAcC3gXfQKdSvTXIecD9wTtP2c8CZwEbgiaYtVbUlyQeBW5t2F1fVlpZ5SZIkSdOmVVFdVV8Dlo1x6JQx2hb/f3t3H2RXXd9x/P2dhQRmgvJUkiWJBhRaeYqSAJ2hziSgGCmETkfHWMtE0QnjIMWWKEZrGVu02KZibbVORpiCwzSlJUqoVORpqXTKg4sghIBmEMjmoSgPQkZJTPrtH/fsepPcm83uuXfP3Xvfr5nMnvO755797v7y8Mlvv+ccuKTJea4DritTiyRJklQVn6goSZIklVS2/UOSpJ42ODg4sj1v3rwKK5FUJUO1JEklzJ//my5IL1aUepftH5IkSVJJhmpJkiSpJEO1JEmSVJKhWpIkSSrJUC1JkiSVZKiWJEmSSjJUS5IkSSUZqiVJkqSSDNWSJElSST5RUZKkEvr7+6suQVIHMFRLklTC5s2bqy5BUgew/UOSJEkqyVAtSZIklWSoliRJkkqyp1qSpBJuvfXWke3zzz+/wkokVal0qI6IPuAHwKbMPC8ijgFWA4cDDwMXZuaOiJgK3ADMA14A3peZzxTnWAF8GNgF/Elm3l62LkmSJsLixYtHtjOzwkokVakV7R+XAevr9r8IXJOZxwEvUQvLFB9fysw3A9cUxxERJwBLgBOBRcDXiqAuSZIkTQqlQnVEzAJ+H/hGsR/AWcC/F4dcD/xBsX1BsU/x+tnF8RcAqzNze2b+FNgAnF6mLkmSJGkilW3/+DLwSeCQYv8I4OXM3FnsDwEzi+2ZwEaAzNwZEb8ojp8J3F93zvr37CYilgHLAKZPn87AwEDJ8rU/tm3b5ve6BzjP3c85br9O+P46z73Bee484w7VEXEe8HxmDkbEguHhBofmKK/t6z27D2auAlYBzJ8/PxcsWNDoMLXYwMAAfq+7n/Pc/Zzj9uuE76/z3Buc585TZqX6TGBxRJwLHAS8jtrK9aERcUCxWj0LGH7U1BAwGxiKiAOA1wMv1o0Pq3+PJEmS1PHG3VOdmSsyc1ZmzqF2oeHdmfkB4B7gPcVhS4Fbiu21xT7F63dn7TLptcCSiJha3DnkOODB8dYlSZIkTbR23Kf6CmB1RFwF/BC4thi/FvhmRGygtkK9BCAz10XETcATwE7gkszc1Ya6JEmSpLZoSajOzAFgoNh+mgZ378jM14D3Nnn/54HPt6IWSZIkaaL5mHJJkiSpJB9TLklSCaeeemrVJUjqAIZqSZJKGBwcrLoESR3A9g9JkiSpJEO1JEmSVJKhWpIkSSrJnmpJkkpYtWrVyPayZcsqrERSlQzVkiSVcPHFF49sG6ql3mX7hyRJklSSoVqSJEkqyVAtSZIklWSoliRpDPpnvYGIGPklSeCFipIkNdQ/6w1s3bSx4WtvvOI/Rraf/eJ5I9vNQnbflIPYteO1/R6fMXM2W4aeG2vJkipkqJYkqYGtmzbuFp6H1YfoPTU6fvg9zc411s8hqTPZ/iFJkiSVZKiWJPW0PXuk7ZWWNB62f0iSetp42jwkaU+uVEuSVMLBbzqNg990WtVlSKrYuEN1RMyOiHsiYn1ErIuIy4rxwyPijoj4SfHxsGI8IuIrEbEhIn4UEafWnWtpcfxPImJp+S9LkqTdtavN46j3XMlR77myBRVKmszKtH/sBC7PzIcj4hBgMCLuAD4I3JWZV0fEp4BPAVcA7waOK36dAfwTcEZEHA5cCcwHsjjP2sx8qURtkiTtphvaPObM6ufZTVv3Gn/jzBk8M7SlgookDRt3qM7MLcCWYvvViFgPzAQuABYUh10PDFAL1RcAN2RmAvdHxKER0V8ce0dmvghQBPNFwL+MtzZJkrrRs5u2kle+bq/x+NzeQVvSxGpJT3VEzAHeBjwATC8C93DwPqo4bCZQfxf9oWKs2bgkSWPm3TwkVSFqC8clThAxDbgX+HxmromIlzPz0LrXX8rMwyLiO8BfZ+Z9xfhdwCeBs4CpmXlVMf5Z4JeZ+XcNPtcyYBnA9OnT561evbpU7do/27ZtY9q0aVWXoTZznrtfr8zx4OAgU2a8ea/xHVs3tGX8tptrP1jdte1Fzl96ScOaxvM55s2bt9f44OAg847u23t8866R43tlnnud8zxxFi5cOJiZ80c7rtQt9SLiQOBm4MbMXFMM/29E9GfmlqK94/lifAiYXff2WcDmYnzBHuMDjT5fZq4CVgHMnz8/FyxY0OgwtdjAwAB+r7uf89z9emWOFy5c2KR3enlbxp/91r+ObP/41Msa1jTWz7F15XK272p4qobtH4uuemXk+JUrV7J8+XLAXutu1it/nieTcYfqqP0s7VpgfWZ+qe6ltcBS4Ori4y114x+LiNXULlT8RRG8bwe+MHyXEOAcYMV465Ik9Yb+WW9g66aNox84CW3f1Tg8x+deGfX4gaP7RrbttZYmTpmV6jOBC4HHIuKRYuzT1ML0TRHxYeA54L3Fa7cB5wIbgF8CHwLIzBcj4q+Ah4rj/nL4okVJkprphrt5tNvUPhr2k7uCLbVembt/3Ac0u/Lj7AbHJ9Cw2SwzrwOuG28tkiRpb81XvF3BllrNJypKkiRJJRmqJUkdzVvktd5wW8iev+bM6q+6NGnSKnX3D0mSWmVfFx7aO91azdpCDrpqa9P/sNiHLe2boVqS1BG88LB6zcI22Ictjcb2D0mSJKkkQ7UkacI064+2R7rz2Yct7ZvtH5KkCdOsxQMmb5vHtLnvAmDbo7dXXEl7eXs+ad9cqZYktVwv3bHjiEWXcsSiS6suozKuYEs1rlRLklrOiw57hyvYUo0r1ZKkceulFWmNjSvY6jWuVEuSRuU9pDVWY70XtvfB1mRnqJYkjcp2juZe+O4/VF3CpGK7iLqVoVqSNGJfK9JqrNvv+iFp/xiqJakH2c6hTjPcg70n20I0WRiqJakH2c6hTmNbiCY77/4hSV2g2V04Dph68Mj24OCgd+eQpDZxpVqSJonR+p2brTwPj0+ZsXNk2xVpTXZHzz6aLQ3aQvpn9bN54+YKKlKvM1RLUkWaheS+KQexa8drDd/TbY/4lkYTBzT/6cpJ/3zSXmPrPrKu4fF9U/vYtX1Xw/MYxNUKHROqI2IR8PdAH/CNzLy64hrQQTEAAAcGSURBVJIkaUxaFZLrV5f3HJd6Te7MhuH58Q8+PubjG41D8yBu2NZYdESojog+4KvAO4Eh4KGIWJuZT1RbmaRONZ4A2+y1Vo2DIVmajJoFccO2xqIjQjVwOrAhM58GiIjVwAWAoVqaQPvq2W13IG13gN3Xa60cl9Q9xhq2m7WYGMJ7Q6eE6plA/b/kQ8AZFdWiFmgWzmbMnM2Woedacq5ODHmdVtNYxleuXNn0NmswMYHUACtpMhhri8lYQ/h4wrkXblYvMrPqGoiI9wLvysyPFPsXAqdn5qV7HLcMWFbs/jbw1IQW2ruOBH5edRFqO+e5+znHvcF57g3O88R5Y2b+1mgHdcpK9RAwu25/FrDXf6sycxWwaqKKUk1E/CAz51ddh9rLee5+znFvcJ57g/PceTrl4S8PAcdFxDERMQVYAqytuCZJkiRpv3TESnVm7oyIjwG3U7ul3nWZua7isiRJkqT90hGhGiAzbwNuq7oONWTLTW9wnrufc9wbnOfe4Dx3mI64UFGSJEmazDqlp1qSJEmatAzV2m8RcWlEPBUR6yLib6quR+0REcsjIiPiyKprUetFxN9GxJMR8aOI+FZEHFp1TWqdiFhU/D29ISI+VXU9aq2ImB0R90TE+uLf4suqrkm/YajWfomIhdSecnlKZp4IrKy4JLVBRMwG3gmM7Qk9mkzuAE7KzFOAHwMrKq5HLRIRfcBXgXcDJwDvj4gTqq1KLbYTuDwz3wL8LnCJc9w5DNXaXx8Frs7M7QCZ+XzF9ag9rgE+CXixRZfKzO9l5s5i935qzwVQdzgd2JCZT2fmDmA1tcUQdYnM3JKZDxfbrwLrqT2VWh3AUK39dTzw9oh4ICLujYjTqi5IrRURi4FNmflo1bVowlwE/GfVRahlZgIb6/aHMHB1rYiYA7wNeKDaSjSsY26pp+pFxJ3AjAYvfYba75XDqP246TTgpog4Nr19zKQyyhx/GjhnYitSO+xrnjPzluKYz1D7UfKNE1mb2ioajPl3dBeKiGnAzcDHM/OVqutRjaFaIzLzHc1ei4iPAmuKEP1gRPwfcCTws4mqT+U1m+OIOBk4Bng0IqDWEvBwRJyemVsnsES1wL7+LANExFLgPOBs/2PcVYaA2XX7s4DNFdWiNomIA6kF6hszc03V9eg3bP/Q/vo2cBZARBwPTAF+XmlFapnMfCwzj8rMOZk5h9o/zqcaqLtPRCwCrgAWZ+Yvq65HLfUQcFxEHBMRU4AlwNqKa1ILRW3V41pgfWZ+qep6tDtDtfbXdcCxEfE4tYtflrrCJU1K/wgcAtwREY9ExNerLkitUVyA+jHgdmoXsN2UmeuqrUotdiZwIXBW8ef3kYg4t+qiVOMTFSVJkqSSXKmWJEmSSjJUS5IkSSUZqiVJkqSSDNWSJElSSYZqSZIkqSRDtSRJklSSoVqSJEkqyVAtSR0uIt4REd8c53sPjoh7I6Kv2L84IrYUD414NCL+LSKOGcP5pkTEf0XEAeOpR5K6laFakjrfXODRcb73ImBNZu4q9k8B/iIz35qZc4G7gDXF449HlZk7ive8b5z1SFJXMlRLUuebCzwSEb9TrBKvi4g7I+JIgIh4SzH+o4j4RERsqHvvB4Bb6vZPBh4f3snMrwMzgNljqOfbxXklSQVDtSR1vrnAY8DNwGWZeSJwB/CnRRvGjcX4KcCxFKE5IqYAx2bmM3XnOglYt8f5fwUcNoZ6HgdOG8fXIUldy544SepgEXEg8DpgAXBfZv6weOkJYDHwh8Cje4w/X2wfCbxcd67ZwKuZ+coe5+8Hnm7y+VcARwAvFB+/kZlPRsSOiDgkM19tyRcqSZOcK9WS1NlOANYXHx+rGz+ZWoA+BXikbvykuv1fAQfVvXYKe69Sfwi4G3gtIr4QEV+OiK8BRMQZwPuBV4uPP83MJ4v3TQVeK/elSVL3MFRLUmebSy0kb6IWrImIY4ELgRuorSAfX4y/FfhjiosaM/MloC8ihoP1bv3UEXEOsAJYDiwDDqa2sj2tOOTHwADwFWAgM79avO8I4GeZ+et2fMGSNBnZ/iFJnW0u8CCwFjg3Ih6jtgJ9UWa+UNxq7zsR8RDwP8AzmVnfyvE94PeAO6mF6gURcTYQ1FbAF2XmUxFxOXBJZm6ve+9bqQX04Y/DFgK3teFrlaRJKzKz6hokSeMUEdMyc1ux/Qng9Zn553Wvvw34s8y8cJTznA/8EbARuDszvxsRHwe+D7wd+H5mDhbHrgFWZOZTbfmiJGkSMlRL0iQWEZ8FlgC/Bv6bWoDevscxFwHX192rusznmwIsycwbyp5LkrqJoVqSJEkqyQsVJUmSpJIM1ZIkSVJJhmpJkiSpJEO1JEmSVJKhWpIkSSrJUC1JkiSVZKiWJEmSSjJUS5IkSSX9P90SRoQhXiUOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Dbins = [-10, -0.233, 0.895, 10]\n",
    "bins = np.linspace(-10, 10, 200)\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(12, 9))\n",
    "counter = 0\n",
    "means = []\n",
    "for ax in axes:\n",
    "    means.append(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogDeff1'].median())\n",
    "    for i in range(3):\n",
    "        fstats_tot[(fstats_tot['Particle Size']==sizes[counter]) & (Dbins[i] < fstats_tot['LogDeff1']) & (fstats_tot['LogDeff1'] < Dbins[i+1])].hist(column='LogDeff1', bins=bins, figsize=(12,3), edgecolor='k', ax=ax, )\n",
    "        ax.set_xlim([-7.5, 3.5])\n",
    "        ax.set_ylim([0, 10000])\n",
    "    ax.axvline(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogDeff1'].median(), color='k', linestyle='dashed', linewidth=3)\n",
    "    ax.set_title(sizes[counter]+ 'nm')\n",
    "    if counter == 2:\n",
    "        ax.set_xlabel(r'$log(D_{eff})$')\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9200    0.8844    0.9019     97895\n",
      "        200     0.5678    0.5525    0.5600     57134\n",
      "        500     0.6109    0.6699    0.6390     55041\n",
      "        nan     0.0000    0.0000    0.0000         0\n",
      "\n",
      "avg / total     0.7432    0.7379    0.7400    210070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_true2 = fstats_tot['Particle Size'].values\n",
    "y_pred2 = list(pd.cut(fstats_tot['LogDeff1'].values, bins=Dbins, labels=['500', '200', '100']).astype(str))\n",
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_tot['LogMeanDeff1'] = np.log(fstats_tot['Mean Deff1']).replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09365521  0.96167416]\n"
     ]
    }
   ],
   "source": [
    "meanD = np.array(means)\n",
    "meanD.sort()\n",
    "Dbins = meanD[0:-1] + np.diff(meanD)/2\n",
    "print(Dbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dbins = [-10, -0.0937, 0.9617, 10]\n",
    "bins = np.linspace(-10, 10, 200)\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(12, 9))\n",
    "counter = 0\n",
    "means = []\n",
    "for ax in axes:\n",
    "    means.append(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogMeanDeff1'].median())\n",
    "    for i in range(3):\n",
    "        fstats_tot[(fstats_tot['Particle Size']==sizes[counter]) & (Dbins[i] < fstats_tot['LogMeanDeff1']) & (fstats_tot['LogMeanDeff1'] < Dbins[i+1])].hist(column='LogMeanDeff1', bins=bins, figsize=(12,3), edgecolor='k', ax=ax, )\n",
    "        ax.set_xlim([-7.5, 3.5])\n",
    "        ax.set_ylim([0, 10000])\n",
    "    ax.axvline(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogMeanDeff1'].median(), color='k', linestyle='dashed', linewidth=3)\n",
    "    ax.set_title(sizes[counter]+ 'nm')\n",
    "    if counter == 2:\n",
    "        ax.set_xlabel(r'$log(D_{eff})$')\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.9996    1.0000    0.9998     97895\n",
      "        200     0.7205    0.8176    0.7660     57134\n",
      "        500     0.7798    0.6698    0.7206     55041\n",
      "        nan     0.0000    0.0000    0.0000         0\n",
      "\n",
      "avg / total     0.8661    0.8639    0.8630    210070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_true2 = fstats_tot['Particle Size'].values\n",
    "y_pred2 = list(pd.cut(fstats_tot['LogMeanDeff1'].values, bins=Dbins, labels=['500', '200', '100']).astype(str))\n",
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = list(pd.cut(size2, bins=[-100, 150, 350, 1000000], labels=['100', '200', '500']).astype(str))\n",
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "size3 = np.random.rand(len(size2))\n",
    "y_pred2 = list(pd.cut(size3, bins=[0, 0.33333333, 0.666666666, 10], labels=['100', '200', '500']).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     0.4643    0.3322    0.3873     97895\n",
      "        200     0.2721    0.3324    0.2992     57134\n",
      "        500     0.2621    0.3344    0.2939     55041\n",
      "\n",
      "avg / total     0.3590    0.3328    0.3389    210070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "size2 = 2*10**9*10**12*kb*T/(fstats_tot['Mean Deff1'].values*6*np.pi*nu)\n",
    "y_pred2 = list(pd.cut(size2, bins=[-100, 150, 350, 1000000], labels=['100', '200', '500']).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        100     1.0000    0.9979    0.9989     97895\n",
      "        200     0.7012    0.1323    0.2226     57134\n",
      "        500     0.5120    0.9450    0.6642     55041\n",
      "        nan     0.0000    0.0000    0.0000         0\n",
      "\n",
      "avg / total     0.7909    0.7486    0.7001    210070\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true2, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46475104,  1.3120489 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = 1.381*10**-23\n",
    "T = 303\n",
    "nu = 0.000797\n",
    "\n",
    "sizes3 = np.array([150, 350])\n",
    "Dbins2 = np.log(2*10**9*10**12*kb*T/(sizes3*6*np.pi*nu))\n",
    "Dbins2.sort()\n",
    "Dbins2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Dbins = [-10, 0.4648, 1.3120, 10]\n",
    "bins = np.linspace(-10, 10, 200)\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(12, 9))\n",
    "counter = 0\n",
    "means = []\n",
    "Ds = np.log(2*10**9*10**12*kb*T/(np.array([100, 200, 500])*6*np.pi*nu))\n",
    "for ax in axes:\n",
    "    means.append(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogDeff1'].median())\n",
    "    for i in range(3):\n",
    "        fstats_tot[(fstats_tot['Particle Size']==sizes[counter]) & (Dbins[i] < fstats_tot['LogDeff1']) & (fstats_tot['LogDeff1'] < Dbins[i+1])].hist(column='LogDeff1', bins=bins, figsize=(12,3), edgecolor='k', ax=ax, )\n",
    "        ax.set_xlim([-7.5, 3.5])\n",
    "        ax.set_ylim([0, 10000])\n",
    "    ax.axvline(Ds[counter], color='k', linestyle='dashed', linewidth=3)\n",
    "    ax.set_title(sizes[counter]+ 'nm')\n",
    "    if counter == 2:\n",
    "        ax.set_xlabel(r'$log(D_{eff})$')\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Dbins = [-10, 0.4648, 1.3120, 10]\n",
    "bins = np.linspace(-10, 10, 200)\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(12, 9))\n",
    "counter = 0\n",
    "means = []\n",
    "Ds = np.log(2*10**9*10**12*kb*T/(np.array([100, 200, 500])*6*np.pi*nu))\n",
    "for ax in axes:\n",
    "    means.append(fstats_tot[fstats_tot['Particle Size']==sizes[counter]]['LogMeanDeff1'].median())\n",
    "    for i in range(3):\n",
    "        fstats_tot[(fstats_tot['Particle Size']==sizes[counter]) & (Dbins[i] < fstats_tot['LogMeanDeff1']) & (fstats_tot['LogMeanDeff1'] < Dbins[i+1])].hist(column='LogMeanDeff1', bins=bins, figsize=(12,3), edgecolor='k', ax=ax, )\n",
    "        ax.set_xlim([-7.5, 3.5])\n",
    "        ax.set_ylim([0, 10000])\n",
    "    ax.axvline(Ds[counter], color='k', linestyle='dashed', linewidth=3)\n",
    "    ax.set_title(sizes[counter]+ 'nm')\n",
    "    if counter == 2:\n",
    "        ax.set_xlabel(r'$log(D_{eff})$')\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
