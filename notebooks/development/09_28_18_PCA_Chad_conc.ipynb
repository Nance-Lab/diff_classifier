{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.pca as pca\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_1uL_XY01.csv\n",
      "features_1uL_XY02.csv\n",
      "features_1uL_XY03.csv\n",
      "features_1uL_XY04.csv\n",
      "features_1uL_XY05.csv\n",
      "features_1uL_XY06.csv\n",
      "features_1uL_XY07.csv\n",
      "features_1uL_XY08.csv\n",
      "features_1uL_XY09.csv\n",
      "features_1uL_XY10.csv\n",
      "features_pt5uL_XY01.csv\n",
      "features_pt5uL_XY02.csv\n",
      "features_pt5uL_XY03.csv\n",
      "features_pt5uL_XY04.csv\n",
      "features_pt5uL_XY05.csv\n",
      "features_pt5uL_XY06.csv\n",
      "features_pt5uL_XY07.csv\n",
      "features_pt5uL_XY08.csv\n",
      "features_pt5uL_XY09.csv\n",
      "features_pt5uL_XY10.csv\n",
      "features_pt1uL_XY01.csv\n",
      "features_pt1uL_XY02.csv\n",
      "features_pt1uL_XY03.csv\n",
      "features_pt1uL_XY04.csv\n",
      "features_pt1uL_XY05.csv\n",
      "features_pt1uL_XY06.csv\n",
      "features_pt1uL_XY07.csv\n",
      "features_pt1uL_XY08.csv\n",
      "features_pt1uL_XY09.csv\n",
      "features_pt1uL_XY10.csv\n",
      "features_pt05uL_XY01.csv\n",
      "features_pt05uL_XY02.csv\n",
      "features_pt05uL_XY03.csv\n",
      "features_pt05uL_XY04.csv\n",
      "features_pt05uL_XY05.csv\n",
      "features_pt05uL_XY06.csv\n",
      "features_pt05uL_XY07.csv\n",
      "features_pt05uL_XY08.csv\n",
      "features_pt05uL_XY09.csv\n",
      "features_pt05uL_XY10.csv\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "remote_folder = 'Gel_Studies/09_19_18_NP_concentration' #Folder in AWS S3 containing files to be analyzed\n",
    "bucket = 'ccurtis.data'\n",
    "vids = 10\n",
    "concs = ['1', 'pt5', 'pt1', 'pt05']\n",
    "nonnum = ['Particle Type', 'Video Number', 'Track_ID', 'Deff2',\n",
    "          'Mean Mean_Intensity', 'Std Mean_Intensity']\n",
    "\n",
    "counter = 0\n",
    "for conc in concs:\n",
    "    for num in range(1, vids+1):\n",
    "        try:\n",
    "            filename = 'features_{}uL_XY{}.csv'.format(conc, '%02d' % num)\n",
    "            #os.remove(filename)\n",
    "            aws.download_s3('{}/{}'.format(remote_folder, filename), filename, bucket_name=bucket)\n",
    "            fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "            fstats['NP concentration'] = pd.Series(fstats.shape[0]*[conc], index=fstats.index)\n",
    "            fstats['Video Number'] = pd.Series(fstats.shape[0]*[num], index=fstats.index)\n",
    "            #fstats['Calcium Concentration'] = pd.Series(fstats.shape[0]*[str(calcs)], index=fstats.index)\n",
    "            #print(num)\n",
    "            print(filename)\n",
    "            counter = counter + 1\n",
    "            if counter == 1:\n",
    "                fstats_tot = fstats\n",
    "            else:\n",
    "                fstats_tot = fstats_tot.append(fstats, ignore_index=True)\n",
    "        except:\n",
    "            print('skip filename: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_new.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_tot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA analyses with too many datapoints fail. You get rows with lots of NAs. I'm going to try making a subset of the data first\n",
    "#and then do a PCA analysis on that.\n",
    "\n",
    "#include all in analysis\n",
    "import random\n",
    "subset = np.sort(np.array(random.sample(range(fstats_new.shape[0]), 500000)))\n",
    "fstats_sub = fstats_new.loc[subset, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with equal sample sizes for each particle type\n",
    "import random\n",
    "counter = 0\n",
    "#mws = ['10k_PEG', '5k_PEG', '1k_PEG', 'PS_COOH']\n",
    "for mw in mws:\n",
    "    fstats_type = fstats_tot[fstats_tot['Particle Type']==mw].reset_index(drop=True)\n",
    "    print(fstats_type.shape)\n",
    "    subset = np.sort(np.array(random.sample(range(fstats_type.shape[0]), 4600)))\n",
    "    if counter == 0:\n",
    "        fstats_sub = fstats_type.loc[subset, :].reset_index(drop=True)\n",
    "    else:\n",
    "        fstats_sub = fstats_sub.append(fstats_type.loc[subset, :].reset_index(drop=True), ignore_index=True)\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mw in mws:\n",
    "    print(fstats_tot[fstats_tot['Particle Type'] == mw].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonnum = ['Particle Type', 'Video Number', 'Track_ID', 'Deff2',\n",
    "          'Mean Mean_Intensity', 'Std Mean_Intensity']\n",
    "\n",
    "#fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "fstats_totMW = fstats_sub[fstats_sub['Particle Type'].isin(mws)].reset_index(drop=True)\n",
    "#nonnum = ['Particle Type', 'Video Number', 'Track_ID', 'Calcium Concentration', 'Deff2']\n",
    "fstats_num = fstats_totMW.drop(nonnum, axis=1)\n",
    "fstats_raw = fstats_num.values\n",
    "#fstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.pca_analysis function provides a completely contained PCA analysis of the input trajectory features dataset. It includes options to impute NaN values (fill in with average values or drop them), and to scale features. Read the docstring for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pcadataset = pca.pca_analysis(fstats_totMW, dropcols=nonnum, n_components=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.kmo function calculates the Kaiser-Meyer-Olkin statistic, a measure of sampling adequacy. Check the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmostat = pca.kmo(pcadataset.scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can then compare average principle component values between subgroups of the data. In this case, all particles were taken from the same sample, so there are no experimental subgroups. I chose to compare short trajectories to long trajectories, as I would expect differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ncomp = 11\n",
    "dicti = {}\n",
    "#test = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==200].as_matrix()), axis=0))[-6:]\n",
    "#test1 = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==500].as_matrix()), axis=0))[-6:]\n",
    "dicti[0] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='5k_PEG'].values[:, -ncomp:], axis=0)\n",
    "dicti[1] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='PS_COOH'].values[:, -ncomp:], axis=0)\n",
    "dicti[2] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='5k_PEG_NH2'].values[:, -ncomp:], axis=0)\n",
    "dicti[3] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='PS_NH2'].values[:, -ncomp:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mws\n",
    "pca.plot_pca(dicti, savefig=True, labels=labels, rticks=np.linspace(-5, 5, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable pcadataset.prcomps shows the user the major contributions to each of the new principle components. When observing the graph above, users can see that there are some differences between short trajectories and long trajectories in component 0 (asymmetry1 being the major contributor) and component 1 (elongation being the major contributor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.prcomps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pca.feature_violin(pcadataset.final, label='Particle Type', lvals=labels, fsubset=11, yrange=[-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats1 = pca.feature_plot_2D(pcadataset.final, label='Particle Type', lvals=labels, randcount=400, yrange=[-6, 6],\n",
    "                              xrange=[-4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats1 = pca.feature_plot_3D(pcadataset.final, label='Particle Type', lvals=labels, randcount=400, ylim=[-12, 12],\n",
    "                              xlim=[-12, 12], zlim=[-12, 12], features=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncomp = 11\n",
    "trainp = np.array([])\n",
    "testp = np.array([])\n",
    "\n",
    "for i in range(0, 20):\n",
    "    KNNmod, X, y = pca.build_model(pcadataset.final, 'Particle Type', labels, equal_sampling=True,\n",
    "                                       tsize=500, input_cols=ncomp, model='MLP', NNhidden_layer=(6, 2))\n",
    "    trainp = np.append(trainp, pca.predict_model(KNNmod, X, y))\n",
    "    \n",
    "    X2 = pcadataset.final.values[:, -ncomp:]\n",
    "    y2 = pcadataset.final['Particle Type'].values\n",
    "    testp = np.append(testp, pca.predict_model(KNNmod, X2, y2))\n",
    "    print('Run {}: {}'.format(i, testp[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} +/ {}'.format(np.mean(trainp), np.std(trainp)))\n",
    "print('{} +/ {}'.format(np.mean(testp), np.std(testp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def build_model(rawdata, feature, featvals, equal_sampling=True,\n",
    "                    tsize=20, from_end=True, input_cols=6, model='KNN',\n",
    "                    **kwargs):\n",
    "    \"\"\"Builds a K-nearest neighbor model using an input dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rawdata : pandas.core.frames.DataFrame\n",
    "        Raw dataset of n samples and p features.\n",
    "    feature : string or int\n",
    "        Feature in rawdata containing output values on which KNN\n",
    "        model is to be based.\n",
    "    featvals : string or int\n",
    "        All values that feature can take.\n",
    "    equal_sampling : bool\n",
    "        If True, training dataset will contain an equal number\n",
    "        of samples that take each value of featvals. If false,\n",
    "        each sample in training dataset will be taken randomly\n",
    "        from rawdata.\n",
    "    tsize : int\n",
    "        Size of training dataset. If equal_sampling is False,\n",
    "        training dataset will be exactly this size. If True,\n",
    "        training dataset will contain N x tsize where N is the\n",
    "        number of unique values in featvals.\n",
    "    n_neighbors : int\n",
    "        Number of nearest neighbors to be used in KNN\n",
    "        algorithm.\n",
    "    from_end : int\n",
    "        If True, in_cols will select features to be used as\n",
    "        training data defined end of rawdata e.g.\n",
    "        rawdata[:, -6:]. If False, input_cols will be read\n",
    "        as a tuple e.g. rawdata[:, 10:15].\n",
    "    input_col : int or tuple\n",
    "        Defined in from_end above.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clf : sklearn.neighbors.classification.KNeighborsClassifier\n",
    "        KNN model\n",
    "    X : numpy.ndarray\n",
    "        training input dataset used to create clf\n",
    "    y : numpy.ndarray\n",
    "        training output dataset used to create clf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    defaults = {'n_neighbors': 5, 'NNsolver': 'lbfgs', 'NNalpha': 1e-5,\n",
    "                'NNhidden_layer': (5, 2), 'NNrandom_state': 1,\n",
    "                'n_estimators': 10}\n",
    "\n",
    "    for defkey in defaults.keys():\n",
    "        if defkey not in kwargs.keys():\n",
    "            kwargs[defkey] = defaults[defkey]\n",
    "    \n",
    "    if equal_sampling:\n",
    "        for featval in featvals:\n",
    "            if from_end:\n",
    "                test = rawdata[rawdata[feature] == featval\n",
    "                               ].values[:, -input_cols:]\n",
    "            else:\n",
    "                test = rawdata[rawdata[feature] == featval\n",
    "                               ].values[:, input_cols[0]:input_cols[1]]\n",
    "            to_plot = np.array(random.sample(range(0, test.shape[0]\n",
    "                                                   ), tsize))\n",
    "            if featval == featvals[0]:\n",
    "                X = test[to_plot, :]\n",
    "                y = rawdata[rawdata[feature] == featval\n",
    "                            ][feature].values[to_plot]\n",
    "            else:\n",
    "                X = np.append(X, test[to_plot, :], axis=0)\n",
    "                y = np.append(y, rawdata[rawdata[feature] == featval\n",
    "                                         ][feature].values[to_plot], axis=0)\n",
    "\n",
    "    else:\n",
    "        if from_end:\n",
    "            test = rawdata.values[:, -input_cols:]\n",
    "        else:\n",
    "            test = rawdata.values[:, input_cols[0]:input_cols[1]]\n",
    "        to_plot = np.array(random.sample(range(0, test.shape[0]), tsize))\n",
    "        X = test[to_plot, :]\n",
    "        y = rawdata[feature].values[to_plot]\n",
    "\n",
    "    if model is 'KNN':\n",
    "        clf = neighbors.KNeighborsClassifier(kwargs['n_neighbors'])\n",
    "    elif model is 'MLP':\n",
    "        clf = MLPClassifier(solver=kwargs['NNsolver'], alpha=kwargs['NNalpha'],\n",
    "                            hidden_layer_sizes=kwargs['NNhidden_layer'],\n",
    "                            random_state=kwargs['NNrandom_state'])\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=kwargs['n_estimators'])\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitesize['Particle Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
