{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and prepping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.pca as pca\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_5k_PEG_2mM_XY01.csv\n",
      "features_5k_PEG_2mM_XY02.csv\n",
      "features_5k_PEG_2mM_XY03.csv\n",
      "features_5k_PEG_2mM_XY04.csv\n",
      "features_5k_PEG_2mM_XY05.csv\n",
      "features_5k_PEG_2mM_XY06.csv\n",
      "features_5k_PEG_2mM_XY07.csv\n",
      "features_5k_PEG_2mM_XY08.csv\n",
      "features_5k_PEG_2mM_XY09.csv\n",
      "features_5k_PEG_2mM_XY10.csv\n",
      "features_PS_COOH_2mM_XY01.csv\n",
      "features_PS_COOH_2mM_XY02.csv\n",
      "features_PS_COOH_2mM_XY03.csv\n",
      "features_PS_COOH_2mM_XY04.csv\n",
      "features_PS_COOH_2mM_XY05.csv\n",
      "features_PS_COOH_2mM_XY06.csv\n",
      "features_PS_COOH_2mM_XY07.csv\n",
      "features_PS_COOH_2mM_XY08.csv\n",
      "skip filename: features_PS_COOH_2mM_XY09.csv\n",
      "features_PS_COOH_2mM_XY10.csv\n",
      "features_5k_PEG_NH2_2mM_XY01.csv\n",
      "features_5k_PEG_NH2_2mM_XY02.csv\n",
      "features_5k_PEG_NH2_2mM_XY03.csv\n",
      "features_5k_PEG_NH2_2mM_XY04.csv\n",
      "features_5k_PEG_NH2_2mM_XY05.csv\n",
      "features_5k_PEG_NH2_2mM_XY06.csv\n",
      "features_5k_PEG_NH2_2mM_XY07.csv\n",
      "features_5k_PEG_NH2_2mM_XY08.csv\n",
      "features_5k_PEG_NH2_2mM_XY09.csv\n",
      "features_5k_PEG_NH2_2mM_XY10.csv\n",
      "features_PS_NH2_2mM_XY01.csv\n",
      "features_PS_NH2_2mM_XY02.csv\n",
      "features_PS_NH2_2mM_XY03.csv\n",
      "features_PS_NH2_2mM_XY04.csv\n",
      "features_PS_NH2_2mM_XY05.csv\n",
      "features_PS_NH2_2mM_XY06.csv\n",
      "features_PS_NH2_2mM_XY07.csv\n",
      "features_PS_NH2_2mM_XY08.csv\n",
      "features_PS_NH2_2mM_XY09.csv\n",
      "features_PS_NH2_2mM_XY10.csv\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "remote_folder = 'Gel_studies' #Folder in AWS S3 containing files to be analyzed\n",
    "bucket = 'dtoghani.data'\n",
    "vids = 10\n",
    "mws = ['5k_PEG', 'PS_COOH', '5k_PEG_NH2', 'PS_NH2']\n",
    "nonnum = ['Particle Type', 'Video Number', 'Track_ID', 'Deff2',\n",
    "          'Mean Mean_Intensity', 'Std Mean_Intensity',\n",
    "          'X', 'Y', 'Mean X', 'Mean Y', 'Std X', 'Std Y']\n",
    "calcs = [2]\n",
    "\n",
    "counter = 0\n",
    "for calc in calcs:\n",
    "    for mw in mws:\n",
    "        for num in range(1, vids+1):\n",
    "            try:\n",
    "                filename = 'features_{}_{}mM_XY{}.csv'.format(mw, calc, '%02d' % num)\n",
    "                #os.remove(filename)\n",
    "                aws.download_s3('{}/{}'.format(remote_folder, filename), filename, bucket_name=bucket)\n",
    "                fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "                fstats['Particle Type'] = pd.Series(fstats.shape[0]*[mw], index=fstats.index)\n",
    "                fstats['Video Number'] = pd.Series(fstats.shape[0]*[num], index=fstats.index)\n",
    "                #fstats['Calcium Concentration'] = pd.Series(fstats.shape[0]*[str(calcs)], index=fstats.index)\n",
    "                #print(num)\n",
    "                print(filename)\n",
    "                counter = counter + 1\n",
    "                if counter == 1:\n",
    "                    fstats_tot = fstats\n",
    "                else:\n",
    "                    fstats_tot = fstats_tot.append(fstats, ignore_index=True)\n",
    "            except:\n",
    "                print('skip filename: {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_new.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_tot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mw in mws:\n",
    "    print(fstats_tot[fstats_tot['Particle Type'] == mw].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fstats = pd.read_csv(filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "#fstats_totMW = fstats_sub[fstats_sub['Particle Type'].isin(mws)].reset_index(drop=True)\n",
    "#nonnum = ['Particle Type', 'Video Number', 'Track_ID', 'Calcium Concentration', 'Deff2']\n",
    "fstats_num = fstats_tot.drop(nonnum, axis=1)\n",
    "fstats_raw = fstats_num.values\n",
    "#fstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.pca_analysis function provides a completely contained PCA analysis of the input trajectory features dataset. It includes options to impute NaN values (fill in with average values or drop them), and to scale features. Read the docstring for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance:\n",
      "0 component: 0.2147677814368581\n",
      "1 component: 0.35314023666979566\n",
      "2 component: 0.44827510319784075\n",
      "3 component: 0.511465173503043\n",
      "4 component: 0.558607008008358\n",
      "5 component: 0.6044616497687325\n",
      "6 component: 0.6423863281246887\n",
      "7 component: 0.6764588579414139\n",
      "8 component: 0.7056231971569751\n",
      "9 component: 0.7335223971932557\n",
      "10 component: 0.7566582083285429\n",
      "11 component: 0.776504123454088\n",
      "12 component: 0.7951502790266778\n",
      "13 component: 0.8122942085037099\n"
     ]
    }
   ],
   "source": [
    "ncomp = 14\n",
    "pcadataset = pca.pca_analysis(fstats_tot, dropcols=nonnum, n_components=ncomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pca.kmo function calculates the Kaiser-Meyer-Olkin statistic, a measure of sampling adequacy. Check the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmostat = pca.kmo(pcadataset.scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.components.to_csv('components.csv')\n",
    "aws.upload_s3('components.csv', '{}/components.csv'.format(remote_folder, filename), bucket_name=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats_num.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can then compare average principle component values between subgroups of the data. In this case, all particles were taken from the same sample, so there are no experimental subgroups. I chose to compare short trajectories to long trajectories, as I would expect differences between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#ncomp = 10\n",
    "dicti = {}\n",
    "#test = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==200].as_matrix()), axis=0))[-6:]\n",
    "#test1 = np.exp(np.nanmean(np.log(pcadataset.final[pcadataset.final['Particle Size']==500].as_matrix()), axis=0))[-6:]\n",
    "dicti[0] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='5k_PEG'].values[:, -ncomp:], axis=0)\n",
    "dicti[1] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='PS_COOH'].values[:, -ncomp:], axis=0)\n",
    "dicti[2] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='5k_PEG_NH2'].values[:, -ncomp:], axis=0)\n",
    "dicti[3] = np.nanmean(pcadataset.final[pcadataset.final['Particle Type']=='PS_NH2'].values[:, -ncomp:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = mws\n",
    "pca.plot_pca(dicti, savefig=True, labels=labels, rticks=np.linspace(-4, 9, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable pcadataset.prcomps shows the user the major contributions to each of the new principle components. When observing the graph above, users can see that there are some differences between short trajectories and long trajectories in component 0 (asymmetry1 being the major contributor) and component 1 (elongation being the major contributor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.prcomps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pca.feature_violin(pcadataset.final, label='Particle Type', lvals=labels, fsubset=ncomp, yrange=[-12, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstats1 = pca.feature_plot_3D(pcadataset.final, label='Particle Type', lvals=labels, randcount=400, ylim=[-12, 12],\n",
    "                              xlim=[-12, 12], zlim=[-12, 12], features=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featofvar = 'Particle Type'\n",
    "test = pcadataset.final.values[:, -ncomp:]\n",
    "y = pcadataset.final[featofvar].values\n",
    "\n",
    "for run in range(1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(test, y, test_size=0.4)\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(900, ), solver='sgd', verbose=True, max_iter=500, tol=0.00001,\n",
    "                        alpha=0.001, batch_size=50, learning_rate_init=0.005, learning_rate='adaptive',\n",
    "                        early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Training Results')\n",
    "    y_true1, y_pred1 = y_train, clf.predict(X_train)\n",
    "    print(classification_report(y_true1, y_pred1, digits=4))\n",
    "    \n",
    "    print('Test Results')\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(clf.loss_curve_, linewidth=4)\n",
    "#ax1.set_xlim([0, 60])\n",
    "#ax1.set_ylim([0.04, 0.18])\n",
    "ax1.set_ylabel('Loss Curve')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(clf.validation_scores_, linewidth=4, c='g')\n",
    "#ax2.set_ylim([0.94, 0.99])\n",
    "ax2.set_ylabel('Validation Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 256, 512, 768, 1024, 1280, 1536, 1792, 2048]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = list(range(0, 2048+1, 256))\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.final['binx'] = pd.cut(pcadataset.final.X, bins, labels=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "pcadataset.final['biny'] = pd.cut(pcadataset.final.Y, bins, labels=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "pcadataset.final['bins'] = 8*pcadataset.final['binx'] + pcadataset.final['biny']\n",
    "pcadataset.final = pcadataset.final[np.isfinite(pcadataset.final.bins)]\n",
    "pca.final.bins = pcadataset.final.bins.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['binx', 'biny', 'bins', 'Track_ID', 'alpha', 'D_fit', 'kurtosis', 'asymmetry1', 'asymmetry2', 'asymmetry3', 'AR', 'elongation', 'boundedness', 'fractal_dim', 'trappedness', 'efficiency', 'straightness', 'MSD_ratio', 'frames', 'X', 'Y', 'Quality', 'Mean_Intensity', 'SN_Ratio', 'Deff1', 'Deff2', 'Mean alpha', 'Std alpha', 'Mean D_fit', 'Std D_fit', 'Mean kurtosis', 'Std kurtosis', 'Mean asymmetry1', 'Std asymmetry1', 'Mean asymmetry2', 'Std asymmetry2', 'Mean asymmetry3', 'Std asymmetry3', 'Mean AR', 'Std AR', 'Mean elongation', 'Std elongation', 'Mean boundedness', 'Std boundedness', 'Mean fractal_dim', 'Std fractal_dim', 'Mean trappedness', 'Std trappedness', 'Mean efficiency', 'Std efficiency', 'Mean straightness', 'Std straightness', 'Mean MSD_ratio', 'Std MSD_ratio', 'Mean frames', 'Std frames', 'Mean X', 'Std X', 'Mean Y', 'Std Y', 'Mean Quality', 'Std Quality', 'Mean Mean_Intensity', 'Std Mean_Intensity', 'Mean SN_Ratio', 'Std SN_Ratio', 'Mean Deff1', 'Std Deff1', 'Mean Deff2', 'Std Deff2', 'Particle Type', 'Video Number', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "cols = pcadataset.final.columns.tolist()\n",
    "cols = cols[-3:] + cols[:-3]\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadataset.final = pcadataset.final[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkerboard(size):\n",
    "    rows = int(size/2)\n",
    "    checks = list(range(0, size*size, size+1))\n",
    "    \n",
    "    for i in range(1, rows):\n",
    "        ssize = size - 2*i\n",
    "        for j in range(0, ssize):\n",
    "            checks.append(2*i + (size+1)*j)\n",
    "\n",
    "    for i in range(1, rows):\n",
    "        ssize = size - 2*i\n",
    "        for j in range(0, ssize):\n",
    "            checks.append(size*size - 1 - (2*i + (size+1)*j))\n",
    "    checks.sort()\n",
    "    return checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.07994082\n",
      "Validation score: 0.978152\n",
      "Iteration 2, loss = 0.05721556\n",
      "Validation score: 0.983714\n",
      "Iteration 3, loss = 0.04420738\n",
      "Validation score: 0.987679\n",
      "Iteration 4, loss = 0.03436394\n",
      "Validation score: 0.989844\n",
      "Iteration 5, loss = 0.02773984\n",
      "Validation score: 0.992537\n",
      "Iteration 6, loss = 0.02315337\n",
      "Validation score: 0.994022\n",
      "Iteration 7, loss = 0.01993042\n",
      "Validation score: 0.995306\n",
      "Iteration 8, loss = 0.01750084\n",
      "Validation score: 0.997005\n",
      "Iteration 9, loss = 0.01575539\n",
      "Validation score: 0.996967\n",
      "Iteration 10, loss = 0.01432590\n",
      "Validation score: 0.997848\n",
      "Iteration 11, loss = 0.01333080\n",
      "Validation score: 0.997898\n",
      "Iteration 12, loss = 0.01248011\n",
      "Validation score: 0.998389\n",
      "Iteration 13, loss = 0.01183610\n",
      "Validation score: 0.998351\n",
      "Iteration 14, loss = 0.01126477\n",
      "Validation score: 0.998376\n",
      "Iteration 15, loss = 0.01083906\n",
      "Validation score: 0.998905\n",
      "Iteration 16, loss = 0.01046874\n",
      "Validation score: 0.998339\n",
      "Iteration 17, loss = 0.01020160\n",
      "Validation score: 0.998981\n",
      "Iteration 18, loss = 0.01000569\n",
      "Validation score: 0.998603\n",
      "Iteration 19, loss = 0.00967626\n",
      "Validation score: 0.998704\n",
      "Iteration 20, loss = 0.00950926\n",
      "Validation score: 0.999295\n",
      "Iteration 21, loss = 0.00951690\n",
      "Validation score: 0.999157\n",
      "Iteration 22, loss = 0.00928477\n",
      "Validation score: 0.998741\n",
      "Iteration 23, loss = 0.00926835\n",
      "Validation score: 0.999434\n",
      "Iteration 24, loss = 0.00912421\n",
      "Validation score: 0.999169\n",
      "Iteration 25, loss = 0.00897029\n",
      "Validation score: 0.999396\n",
      "Iteration 26, loss = 0.00884321\n",
      "Validation score: 0.999333\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.001000\n",
      "Iteration 27, loss = 0.00811843\n",
      "Validation score: 0.999648\n",
      "Iteration 28, loss = 0.00803132\n",
      "Validation score: 0.999673\n",
      "Iteration 29, loss = 0.00800899\n",
      "Validation score: 0.999622\n",
      "Iteration 30, loss = 0.00799670\n",
      "Validation score: 0.999648\n",
      "Iteration 31, loss = 0.00797912\n",
      "Validation score: 0.999698\n",
      "Iteration 32, loss = 0.00797577\n",
      "Validation score: 0.999673\n",
      "Iteration 33, loss = 0.00796757\n",
      "Validation score: 0.999635\n",
      "Iteration 34, loss = 0.00795694\n",
      "Validation score: 0.999673\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 35, loss = 0.00788110\n",
      "Validation score: 0.999660\n",
      "Iteration 36, loss = 0.00787384\n",
      "Validation score: 0.999622\n",
      "Iteration 37, loss = 0.00787092\n",
      "Validation score: 0.999673\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 38, loss = 0.00785401\n",
      "Validation score: 0.999660\n",
      "Iteration 39, loss = 0.00785270\n",
      "Validation score: 0.999648\n",
      "Iteration 40, loss = 0.00785160\n",
      "Validation score: 0.999660\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 41, loss = 0.00784820\n",
      "Validation score: 0.999660\n",
      "Iteration 42, loss = 0.00784797\n",
      "Validation score: 0.999660\n",
      "Iteration 43, loss = 0.00784787\n",
      "Validation score: 0.999660\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 44, loss = 0.00784693\n",
      "Validation score: 0.999660\n",
      "Iteration 45, loss = 0.00784690\n",
      "Validation score: 0.999660\n",
      "Iteration 46, loss = 0.00784689\n",
      "Validation score: 0.999660\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 47, loss = 0.00784669\n",
      "Validation score: 0.999660\n",
      "Iteration 48, loss = 0.00784669\n",
      "Validation score: 0.999660\n",
      "Iteration 49, loss = 0.00784669\n",
      "Validation score: 0.999660\n",
      "Validation score did not improve more than tol=0.000010 for two consecutive epochs. Learning rate too small. Stopping.\n",
      "Training Results\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     5k_PEG     0.9997    0.9999    0.9998    246757\n",
      " 5k_PEG_NH2     0.9998    0.9996    0.9997    189917\n",
      "    PS_COOH     1.0000    1.0000    1.0000    352129\n",
      "     PS_NH2     1.0000    1.0000    1.0000      5761\n",
      "\n",
      "avg / total     0.9999    0.9999    0.9999    794564\n",
      "\n",
      "Test Results\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     5k_PEG     0.9504    0.9551    0.9528    249710\n",
      " 5k_PEG_NH2     0.9451    0.9363    0.9407    191174\n",
      "    PS_COOH     0.9935    0.9955    0.9945    352051\n",
      "     PS_NH2     0.9822    0.9574    0.9697      6010\n",
      "\n",
      "avg / total     0.9684    0.9684    0.9684    798945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featofvar = 'Particle Type'\n",
    "\n",
    "X_train = pcadataset.final[pcadataset.final.bins.isin(checkerboard(8))].values[:, -ncomp:]\n",
    "X_test = pcadataset.final[~pcadataset.final.bins.isin(checkerboard(8))].values[:, -ncomp:]\n",
    "y_train = pcadataset.final[pcadataset.final.bins.isin(checkerboard(8))][featofvar].values\n",
    "y_test = pcadataset.final[~pcadataset.final.bins.isin(checkerboard(8))][featofvar].values\n",
    "\n",
    "for run in range(1):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(900, ), solver='sgd', verbose=True, max_iter=500, tol=0.00001,\n",
    "                        alpha=0.001, batch_size=50, learning_rate_init=0.005, learning_rate='adaptive',\n",
    "                        early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Training Results')\n",
    "    y_true1, y_pred1 = y_train, clf.predict(X_train)\n",
    "    print(classification_report(y_true1, y_pred1, digits=4))\n",
    "    \n",
    "    print('Test Results')\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
